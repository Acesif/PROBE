{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ceeae8f-ef7d-4b00-9f31-26785721d9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import numpy\n",
    "import collections\n",
    "import abc\n",
    "import numpy.typing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d32c7db-3ab7-4614-986a-8ca2f73a4aaf",
   "metadata": {},
   "source": [
    "# Getting the data\n",
    "\n",
    "Note that the `*_names` variables are not needed to play the \"prediction games\"; they are only for interpreting the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e830818c-ddb4-4fde-a2e9-2b8ecdfa41a5",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'systems_by_benchmarks.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m systems_by_benchmarks \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystems_by_benchmarks.npy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m benchmarks_by_features \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbenchmarks_by_features.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m collector_names \u001b[38;5;241m=\u001b[39m pathlib\u001b[38;5;241m.\u001b[39mPath(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcollectors.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mread_text()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/nix/store/fcxqkrbvqb1fc2fmpxvgvk3788m2h3ip-python3-3.10.13-env/lib/python3.10/site-packages/numpy/lib/npyio.py:427\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    425\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 427\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    428\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'systems_by_benchmarks.npy'"
     ]
    }
   ],
   "source": [
    "systems_by_benchmarks = numpy.load(\"systems_by_benchmarks.npy\")\n",
    "benchmarks_by_features = numpy.load(\"benchmarks_by_features.npy\")\n",
    "\n",
    "collector_names = pathlib.Path(\"collectors.txt\").read_text().split(\"\\n\")\n",
    "benchmark_names = pathlib.Path(\"benchmarks.txt\").read_text().split(\"\\n\")\n",
    "feature_names = pathlib.Path(\"features.txt\").read_text().split(\"\\n\")\n",
    "\n",
    "n_systems, n_benchmarks = systems_by_benchmarks.shape\n",
    "_, n_features = benchmarks_by_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ffc185-3c9d-4d56-b358-8b3e72ae2d91",
   "metadata": {},
   "source": [
    "# Let's play the new system game\n",
    "\n",
    "It is more of a dialogue.\n",
    "\n",
    "- **Given** integer N and workloads x benchmark matrix\n",
    "- **Select** N workloads\n",
    "- **Given** new system's log slowdown ratio on N selected workloads\n",
    "- **Predict** new system's log slowdown ratio on all other workloads\n",
    "\n",
    "I initially scored this game by cross-validated root-mean-squared-error. However, I've found that even with cross-validation, more complex models are not \"punished\" enough. So I decided to also include Akaike Information Criterion (modified for small sample size). But in order to compute the AIC, one has to know the likelihood function. If you're lazy, your model has uninformitave priors, and your errors are normally distributed (although with unknown variance), you can use `naive_log_likelihood`, which computes the likelihood-maximizing variance, and returns the likelihood of the data based on that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "57cea0e6-a979-42f4-b331-6c33f26c42cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewSystemPredictor:\n",
    "    @abc.abstractmethod\n",
    "    def select_benchmarks(\n",
    "        self,\n",
    "        k: int,\n",
    "        systems_by_benchmarks: numpy.typing.NDArray,\n",
    "        benchmarks_by_features: numpy.typing.NDArray,\n",
    "    ) -> tuple[list[int], numpy.float64]:\n",
    "        \"\"\"\n",
    "        k: number of benchmarks to select\n",
    "        systems_by_benchmarks: array where the (i,j)th element is the log of the ith system's slowdown on the jth benchmark\n",
    "        benchmarks_by_features: array where the (j,m)th element is the mth feature of the jth benchmark\n",
    "\n",
    "        returns a tuple containing:\n",
    "          - k benchmarks to select\n",
    "          - the log-liklihood\n",
    "        \n",
    "        Liklihood is the probability of observing this data given the parameters you inferred\n",
    "        Used to compute the Akaike Information Criterion.\n",
    "        Return numpy.NaN if you just don't care.\n",
    "        \"\"\"\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def predict_new_system(\n",
    "        new_systems_by_selected_benchmarks: numpy.typing.NDArray,\n",
    "    ) -> numpy.typing.NDArray:\n",
    "        \"\"\"\n",
    "        new_system_by_selected_benchmarks: array where the (g,p)th element is the log of the gth new system's slowdown on the pth *selected* benchmark\n",
    "\n",
    "        returns an array where the (g,q)th element is the log of the gth new system's slowdown on the qth *unselected* benchmark\n",
    "        \"\"\"\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def n_parameters(self) -> int:\n",
    "        \"\"\"\n",
    "        Returns the number of parameters used to make this estimation.\n",
    "        Used to calculate the Akaike Information Criterion.\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0643da70-9b09-4ec4-bf8b-d1f0c69416a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.model_selection\n",
    "\n",
    "\n",
    "mean_absolute_error = lambda a, b: numpy.mean(numpy.fabs(a - b))\n",
    "\n",
    "root_mean_squared_error = lambda a, b: numpy.sqrt(numpy.mean((a-b)**2))\n",
    "\n",
    "\n",
    "def aicc(k: int, log_likelihood: float, n_points: int) -> float:\n",
    "    aic = 2 * k - 2 * log_likelihood\n",
    "    return aic + (2 * k**2 + 2 * k) / (n_points - k - 1)\n",
    "\n",
    "\n",
    "def test_system_predictors(\n",
    "    predictors: list[NewSystemPredictor]\n",
    ") -> None:\n",
    "    systems = list(range(n_systems))\n",
    "    cv_splitter = sklearn.model_selection.LeaveOneOut()\n",
    "    print(\"RMSE (lower is better), stddev RMSE (lower is better), AICc (higher is better)\")\n",
    "    for predictor in predictors:\n",
    "        results = []\n",
    "        selected = collections.Counter()\n",
    "        for train_systems, test_systems in cv_splitter.split(systems):\n",
    "            selected_benchmarks, _ = predictor.select_benchmarks(\n",
    "                systems_by_benchmarks[train_systems],\n",
    "                benchmarks_by_features,\n",
    "            )\n",
    "            unselected_benchmarks = [\n",
    "                benchmark\n",
    "                for benchmark in range(n_benchmarks)\n",
    "                if benchmark not in selected_benchmarks\n",
    "            ]\n",
    "            predicted = predictor.predict_new_systems(\n",
    "                systems_by_benchmarks[test_systems, :][:, selected_benchmarks],\n",
    "            )\n",
    "            for benchmark in selected_benchmarks:\n",
    "                selected[benchmark] += 1\n",
    "            actual = systems_by_benchmarks[test_systems, :][:, unselected_benchmarks]\n",
    "            results.append(root_mean_squared_error(actual, predicted))\n",
    "        result_mean = numpy.mean(results)\n",
    "        result_std = numpy.std(results)\n",
    "        _, log_likelihood = predictor.select_benchmarks(systems_by_benchmarks, benchmarks_by_features)\n",
    "        n_datapoints = len(systems_by_benchmarks.flatten()) + len(benchmarks_by_features.flatten())\n",
    "        my_aicc = aicc(predictor.n_parameters(), log_likelihood, n_datapoints)\n",
    "        print(\n",
    "            f\"{result_mean:.2f}\",\n",
    "            f\"{result_std:.2f}\",\n",
    "            f\"{numpy.log(my_aicc):.2f}\",\n",
    "            predictor,\n",
    "            {benchmark_names[benchmark]: count for benchmark, count in selected.items()},\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0ef741-83e8-4334-9723-404deb630dbd",
   "metadata": {},
   "source": [
    "Assume a model predicts $\\hat{r}_i = f(r_i) + \\eta_i$ where $\\eta_i$ is normally distributed around 0 with unknown variance.\n",
    "\n",
    "Let's find the variance which maximizes likelihood.\n",
    "\n",
    "First, I'll write down the PDF (which is likelihood function) for the Normal distribution, where $\\mu$ is the prediction and $x$ is the observation:\n",
    "\n",
    "$$f(x | \\mu, \\sigma) = \\frac{1}{\\sigma * \\sqrt{2\\pi}) * \\exp(-\\frac{1}{2} \\left( \\frac{x-\\mu}{\\sigma} \\right^2 $$\n",
    "\n",
    "Then log both sides.\n",
    "\n",
    "$$\\log f(x | \\mu, \\sigma) = -\\log(\\sigma \\sqrt{2\\pi}) - \\frac{1}{2} \\left( \\frac{x-\\mu}{\\sigma} \\right)^2 $$\n",
    "\n",
    "In order to maximize, take a derivative with $\\sigma$.\n",
    "\n",
    "$$\\frac{d}{d\\sigma} \\log f(x | \\mu, \\sigma) = -\\frac{1}{\\sigma} + \\frac{(x - \\mu)^2}{\\sigma^3} $$\n",
    "\n",
    "Set that to zero.\n",
    "\n",
    "$$0 = \\frac{d}{d\\sigma} \\log f(x | \\mu, \\sigma) \\implies \\frac{1}{\\sigma} = \\frac{(x - \\mu)^2}{\\sigma^3} \\implies \\sigma = |x - \\mu|$$\n",
    "\n",
    "Therefore $\\log f(x | \\mu, \\sigma)$ has a maximum at $\\sigma = x - \\mu|$.\n",
    "\n",
    "We can plug that back in to the log-likelihood function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b55277eb-a8ff-4a74-8e8e-c665adf29761",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_log_likelihood(actual, predicted) -> float:\n",
    "    std = numpy.clip(actual - predicted, 1e-2, None)\n",
    "    # Some of the values we hit \"dead-on\"\n",
    "    # This predicts the sigma should be 0, which is wrong\n",
    "    # It should actually be a small positive number.\n",
    "    \n",
    "    # Plugging this in to the log PDF above\n",
    "    return numpy.sum(-1/2*((actual - predicted)/std)**2 - numpy.log(std) + 1/2*numpy.log(2*numpy.pi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6d1bcbf9-0b2d-412e-843c-c0318be9738a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.linalg.interpolative\n",
    "\n",
    "class InitialSystemPredictor(NewSystemPredictor):\n",
    "    \"\"\"\n",
    "    This is a simple predictor just to test the mechanics.\n",
    "\n",
    "    It simply selects self.benchmarks.\n",
    "    Then it runs a regression to all the unselected benchmarks based on the selected benchmarks.\n",
    "    That's it.\n",
    "    \"\"\"\n",
    "    def __init__(self, benchmarks: list[int]) -> None:\n",
    "        self.benchmarks = benchmarks\n",
    "\n",
    "    def select_benchmarks(\n",
    "        self,\n",
    "        systems_by_benchmarks: numpy.typing.NDArray,\n",
    "        benchmarks_by_features: numpy.typing.NDArray,\n",
    "    ) -> tuple[list[int], numpy.float64]:\n",
    "        unselected_benchmarks = [\n",
    "            benchmark\n",
    "            for benchmark in range(systems_by_benchmarks.shape[1])\n",
    "            if benchmark not in self.benchmarks\n",
    "        ]\n",
    "        self.coeffs = numpy.linalg.pinv(systems_by_benchmarks[:, self.benchmarks]) @ systems_by_benchmarks[:, unselected_benchmarks]\n",
    "        log_likelihood = naive_log_likelihood(\n",
    "            systems_by_benchmarks[:, unselected_benchmarks],\n",
    "            systems_by_benchmarks[:, self.benchmarks] @ self.coeffs,\n",
    "        )\n",
    "        # the selected benchmarks will get probability = 1, log likelihood = 0, so you can imagine I wrote ... + 0 + 0 + 0 to the end\n",
    "        return self.benchmarks, log_likelihood\n",
    "\n",
    "    def predict_new_systems(\n",
    "        self,\n",
    "        new_systems_by_selected_benchmarks: numpy.typing.NDArray,\n",
    "    ) -> numpy.typing.NDArray:\n",
    "        return new_systems_by_selected_benchmarks @ self.coeffs\n",
    "\n",
    "    def n_parameters(self) -> int:\n",
    "        return len(self.coeffs)\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"{self.__class__.__name__}({self.benchmarks!r})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "85780d36-b787-4fbe-8a49-f0bf7cf02659",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.linalg.interpolative\n",
    "\n",
    "class InterpolativeDecompositionSystemPredictor(NewSystemPredictor):\n",
    "    \"\"\"\n",
    "    This method uses Interpolative Decomposition (ID).\n",
    "\n",
    "    ID factors a matrix A into B @ C.\n",
    "    It selects k columns of A, and puts those in B.\n",
    "    It puts the identity matrix in the corresponding columns of C.\n",
    "    The remaining N - k columns of A are predicted from a linear regression on the k columns of A (equivalently, all the columns of B).\n",
    "\n",
    "    This method should be pretty good.\n",
    "    \"\"\"\n",
    "    def __init__(self, k: int, use_features: bool) -> None:\n",
    "        self.k = k\n",
    "        self.use_features = use_features\n",
    "\n",
    "    def select_benchmarks(\n",
    "        self,\n",
    "        systems_by_benchmarks: numpy.typing.NDArray,\n",
    "        benchmarks_by_features: numpy.typing.NDArray,\n",
    "    ) -> tuple[list[int], numpy.float64]:\n",
    "        if self.use_features:\n",
    "            data = numpy.vstack([\n",
    "                systems_by_benchmarks,\n",
    "                benchmarks_by_features.T,\n",
    "            ])\n",
    "        else:\n",
    "            data = systems_by_benchmarks\n",
    "        idx, proj = scipy.linalg.interpolative.interp_decomp(data, self.k, rand=False)\n",
    "        self.idx = idx\n",
    "        self.proj = proj\n",
    "        # skel = scipy.linalg.interpolative.reconstruct_skel_matrix(data, self.k, idx)\n",
    "        # data_est = scipy.linalg.interpolative.reconstruct_matrix_from_id(skel, idx, proj)[:len(systems_by_benchmarks), :]\n",
    "        log_likelihood = naive_log_likelihood(\n",
    "            systems_by_benchmarks[:, self.idx[self.k:]],\n",
    "            systems_by_benchmarks[:, self.idx[:self.k]] @ self.proj,\n",
    "        )\n",
    "        return idx[:self.k], log_likelihood\n",
    "\n",
    "    def predict_new_systems(\n",
    "        self,\n",
    "        new_systems_by_selected_benchmarks: numpy.typing.NDArray,\n",
    "    ) -> numpy.typing.NDArray:\n",
    "        return new_systems_by_selected_benchmarks @ self.proj\n",
    "\n",
    "    def n_parameters(self) -> int:\n",
    "        return len(self.proj.flatten())\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"{self.__class__.__name__}({self.k}, {self.use_features})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "48ed3509-1d72-4f85-9a4b-e6bc17eb35d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<!-- Disable line wrapping in cell outputs to make the output more readable -->\n",
       "<style>\n",
       "div.jp-OutputArea-output pre {\n",
       "    white-space: pre;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<!-- Disable line wrapping in cell outputs to make the output more readable -->\n",
    "<style>\n",
    "div.jp-OutputArea-output pre {\n",
    "    white-space: pre;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f3b3001f-0443-4235-b275-3832d24fc1f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (lower is better), stddev RMSE (lower is better), AICc (higher is better)\n",
      "0.85 0.86 11.54 InitialSystemPredictor([0]) {'a-data-sci': 5}\n",
      "1.25 1.07 14.12 InitialSystemPredictor([0, 1]) {'a-data-sci': 5, 'archive': 5}\n",
      "1.74 2.41 11.36 InitialSystemPredictor([10, 20, 30]) {'blastn-NM_003949': 5, 'blastn-NM_024506': 5, 'blastn-NM_068205': 5}\n",
      "0.56 0.52 11.83 InterpolativeDecompositionSystemPredictor(1, True) {'megablast-NG_008953': 5}\n",
      "0.53 0.52 11.55 InterpolativeDecompositionSystemPredictor(2, True) {'megablast-NG_008953': 5, 'postmark': 5}\n",
      "0.53 0.52 11.35 InterpolativeDecompositionSystemPredictor(3, True) {'megablast-NG_008953': 5, 'postmark': 5, 'fstat': 5}\n",
      "0.53 0.52 11.33 InterpolativeDecompositionSystemPredictor(4, True) {'megablast-NG_008953': 5, 'postmark': 5, 'fstat': 5, 'true': 5}\n",
      "0.53 0.52 11.34 InterpolativeDecompositionSystemPredictor(5, True) {'megablast-NG_008953': 5, 'postmark': 5, 'fstat': 5, 'true': 5, 'exec': 5}\n",
      "0.52 0.52 11.41 InterpolativeDecompositionSystemPredictor(6, True) {'megablast-NG_008953': 5, 'postmark': 5, 'fstat': 5, 'true': 5, 'exec': 5, 'git setuptools_scm': 5}\n",
      "0.48 0.46 11.99 InterpolativeDecompositionSystemPredictor(7, True) {'megablast-NG_008953': 5, 'postmark': 5, 'fstat': 5, 'true': 5, 'exec': 5, 'git setuptools_scm': 5, 'archive pigz': 3, 'archive pbzip2': 2}\n",
      "0.46 0.47 12.46 InterpolativeDecompositionSystemPredictor(8, True) {'megablast-NG_008953': 5, 'postmark': 5, 'fstat': 5, 'true': 5, 'exec': 5, 'git setuptools_scm': 5, 'archive pigz': 3, 'blastn-XM_386257': 4, 'archive pbzip2': 2, 'stat': 1}\n",
      "0.48 0.45 11.98 InterpolativeDecompositionSystemPredictor(9, True) {'megablast-NG_008953': 5, 'postmark': 5, 'fstat': 5, 'true': 5, 'exec': 5, 'git setuptools_scm': 5, 'archive pigz': 3, 'blastn-XM_386257': 4, 'stat': 3, 'open/close': 3, 'archive pbzip2': 2}\n",
      "0.46 0.43 11.11 InterpolativeDecompositionSystemPredictor(10, True) {'megablast-NG_008953': 5, 'postmark': 5, 'fstat': 5, 'true': 5, 'exec': 5, 'git setuptools_scm': 5, 'archive pigz': 3, 'blastn-XM_386257': 4, 'stat': 4, 'blastn-XM_546594': 1, 'open/close': 4, 'blastn-NM_123974': 1, 'archive pbzip2': 2, 'page-fault': 1}\n",
      "0.46 0.44 9.57 InterpolativeDecompositionSystemPredictor(11, True) {'megablast-NG_008953': 5, 'postmark': 5, 'fstat': 5, 'true': 5, 'exec': 5, 'git setuptools_scm': 5, 'archive pigz': 3, 'blastn-XM_386257': 4, 'stat': 4, 'blastn-XM_546594': 2, 'megablast-NG_000006': 1, 'open/close': 4, 'blastn-NM_123974': 1, 'archive': 2, 'archive pbzip2': 2, 'page-fault': 1, 'unarchive pigz': 1}\n",
      "0.47 0.44 10.44 InterpolativeDecompositionSystemPredictor(12, True) {'megablast-NG_008953': 5, 'postmark': 5, 'fstat': 5, 'true': 5, 'exec': 5, 'git setuptools_scm': 5, 'archive pigz': 3, 'blastn-XM_386257': 4, 'stat': 5, 'blastn-XM_546594': 2, 'megablast-NG_000006': 1, 'echo': 2, 'open/close': 4, 'blastn-NM_123974': 1, 'archive': 3, 'archive pbzip2': 2, 'page-fault': 1, 'unarchive pigz': 2}\n",
      "0.53 0.44 13.02 InterpolativeDecompositionSystemPredictor(1, False) {'echo': 1, 'postmark': 3, 'true': 1}\n",
      "0.51 0.44 13.31 InterpolativeDecompositionSystemPredictor(2, False) {'echo': 1, 'stat': 1, 'postmark': 4, 'true': 3, 'open/close': 1}\n",
      "0.66 0.49 11.05 InterpolativeDecompositionSystemPredictor(3, False) {'echo': 1, 'stat': 2, 'postmark': 5, 'true': 3, 'fs': 3, 'open/close': 1}\n",
      "0.55 0.44 nan InterpolativeDecompositionSystemPredictor(4, False) {'echo': 1, 'stat': 3, 'postmark': 5, 'blastn-XM_499196': 1, 'true': 4, 'fs': 3, 'blastn-NM_010585': 1, 'open/close': 1, 'blastp-NP_436070': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_849069/3870894754.py:48: RuntimeWarning: invalid value encountered in log\n",
      "  f\"{numpy.log(my_aicc):.2f}\",\n"
     ]
    }
   ],
   "source": [
    "test_system_predictors([\n",
    "    InitialSystemPredictor([0]),\n",
    "    InitialSystemPredictor([0, 1]),\n",
    "    InitialSystemPredictor([10, 20, 30]),\n",
    "    *[\n",
    "        InterpolativeDecompositionSystemPredictor(k, True)\n",
    "        for k in range(1, n_systems + n_features)\n",
    "    ],\n",
    "    *[\n",
    "        InterpolativeDecompositionSystemPredictor(k, False)\n",
    "        for k in range(1, n_systems)\n",
    "    ],\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69248139-ee90-47a6-82bd-679a279f21e7",
   "metadata": {},
   "source": [
    "# Let's play the new benchmark game\n",
    "\n",
    "- **Given** the statistical features of a new workload\n",
    "- **Predict** the workload's log slowdown ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c8962289-c5bf-4ca3-afc4-bfc61e61aeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewBenchmarkPredictor(abc.ABC):\n",
    "    @abc.abstractmethod\n",
    "    def predict_new_benchmark(\n",
    "        self,\n",
    "        systems_by_benchmarks: numpy.typing.NDArray,\n",
    "        benchmarks_by_features: numpy.typing.NDArray,\n",
    "        new_benchmarks_by_features: numpy.typing.NDArray,\n",
    "    ) -> tuple[numpy.typing.NDArray, numpy.float64]:\n",
    "        \"\"\"\n",
    "        k: number of benchmarks to select\n",
    "        systems_by_benchmarks: array where the (i,j)th element is the log of the ith system's slowdown on the jth benchmark\n",
    "        benchmarks_by_features: array where the (j,m)th element is the mth feature of the jth benchmark\n",
    "        new_benchmarks_by_features: an array where the (k,m)th element is the mth feature of the kth new benchmark\n",
    "\n",
    "        returns a tuple containing:\n",
    "          - an array where the kth element is the log slowdown of the kth *new* benchmark\n",
    "          - the log-liklihood\n",
    "        \n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def n_parameters(self) -> int:\n",
    "        \"\"\"\n",
    "        Returns the number of parameters used to make this estimation.\n",
    "        Used to calculate the Akaike Information Criterion.\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0591b279-1639-4cb9-ac9a-f4654f7b261f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_benchmark_predictors(\n",
    "    predictors: list[NewBenchmarkPredictor],\n",
    ") -> None:\n",
    "    benchmarks = list(range(n_benchmarks))\n",
    "    test_size = 0.1\n",
    "    cv_splitter = sklearn.model_selection.ShuffleSplit(n_splits=10, test_size=test_size, random_state=0)\n",
    "    print(\"RMSE (lower is better), stddev RMSE (lower is better), AICc (higher is better)\")\n",
    "    for predictor in predictors:\n",
    "        results = []\n",
    "        for train_benchmarks, test_benchmarks in cv_splitter.split(benchmarks):\n",
    "            predicted, _ = predictor.predict_new_benchmark(\n",
    "                systems_by_benchmarks[:, train_benchmarks],\n",
    "                benchmarks_by_features[train_benchmarks, :],\n",
    "                benchmarks_by_features[test_benchmarks, :],\n",
    "            )\n",
    "            actual = systems_by_benchmarks[:, test_benchmarks]\n",
    "            results.append(root_mean_squared_error(actual, predicted))\n",
    "        result_mean = numpy.mean(results)\n",
    "        result_std = numpy.std(results)\n",
    "        _, log_likelihood = predictor.predict_new_benchmark(systems_by_benchmarks, benchmarks_by_features, benchmarks_by_features)\n",
    "        n_datapoints = len(systems_by_benchmarks.flatten()) + len(benchmarks_by_features.flatten())\n",
    "        my_aicc = aicc(predictor.n_parameters(), log_likelihood, n_datapoints)\n",
    "        print(\n",
    "            f\"{result_mean:.2f}\",\n",
    "            f\"{result_std:.2f}\",\n",
    "            f\"{numpy.log(my_aicc):.2f}\",\n",
    "            predictor,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bba004ae-063f-4347-99f9-eac7902bcb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.linalg.interpolative\n",
    "\n",
    "class Regression(NewBenchmarkPredictor):\n",
    "    \"\"\"\n",
    "    This method simply regresses performance on the full set of featuers.\n",
    "\n",
    "    No linear method should be able to do better in RMSE, but dimensionality reduction may help with AIC.\n",
    "    \"\"\"\n",
    "    def n_parameters(self) -> int:\n",
    "        return len(self.systems_by_features.flatten())\n",
    "\n",
    "    def predict_new_benchmark(\n",
    "        self,\n",
    "        systems_by_benchmarks: numpy.typing.NDArray,\n",
    "        benchmarks_by_features: numpy.typing.NDArray,\n",
    "        new_benchmarks_by_features: numpy.typing.NDArray,\n",
    "    ) -> tuple[numpy.typing.NDArray, numpy.float64]:\n",
    "        self.systems_by_features = systems_by_benchmarks @ numpy.linalg.pinv(benchmarks_by_features.T)\n",
    "        log_likelihood = naive_log_likelihood(\n",
    "            self.systems_by_features @ benchmarks_by_features.T,\n",
    "            systems_by_benchmarks,\n",
    "        )\n",
    "        return self.systems_by_features @ new_benchmarks_by_features.T, log_likelihood\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"{self.__class__.__name__}()\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bdafebb4-4532-4dfb-9ced-802c5b682cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.linalg.interpolative\n",
    "\n",
    "class LowRankMatrixFactorization(NewBenchmarkPredictor):\n",
    "    \"\"\"\n",
    "    Like Regression, but use a low-rank compression\n",
    "    \"\"\"\n",
    "    def __init__(self, dim: int) -> None:\n",
    "        self.dim = dim\n",
    "\n",
    "    def n_parameters(self) -> int:\n",
    "        return len(self.a.flatten()) + len(self.b.flatten())\n",
    "\n",
    "    def predict_new_benchmark(\n",
    "        self,\n",
    "        systems_by_benchmarks: numpy.typing.NDArray,\n",
    "        benchmarks_by_features: numpy.typing.NDArray,\n",
    "        new_benchmarks_by_features: numpy.typing.NDArray,\n",
    "    ) -> tuple[numpy.typing.NDArray, numpy.float64]:\n",
    "        tmp = systems_by_benchmarks @ numpy.linalg.pinv(benchmarks_by_features.T)\n",
    "        u, s, vh = numpy.linalg.svd(tmp, full_matrices=False)\n",
    "        self.a = (u[:, :self.dim] * s[:self.dim])\n",
    "        self.b = vh[:self.dim, :]\n",
    "        log_likelihood = naive_log_likelihood(\n",
    "            self.a @ self.b @ benchmarks_by_features.T,\n",
    "            systems_by_benchmarks,\n",
    "        )\n",
    "        return self.a @ self.b @ new_benchmarks_by_features.T, log_likelihood\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"{self.__class__.__name__}({self.dim})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e8984333-7fc2-47a7-88f4-69bb3ae3d2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.linalg.interpolative\n",
    "\n",
    "class GreedySubsetMatrixFactorization(NewBenchmarkPredictor):\n",
    "    \"\"\"\n",
    "    This method tries to select only dim features.\n",
    "\n",
    "    This is subtly different from \"compressing to a matrix of rank dim\".\n",
    "\n",
    "    Using only dim features, means the other coefficients **have to be** zero.\n",
    "\n",
    "    It's greedy because it picks the best feature, and adds next best given the current set, etc.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim: int) -> None:\n",
    "        self.dim = dim\n",
    "\n",
    "    def n_parameters(self) -> int:\n",
    "        return len(self.systems_by_features.flatten())\n",
    "\n",
    "    def predict_new_benchmark(\n",
    "        self,\n",
    "        systems_by_benchmarks: numpy.typing.NDArray,\n",
    "        benchmarks_by_features: numpy.typing.NDArray,\n",
    "        new_benchmarks_by_features: numpy.typing.NDArray,\n",
    "    ) -> tuple[numpy.typing.NDArray, numpy.float64]:\n",
    "        def test_goodness(features: list[int]) -> numpy.float64:\n",
    "            systems_by_features = systems_by_benchmarks @ numpy.linalg.pinv(benchmarks_by_features[:, features].T)\n",
    "            return numpy.sum((systems_by_features @ benchmarks_by_features[:, features].T - systems_by_benchmarks)**2)\n",
    "        selected_features = []\n",
    "        while len(selected_features) < self.dim:\n",
    "            unselected_features = [\n",
    "                feature\n",
    "                for feature in range(benchmarks_by_features.shape[1])\n",
    "                if feature not in selected_features\n",
    "            ]\n",
    "            selected_features = max([\n",
    "                selected_features + [candidate_feature]\n",
    "                for candidate_feature in unselected_features\n",
    "            ], key=test_goodness)\n",
    "        self.features = selected_features\n",
    "        self.systems_by_features = systems_by_benchmarks @ numpy.linalg.pinv(benchmarks_by_features[:, self.features].T)\n",
    "        log_likelihood = naive_log_likelihood(\n",
    "            self.systems_by_features @ benchmarks_by_features[:, self.features].T,\n",
    "            systems_by_benchmarks\n",
    "        )\n",
    "        return self.systems_by_features @ new_benchmarks_by_features[:, self.features].T, log_likelihood\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        features = \", \".join(feature_names[i] for i in self.features)\n",
    "        return f\"{self.__class__.__name__}({self.dim}): {features}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e6a67dd4-9432-477e-919a-ed197d07814f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (lower is better), stddev RMSE (lower is better), AICc (higher is better)\n",
      "0.46 0.11 14.08 MatrixFactorization()\n",
      "0.49 0.10 14.30 LowRankMatrixFactorization(1)\n",
      "0.48 0.10 14.13 LowRankMatrixFactorization(2)\n",
      "0.46 0.11 14.09 LowRankMatrixFactorization(3)\n",
      "0.46 0.11 14.08 LowRankMatrixFactorization(4)\n",
      "0.46 0.11 14.08 LowRankMatrixFactorization(5)\n",
      "0.46 0.11 14.08 LowRankMatrixFactorization(6)\n",
      "0.46 0.11 14.08 LowRankMatrixFactorization(7)\n",
      "0.46 0.11 14.08 LowRankMatrixFactorization(8)\n",
      "0.75 0.07 15.74 GreedySubsetMatrixFactorization(1): memory_mean\n",
      "0.75 0.09 15.67 GreedySubsetMatrixFactorization(2): memory_mean, exec_syscalls_per_sec\n",
      "0.71 0.07 15.59 GreedySubsetMatrixFactorization(3): memory_mean, exec_syscalls_per_sec, other_syscalls_per_sec\n",
      "0.72 0.08 15.57 GreedySubsetMatrixFactorization(4): memory_mean, exec_syscalls_per_sec, other_syscalls_per_sec, file_syscalls_per_sec\n",
      "0.72 0.08 15.57 GreedySubsetMatrixFactorization(5): memory_mean, exec_syscalls_per_sec, other_syscalls_per_sec, file_syscalls_per_sec, fd_syscalls_per_sec\n",
      "0.68 0.06 15.49 GreedySubsetMatrixFactorization(6): memory_mean, exec_syscalls_per_sec, other_syscalls_per_sec, file_syscalls_per_sec, fd_syscalls_per_sec, n_ops_per_sec\n",
      "0.68 0.06 15.49 GreedySubsetMatrixFactorization(7): memory_mean, exec_syscalls_per_sec, other_syscalls_per_sec, file_syscalls_per_sec, fd_syscalls_per_sec, n_ops_per_sec, socket_syscalls_per_sec\n",
      "0.46 0.11 14.08 GreedySubsetMatrixFactorization(8): memory_mean, exec_syscalls_per_sec, other_syscalls_per_sec, file_syscalls_per_sec, fd_syscalls_per_sec, n_ops_per_sec, socket_syscalls_per_sec, cputime_per_sec\n"
     ]
    }
   ],
   "source": [
    "test_benchmark_predictors([\n",
    "    Regression(),\n",
    "    *[\n",
    "        LowRankMatrixFactorization(i)\n",
    "        for i in range(1, n_features + 1)\n",
    "    ],\n",
    "    *[\n",
    "        GreedySubsetMatrixFactorization(i)\n",
    "        for i in range(1, n_features + 1)\n",
    "    ],\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a896cd-1ced-45e2-b757-428c6189047e",
   "metadata": {},
   "source": [
    "# What about new-system-and-benchmark?\n",
    "\n",
    "- **Given** integer N and workloads x benchmark matrix\n",
    "- **Select** N workloads\n",
    "- **Given** new system's log slowdown ratio on N selected workloads and features of new workload\n",
    "- **Predict** new system's log slowdown ratio on new workload\n",
    "\n",
    "Not implemented yet. Maybe won't be ever. Who would be picking a new system and new benchmark at the same time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "73e6be74-ff13-44a2-b5d6-5f9178826a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore this clase\n",
    "\n",
    "class NewSystemAndBenchmarkProblem(abc.ABC):\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def select_benchmarks(\n",
    "        self,\n",
    "        k: int,\n",
    "        systems_by_benchmarks: numpy.typing.NDArray,\n",
    "        benchmarks_by_features: numpy.typing.NDArray,\n",
    "    ) -> tuple[list[int], numpy.float64]:\n",
    "        \"\"\"\n",
    "        k: number of benchmarks to select\n",
    "        systems_by_benchmarks: array where the (i,j)th element is the log of the ith system's slowdown on the jth benchmark\n",
    "        benchmarks_by_features: array where the (j,m)th element is the mth feature of the jth benchmark\n",
    "\n",
    "\n",
    "        returns a tuple containing:\n",
    "          - k benchmarks to select\n",
    "          - the log-liklihood\n",
    "        \n",
    "        Liklihood is the probability of observing this data given the parameters you inferred\n",
    "        Used to compute the Akaike Information Criterion.\n",
    "        Return numpy.NaN if you just don't care.     \n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def n_parameters(self) -> int:\n",
    "        \"\"\"\n",
    "        Returns the number of parameters used to make this estimation.\n",
    "        Used to calculate the Akaike Information Criterion.\n",
    "        \"\"\"\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def predict_new_system_on_new_benchmarks(\n",
    "        self,\n",
    "        new_system_by_selected_benchmarks: numpy.typing.NDArray,\n",
    "        selected_benchmarks_by_features: numpy.typing.NDArray,\n",
    "        new_benchmarks_by_features: numpy.typing.NDArray,\n",
    "    ) -> numpy.typing.NDArray:\n",
    "        \"\"\"\n",
    "        new_system_by_selected_benchmarks: array where the qth element is the log of the new system's slowdown on the qth selected benchmark\n",
    "        selected_benchmarks_by_features: array where the (q,m)th element is the mth feature of the qth selected benchmark\n",
    "        new_benchmarks_by_features: array where the (p,m)th element is the mth feature of the pth new benchmark\n",
    "\n",
    "        returns an array where the pth element is the log slowdown of the new system on the pth new benchmark\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4dc341b-e2e0-4762-9494-3690ce6c20dd",
   "metadata": {},
   "source": [
    "```bash\n",
    "mkdir stage\n",
    "cd stage\n",
    "cp ../cross-val-no-data.ipynb .\n",
    "cp ../output/{systems,benchmarks,collectors}.txt\n",
    "cp ../output/{systems_by_benchmarks,benchmarks_by_features}.npy .\n",
    "cd ..\n",
    "tar czvf stage.tar.gz stage\n",
    "gsutil cp stage.tar.gz gs://data234\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027c5396-458d-43e0-a9fc-49b9ed15adaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
