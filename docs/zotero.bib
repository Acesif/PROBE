
@inproceedings{gamblin_spack_2015,
	location = {New York, {NY}, {USA}},
	title = {The Spack package manager: bringing order to {HPC} software chaos},
	isbn = {978-1-4503-3723-6},
	url = {https://doi.org/10.1145/2807591.2807623},
	doi = {10.1145/2807591.2807623},
	series = {{SC} '15},
	shorttitle = {The Spack package manager},
	abstract = {Large {HPC} centers spend considerable time supporting software for thousands of users, but the complexity of {HPC} software is quickly outpacing the capabilities of existing software management tools. Scientific applications require specific versions of compilers, {MPI}, and other dependency libraries, so using a single, standard software stack is infeasible. However, managing many configurations is difficult because the configuration space is combinatorial in size. We introduce Spack, a tool used at Lawrence Livermore National Laboratory to manage this complexity. Spack provides a novel, recursive specification syntax to invoke parametric builds of packages and dependencies. It allows any number of builds to coexist on the same system, and it ensures that installed packages can find their dependencies, regardless of the environment. We show through real-world use cases that Spack supports diverse and demanding applications, bringing order to {HPC} software chaos.},
	pages = {1--12},
	booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
	publisher = {Association for Computing Machinery},
	author = {Gamblin, Todd and {LeGendre}, Matthew and Collette, Michael R. and Lee, Gregory L. and Moody, Adam and de Supinski, Bronis R. and Futral, Scott},
	urldate = {2022-04-10},
	date = {2015-11-15},
	note = {interest: 80},
	keywords = {research software engineering, high-performance computing, package managers, operating systems, reproducibility engineering, project-acm-rep, project-astrophysics, project-provenance-pp},
	file = {Full Text PDF:/home/sam/Zotero/storage/7RMEVM2B/Gamblin et al. - 2015 - The Spack package manager bringing order to HPC s.pdf:application/pdf},
}

@article{collberg_repeatability_2016,
	title = {Repeatability in computer systems research},
	volume = {59},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/2812803},
	doi = {10.1145/2812803},
	abstract = {To encourage repeatable research, fund repeatability engineering and reward commitments to sharing research artifacts.},
	pages = {62--69},
	number = {3},
	journaltitle = {Commun. {ACM}},
	author = {Collberg, Christian and Proebsting, Todd A.},
	urldate = {2022-05-27},
	date = {2016-02-25},
	langid = {english},
	keywords = {research software engineering, internship-project, project-acm-rep, project-provenance-pp},
	file = {2812803.pdf:/home/sam/Zotero/storage/JGDDR733/2812803.pdf:application/pdf},
}

@inproceedings{zhao_why_2012,
	location = {Chicago, {IL}},
	title = {Why workflows break — understanding and combating decay in Taverna workflows},
	url = {https://www.research.manchester.ac.uk/portal/en/publications/why-workflows-break--understanding-and-combating-decay-in-taverna-workflows(cba81ca4-e92c-408e-8442-383d1f15fcdf)/export.html},
	doi = {10.1109/eScience.2012.6404482},
	abstract = {Workflows provide a popular means for preserving scientific methods by explicitly encoding their process. However, some of them are subject to a decay in their ability to be re-executed or reproduce the same results over time, largely due to the volatility of the resources required for workflow executions. This paper provides an analysis of the root causes of workflow decay based on an empirical study of a collection of Taverna workflows from the {myExperiment} repository. Although our analysis was based on a specific type of workflow, the outcomes and methodology should be applicable to workflows from other systems, at least those whose executions also rely largely on accessing third-party resources. Based on our understanding about decay we recommend a minimal set of auxiliary resources to be preserved together with the workflows as an aggregation object and provide a software tool for end-users to create such aggregations and to assess their completeness},
	eventtitle = {2012 {IEEE} 8th International Conference on E-Science (e-Science)},
	pages = {9},
	booktitle = {2012 {IEEE} 8th International Conference on E-Science (e-Science)},
	publisher = {{IEEE}},
	author = {Zhao, Jun and Gomez-Perez, Jose-Manuel and Belhajjame, Khalid and Klyne, Graham and Garcia-cuesta, Esteban and Garrido, Aleix and Hettne, Kristina and Roos, Marco and De Roure, David and Goble, Carole},
	date = {2012-10},
	keywords = {workflow managers, internship-project, project-acm-rep, project-provenance-pp},
	file = {Why_workflows_break__Understanding_and_combating_decay_in_Taverna_workflows.pdf:/home/sam/Zotero/storage/2BQSSKJF/Why_workflows_break__Understanding_and_combating_decay_in_Taverna_workflows.pdf:application/pdf},
}

@report{collberg_repeatability_2015,
	title = {Repeatability and Benefaction in Computer Systems Research—A Study and a Modest Proposal},
	url = {http://repeatability.cs.arizona.edu/v2/RepeatabilityTR.pdf},
	abstract = {We describe a study into the extent to which Computer Systems researchers share their code and data and the extent to which such code builds. Starting with 601 papers from {ACM} conferences and journals, we examine 402 papers whose results were backed by code. For 32.3\% of these papers we were able to obtain the code and build it within 30 minutes; for 48.3\% of the
papers we managed to build the code, but it may have required extra effort; for 54.0\% of the papers either we managed to build the code or the authors stated the code would build with reasonable effort. We also propose a novel sharing specification scheme that requires researchers to specify the level of sharing that reviewers and readers can assume from a paper.},
	number = {14-04},
	institution = {University of Arizona},
	author = {Collberg, Christian and Proebsting, Todd and Warren, Alex M},
	date = {2015-02-27},
	keywords = {reproducibility engineering, project-provenance-pp},
	file = {RepeatabilityTR.pdf:/home/sam/Zotero/storage/K9CA8GSQ/RepeatabilityTR.pdf:application/pdf},
}

@article{beyer_reliable_2019,
	title = {Reliable benchmarking: requirements and solutions},
	volume = {21},
	issn = {1433-2779, 1433-2787},
	url = {https://link.springer.com/10.1007/s10009-017-0469-y},
	doi = {10.1007/s10009-017-0469-y},
	shorttitle = {Reliable benchmarking},
	abstract = {Benchmarking is a widely used method in experimental computer science, in particular, for the comparative evaluation of tools and algorithms. As a consequence, a number of questions need to be answered in order to ensure proper benchmarking, resource measurement, and presentation of results, all of which is essential for researchers, tool developers, and users, as well as for tool competitions. We identify a set of requirements that are indispensable for reliable benchmarking and resource measurement of time and memory usage of automatic solvers, verifiers, and similar tools, and discuss limitations of existing methods and benchmarking tools. Fulfilling these requirements in a benchmarking framework can (on Linux systems) currently only be done by using the cgroup and namespace features of the kernel. We developed {BenchExec}, a ready-to-use, tool-independent, and open-source implementation of a benchmarking framework that fulfills all presented requirements, making reliable benchmarking and resource measurement easy. Our framework is able to work with a wide range of different tools, has proven its reliability and usefulness in the International Competition on Software Verification, and is used by several research groups worldwide to ensure reliable benchmarking. Finally, we present guidelines on how to present measurement results in a scientifically valid and comprehensible way.},
	pages = {1--29},
	number = {1},
	journaltitle = {Int J Softw Tools Technol Transfer},
	author = {Beyer, Dirk and Löwe, Stefan and Wendler, Philipp},
	urldate = {2022-06-30},
	date = {2019-02-06},
	langid = {english},
	note = {interest: 90},
	keywords = {software benchmarking, reproducibility engineering, metascience, project-provenance-pp},
	file = {Beyer2019_Article_ReliableBenchmarkingRequiremen.pdf:/home/sam/Zotero/storage/5XXRH3IC/Beyer2019_Article_ReliableBenchmarkingRequiremen.pdf:application/pdf;Current_ReliableBenchmarking.pdf:/home/sam/Zotero/storage/XX3G42I4/Current_ReliableBenchmarking.pdf:application/pdf},
}

@article{freire_provenance_2008,
	title = {Provenance for Computational Tasks: A Survey},
	volume = {10},
	issn = {1521-9615},
	url = {http://ieeexplore.ieee.org/document/4488060/},
	doi = {10.1109/MCSE.2008.79},
	shorttitle = {Provenance for Computational Tasks},
	abstract = {The problem of systematically capturing and managing provenance for computational tasks has recently received significant attention because of its relevance to a wide range of domains and applications. The authors give an overview of important concepts related to provenance management, so that potential users can make informed decisions when selecting or designing a provenance solution.},
	pages = {11--21},
	number = {3},
	journaltitle = {Comput. Sci. Eng.},
	author = {Freire, Juliana and Koop, David and Santos, Emanuele and Silva, Cl},
	urldate = {2022-07-08},
	date = {2008-05},
	note = {interest: 97},
	keywords = {provenance, project-provenance-pp},
}

@article{moreau_special_2008,
	title = {Special Issue: The First Provenance Challenge},
	volume = {20},
	issn = {15320626, 15320634},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/cpe.1233},
	doi = {10.1002/cpe.1233},
	shorttitle = {Special Issue},
	pages = {409--418},
	number = {5},
	journaltitle = {Concurrency Computat.: Pract. Exper.},
	author = {Moreau, Luc and Ludäscher, Bertram and Altintas, Ilkay and Barga, Roger S. and Bowers, Shawn and Callahan, Steven and Chin, George and Clifford, Ben and Cohen, Shirley and Cohen-Boulakia, Sarah and Davidson, Susan and Deelman, Ewa and Digiampietri, Luciano and Foster, Ian and Freire, Juliana and Frew, James and Futrelle, Joe and Gibson, Tara and Gil, Yolanda and Goble, Carole and Golbeck, Jennifer and Groth, Paul and Holland, David A. and Jiang, Sheng and Kim, Jihie and Koop, David and Krenek, Ales and {McPhillips}, Timothy and Mehta, Gaurang and Miles, Simon and Metzger, Dominic and Munroe, Steve and Myers, Jim and Plale, Beth and Podhorszki, Norbert and Ratnakar, Varun and Santos, Emanuele and Scheidegger, Carlos and Schuchardt, Karen and Seltzer, Margo and Simmhan, Yogesh L. and Silva, Claudio and Slaughter, Peter and Stephan, Eric and Stevens, Robert and Turi, Daniele and Vo, Huy and Wilde, Mike and Zhao, Jun and Zhao, Yong},
	urldate = {2022-07-08},
	date = {2008-04-10},
	langid = {english},
	note = {interest: 87},
	keywords = {provenance, project-provenance-pp},
}

@inproceedings{ferreira_da_silva_workflowhub_2020,
	location = {Georgia, {USA}},
	title = {{WorkflowHub}: Community Framework for Enabling Scientific Workflow Research and Development},
	doi = {10.1109/WORKS51914.2020.00012},
	shorttitle = {{WorkflowHub}},
	abstract = {Scientific workflows are a cornerstone of modern scientific computing. They are used to describe complex computational applications that require efficient and robust management of large volumes of data, which are typically stored/processed on heterogeneous, distributed resources. The workflow research and development community has employed a number of methods for the quantitative evaluation of existing and novel workflow algorithms and systems. In particular, a common approach is to simulate workflow executions. In previous work, we have presented a collection of tools that have been used for aiding research and development activities in the Pegasus project, and that have been adopted by others for conducting workflow research. Despite their popularity, there are several shortcomings that prevent easy adoption, maintenance, and consistency with the evolving structures and computational requirements of production workflows. In this work, we present {WorkflowHub}, a community framework that provides a collection of tools for analyzing workflow execution traces, producing realistic synthetic workflow traces, and simulating workflow executions. We demonstrate the realism of the generated synthetic traces by comparing simulated executions of these traces with actual workflow executions. We also contrast these results with those obtained when using the previously available collection of tools. We find that our framework not only can be used to generate representative synthetic workflow traces (i.e., with workflow structures and task characteristics distributions that resemble those in traces obtained from real-world workflow executions), but can also generate representative workflow traces at larger scales than that of available workflow traces.},
	eventtitle = {2020 {IEEE}/{ACM} Workflows in Support of Large-Scale Science ({WORKS})},
	pages = {49--56},
	booktitle = {2020 {IEEE}/{ACM} Workflows in Support of Large-Scale Science ({WORKS})},
	publisher = {{IEEE}},
	author = {Ferreira da Silva, Rafael and Pottier, Loïc and Coleman, Tainã and Deelman, Ewa and Casanova, Henri},
	date = {2020-11},
	keywords = {workflow managers, project-acm-rep, project-provenance-pp},
	file = {IEEE Xplore Abstract Record:/home/sam/Zotero/storage/C33VHWM8/9308170.html:text/html;IEEE Xplore Full Text PDF:/home/sam/Zotero/storage/262NG8DJ/Silva et al. - 2020 - WorkflowHub Community Framework for Enabling Scie.pdf:application/pdf;Submitted Version:/home/sam/Zotero/storage/8UFHN6M6/Silva et al. - 2020 - WorkflowHub Community Framework for Enabling Scie.pdf:application/pdf},
}

@article{trisovic_large-scale_2022,
	title = {A large-scale study on research code quality and execution},
	volume = {9},
	rights = {2022 The Author(s)},
	issn = {2052-4463},
	url = {https://www.nature.com/articles/s41597-022-01143-6},
	doi = {10.1038/s41597-022-01143-6},
	abstract = {This article presents a study on the quality and execution of research code from publicly-available replication datasets at the Harvard Dataverse repository. Research code is typically created by a group of scientists and published together with academic papers to facilitate research transparency and reproducibility. For this study, we define ten questions to address aspects impacting research reproducibility and reuse. First, we retrieve and analyze more than 2000 replication datasets with over 9000 unique R files published from 2010 to 2020. Second, we execute the code in a clean runtime environment to assess its ease of reuse. Common coding errors were identified, and some of them were solved with automatic code cleaning to aid code execution. We find that 74\% of R files failed to complete without error in the initial execution, while 56\% failed when code cleaning was applied, showing that many errors can be prevented with good coding practices. We also analyze the replication datasets from journals’ collections and discuss the impact of the journal policy strictness on the code re-execution rate. Finally, based on our results, we propose a set of recommendations for code dissemination aimed at researchers, journals, and repositories.},
	pages = {60},
	number = {1},
	journaltitle = {Sci Data},
	author = {Trisovic, Ana and Lau, Matthew K. and Pasquier, Thomas and Crosas, Mercè},
	urldate = {2022-12-13},
	date = {2022-02-21},
	langid = {english},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {reproducibility engineering, project-acm-rep, project-provenance-pp},
	file = {Full Text PDF:/home/sam/Zotero/storage/YI4U9WQW/Trisovic et al. - 2022 - A large-scale study on research code quality and e.pdf:application/pdf},
}

@inproceedings{elsner_empirically_2021,
	location = {New York, {NY}, {USA}},
	title = {Empirically evaluating readily available information for regression test optimization in continuous integration},
	isbn = {978-1-4503-8459-9},
	url = {https://doi.org/10.1145/3460319.3464834},
	doi = {10.1145/3460319.3464834},
	series = {{ISSTA} 2021},
	abstract = {Regression test selection ({RTS}) and prioritization ({RTP}) techniques aim to reduce testing efforts and developer feedback time after a change to the code base. Using various information sources, including test traces, build dependencies, version control data, and test histories, they have been shown to be effective. However, not all of these sources are guaranteed to be available and accessible for arbitrary continuous integration ({CI}) environments. In contrast, metadata from version control systems ({VCSs}) and {CI} systems are readily available and inexpensive. Yet, corresponding {RTP} and {RTS} techniques are scattered across research and often only evaluated on synthetic faults or in a specific industrial context. It is cumbersome for practitioners to identify insights that apply to their context, let alone to calibrate associated parameters for maximum cost-effectiveness. This paper consolidates existing work on {RTP} and unsafe {RTS} into an actionable methodology to build and evaluate such approaches that exclusively rely on {CI} and {VCS} metadata. To investigate how these approaches from prior research compare in heterogeneous settings, we apply the methodology in a large-scale empirical study on a set of 23 projects covering 37,000 {CI} logs and 76,000 {VCS} commits. We find that these approaches significantly outperform established {RTP} baselines and, while still triggering 90\% of the failures, we show that practitioners can expect to save on average 84\% of test execution time for unsafe {RTS}. We also find that it can be beneficial to limit training data, features from test history work better than change-based features, and, somewhat surprisingly, simple and well-known heuristics often outperform complex machine-learned models.},
	pages = {491--504},
	booktitle = {Proceedings of the 30th {ACM} {SIGSOFT} International Symposium on Software Testing and Analysis},
	publisher = {Association for Computing Machinery},
	author = {Elsner, Daniel and Hauer, Florian and Pretschner, Alexander and Reimer, Silke},
	urldate = {2023-01-19},
	date = {2021-07-11},
	note = {interest: 99},
	keywords = {continuous integration, project-acm-rep, regression testing, project-provenance-pp},
	file = {Full Text PDF:/home/sam/Zotero/storage/CG4ZZ7MN/Elsner et al. - 2021 - Empirically evaluating readily available informati.pdf:application/pdf},
}

@inproceedings{rougier_rescience_2019,
	location = {Cham},
	title = {{ReScience} C: A Journal for Reproducible Replications in Computational Science},
	isbn = {978-3-030-23987-9},
	doi = {10.1007/978-3-030-23987-9_14},
	series = {Lecture Notes in Computer Science},
	shorttitle = {{ReScience} C},
	abstract = {Independent replication is one of the most powerful methods to verify published scientific studies. In computational science, it requires the reimplementation of the methods described in the original article by a different team of researchers. Replication is often performed by scientists who wish to gain a better understanding of a published method, but its results are rarely made public. {ReScience} C is a peer-reviewed journal dedicated to the publication of high-quality computational replications that provide added value to the scientific community. To this end, {ReScience} C requires replications to be reproducible and implemented using Open Source languages and libraries. In this article, we provide an overview of {ReScience} C’s goals and quality standards, outline the submission and reviewing processes, and summarize the experience of its first three years of operation, concluding with an outlook towards evolutions envisaged for the near future.},
	pages = {150--156},
	booktitle = {Reproducible Research in Pattern Recognition},
	publisher = {Springer International Publishing},
	author = {Rougier, Nicolas P. and Hinsen, Konrad},
	editor = {Kerautret, Bertrand and Colom, Miguel and Lopresti, Daniel and Monasse, Pascal and Talbot, Hugues},
	date = {2019},
	langid = {english},
	keywords = {reproducibility engineering, project-acm-rep, project-provenance-pp},
	file = {Submitted Version:/home/sam/Zotero/storage/3YZGFA62/Rougier and Hinsen - 2019 - ReScience C A Journal for Reproducible Replicatio.pdf:application/pdf},
}

@article{krafczyk_learning_2021,
	title = {Learning from reproducing computational results: introducing three principles and the Reproduction Package},
	volume = {379},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsta.2020.0069},
	doi = {10.1098/rsta.2020.0069},
	shorttitle = {Learning from reproducing computational results},
	abstract = {We carry out efforts to reproduce computational results for seven published articles and identify barriers to computational reproducibility. We then derive three principles to guide the practice and dissemination of reproducible computational research: (i) Provide transparency regarding how computational results are produced; (ii) When writing and releasing research software, aim for ease of (re-)executability; (iii) Make any code upon which the results rely as deterministic as possible. We then exemplify these three principles with 12 specific guidelines for their implementation in practice. We illustrate the three principles of reproducible research with a series of vignettes from our experimental reproducibility work. We define a novel Reproduction Package, a formalism that specifies a structured way to share computational research artifacts that implements the guidelines generated from our reproduction efforts to allow others to build, reproduce and extend computational science. We make our reproduction efforts in this paper publicly available as exemplar Reproduction Packages.

This article is part of the theme issue ‘Reliability and reproducibility in computational science: implementing verification, validation and uncertainty quantification in silico’.},
	pages = {20200069},
	number = {2197},
	journaltitle = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	author = {Krafczyk, M. S. and Shi, A. and Bhaskar, A. and Marinov, D. and Stodden, V.},
	urldate = {2023-01-31},
	date = {2021-03-29},
	note = {Publisher: Royal Society},
	keywords = {reproducibility engineering, project-acm-rep, project-provenance-pp},
	file = {Full Text PDF:/home/sam/Zotero/storage/P3RYMMMA/Krafczyk et al. - 2021 - Learning from reproducing computational results i.pdf:application/pdf},
}

@inproceedings{gomez-perez_when_2013,
	location = {Berlin, Heidelberg},
	title = {When History Matters - Assessing Reliability for the Reuse of Scientific Workflows},
	isbn = {978-3-642-41338-4},
	doi = {10.1007/978-3-642-41338-4_6},
	series = {Lecture Notes in Computer Science},
	abstract = {Scientific workflows play an important role in computational research as essential artifacts for communicating the methods used to produce research findings. We are witnessing a growing number of efforts that treat workflows as first-class artifacts for sharing and exchanging scientific knowledge, either as part of scholarly articles or as stand-alone objects. However, workflows are not born to be reliable, which can seriously damage their reusability and trustworthiness as knowledge exchange instruments. Scientific workflows are commonly subject to decay, which consequently undermines their reliability over their lifetime. The reliability of workflows can be notably improved by advocating scientists to preserve a minimal set of information that is essential to assist the interpretations of these workflows and hence improve their potential for reproducibility and reusability. In this paper we show how, by measuring and monitoring the completeness and stability of scientific workflows over time we are able to provide scientists with a measure of their reliability, supporting the reuse of trustworthy scientific knowledge.},
	pages = {81--97},
	booktitle = {The Semantic Web – {ISWC} 2013},
	publisher = {Springer},
	author = {Gómez-Pérez, José Manuel and García-Cuesta, Esteban and Garrido, Aleix and Ruiz, José Enrique and Zhao, Jun and Klyne, Graham},
	editor = {Alani, Harith and Kagal, Lalana and Fokoue, Achille and Groth, Paul and Biemann, Chris and Parreira, Josiane Xavier and Aroyo, Lora and Noy, Natasha and Welty, Chris and Janowicz, Krzysztof},
	date = {2013},
	langid = {english},
	keywords = {workflow managers, reproducibility, project-acm-rep, project-provenance-pp},
}

@inproceedings{butt_provone_2020,
	location = {Cham},
	title = {{ProvONE}+: A Provenance Model for Scientific Workflows},
	isbn = {978-3-030-62008-0},
	doi = {10.1007/978-3-030-62008-0_30},
	series = {Lecture Notes in Computer Science},
	shorttitle = {{ProvONE}+},
	abstract = {The provenance of workflows is essential, both for the data they derive and for their specification, to allow for the reproducibility, sharing and reuse of information in the scientific community. Although the formal modelling of scientific workflow provenance was of interest and studied, in many fields like semantic web, yet no provenance model has existed, we are aware of, to model control-flow driven scientific workflows. The provenance models proposed by the semantic web community for data-driven scientific workflows may capture the provenance of control-flow driven workflows execution traces (i.e., retrospective provenance) but underspecify the workflow structure (i.e., workflow provenance). An underspecified or incomplete structure of a workflow results in the misinterpretation of a scientific experiment and precludes conformance checking of the workflow, thereby restricting the gains of provenance. To overcome the limitation, we present a formal, lightweight and general-purpose specification model for the control-flows involved scientific workflows. The proposed model can be combined with the existing provenance models and easy to extend to specify the common control-flow patterns. In this article, we inspire the need for control-flow driven scientific workflow provenance model, provide an overview of its key classes and properties, and briefly discuss its integration with the {ProvONE} provenance model as well as its compatibility to {PROV}-{DM}. We will also focus on the sample modelling using the proposed model and present a comprehensive implementation scenario from the agricultural domain for validating the model.},
	pages = {431--444},
	booktitle = {Web Information Systems Engineering – {WISE} 2020},
	publisher = {Springer International Publishing},
	author = {Butt, Anila Sahar and Fitch, Peter},
	editor = {Huang, Zhisheng and Beek, Wouter and Wang, Hua and Zhou, Rui and Zhang, Yanchun},
	date = {2020},
	langid = {english},
	keywords = {semantic web, provenance, project-acm-rep, project-provenance-pp},
	file = {Full Text PDF:/home/sam/Zotero/storage/XIV45RJ9/Butt and Fitch - 2020 - ProvONE+ A Provenance Model for Scientific Workfl.pdf:application/pdf},
}

@article{oliveira_provenance_2018,
	title = {Provenance Analytics for Workflow-Based Computational Experiments: A Survey},
	volume = {51},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/3184900},
	doi = {10.1145/3184900},
	shorttitle = {Provenance Analytics for Workflow-Based Computational Experiments},
	abstract = {Until not long ago, manually capturing and storing provenance from scientific experiments were constant concerns for scientists. With the advent of computational experiments (modeled as scientific workflows) and Scientific Workflow Management Systems, produced and consumed data, as well as the provenance of a given experiment, are automatically managed, so provenance capturing and storing in such a context is no longer a major concern. Similarly to several existing big data problems, the bottom line is now on how to analyze the large amounts of provenance data generated by workflow executions and how to be able to extract useful knowledge of this data. In this context, this article surveys the current state of the art on provenance analytics by presenting the key initiatives that have been taken to support provenance data analysis. We also contribute by proposing a taxonomy to classify elements related to provenance analytics.},
	pages = {53:1--53:25},
	number = {3},
	journaltitle = {{ACM} Comput. Surv.},
	author = {Oliveira, Wellington and Oliveira, Daniel De and Braganholo, Vanessa},
	urldate = {2023-02-23},
	date = {2018-05-23},
	note = {interest: 95},
	keywords = {provenance, project-provenance-pp},
	file = {Full Text PDF:/home/sam/Zotero/storage/IT8P4NF7/Oliveira et al. - 2018 - Provenance Analytics for Workflow-Based Computatio.pdf:application/pdf},
}

@article{howison_retract_2014,
	title = {Retract bit-rotten publications: Aligning incentives for sustaining scientific software},
	url = {https://figshare.com/articles/journal_contribution/Retract_bit_rotten_publications_Aligning_incentives_for_sustaining_scientific_software/1111632/1},
	doi = {10.6084/m9.figshare.1111632.v1},
	shorttitle = {Retract bit-rotten publications},
	abstract = {A provocation for the {WSSSPE}2 workshop},
	author = {Howison, James},
	urldate = {2023-02-23},
	date = {2014-07-20},
	langid = {english},
	note = {Publisher: figshare},
	keywords = {continuous integration, reproducibility engineering, project-provenance-pp},
	file = {Snapshot:/home/sam/Zotero/storage/7VJNSRVB/1111632.html:text/html},
}

@online{noauthor_confirmation_2014,
	title = {Confirmation Depth as a measure of reproducible scientific research.},
	url = {http://davidsoergel.com/posts/confirmation-depth-as-a-measure-of-reproducible-scientific-research},
	abstract = {What does it mean to reproduce a scientific study?  Confirmation Depth provides a guiding principle.},
	titleaddon = {David Soergel},
	urldate = {2023-02-23},
	date = {2014-10-21},
	langid = {english},
	keywords = {project-provenance-pp},
	file = {Snapshot:/home/sam/Zotero/storage/KNRLR2QW/confirmation-depth-as-a-measure-of-reproducible-scientific-research.html:text/html},
}

@misc{mesnard_reproducible_2016,
	title = {Reproducible and replicable {CFD}: it's harder than you think},
	url = {http://arxiv.org/abs/1605.04339},
	doi = {10.48550/arXiv.1605.04339},
	shorttitle = {Reproducible and replicable {CFD}},
	abstract = {Completing a full replication study of our previously published findings on bluff-body aerodynamics was harder than we thought. Despite the fact that we have good reproducible-research practices, sharing our code and data openly. Here's what we learned from three years, four {CFD} codes and hundreds of runs.},
	number = {{arXiv}:1605.04339},
	publisher = {{arXiv}},
	author = {Mesnard, Olivier and Barba, Lorena A.},
	urldate = {2023-02-23},
	date = {2016-10-14},
	eprinttype = {arxiv},
	eprint = {1605.04339 [physics]},
	note = {interest: 94},
	keywords = {reproducibility engineering, project-provenance-pp},
	file = {arXiv Fulltext PDF:/home/sam/Zotero/storage/4C9ZRCHY/Mesnard and Barba - 2016 - Reproducible and replicable CFD it's harder than .pdf:application/pdf;arXiv.org Snapshot:/home/sam/Zotero/storage/7KT47NJL/1605.html:text/html},
}

@article{timperley_understanding_2021,
	title = {Understanding and Improving Artifact Sharing in Software Engineering Research},
	volume = {26},
	issn = {1382-3256, 1573-7616},
	url = {http://arxiv.org/abs/2008.01046},
	doi = {10.1007/s10664-021-09973-5},
	abstract = {In recent years, many software engineering researchers have begun to include artifacts alongside their research papers. Ideally, artifacts, including tools, benchmarks, and data, support the dissemination of ideas, provide evidence for research claims, and serve as a starting point for future research. However, in practice, artifacts suffer from a variety of issues that prevent the realization of their full potential. To help the software engineering community realize the full potential of artifacts, we seek to understand the challenges involved in the creation, sharing, and use of artifacts. To that end, we perform a mixed-methods study including a survey of artifacts in software engineering publications, and an online survey of 153 software engineering researchers. By analyzing the perspectives of artifact creators, users, and reviewers, we identify several high-level challenges that affect the quality of artifacts including mismatched expectations between these groups, and a lack of sufficient reward for both creators and reviewers. Using Diffusion of Innovations as an analytical framework, we examine how these challenges relate to one another, and build an understanding of the factors that affect the sharing and success of artifacts. Finally, we make recommendations to improve the quality of artifacts based on our results and existing best practices.},
	pages = {67},
	number = {4},
	journaltitle = {Empir Software Eng},
	author = {Timperley, Christopher S. and Herckis, Lauren and Goues, Claire Le and Hilton, Michael},
	urldate = {2023-05-06},
	date = {2021-07},
	eprinttype = {arxiv},
	eprint = {2008.01046 [cs]},
	note = {interest: 90},
	keywords = {research software engineering, reproducibility engineering, project-provenance-pp, artifact evaluation},
	file = {arXiv Fulltext PDF:/home/sam/Zotero/storage/BM7CHTK8/Timperley et al. - 2021 - Understanding and Improving Artifact Sharing in So.pdf:application/pdf;arXiv.org Snapshot:/home/sam/Zotero/storage/ID3NVV6E/2008.html:text/html},
}

@article{constantin_document_2016,
	title = {The Document Components Ontology ({DoCO})},
	volume = {7},
	issn = {1570-0844},
	url = {https://content.iospress.com/articles/semantic-web/sw177},
	doi = {10.3233/SW-150177},
	abstract = {The availability in machine-readable form of descriptions of the structure of documents, as well as of the document discourse (e.g. the scientific discourse within scholarly articles), is crucial for facilitating semantic publishing and the overall c},
	pages = {167--181},
	number = {2},
	journaltitle = {Semantic Web},
	author = {Constantin, Alexandru and Peroni, Silvio and Pettifer, Steve and Shotton, David and Vitali, Fabio},
	urldate = {2023-05-25},
	date = {2016-01-01},
	langid = {english},
	note = {Publisher: {IOS} Press},
	keywords = {semantic web, project-provenance-pp},
	file = {Full Text PDF:/home/sam/Zotero/storage/AK4CAYTZ/Constantin et al. - 2016 - The&nbsp\;Document&nbsp\;Components&nbsp\;Ontology&nb.pdf:application/pdf},
}

@article{shotton_cito_2010,
	title = {{CiTO}, the Citation Typing Ontology},
	volume = {1},
	issn = {2041-1480},
	url = {https://doi.org/10.1186/2041-1480-1-S1-S6},
	doi = {10.1186/2041-1480-1-S1-S6},
	abstract = {{CiTO}, the Citation Typing Ontology, is an ontology for describing the nature of reference citations in scientific research articles and other scholarly works, both to other such publications and also to Web information resources, and for publishing these descriptions on the Semantic Web. Citation are described in terms of the factual and rhetorical relationships between citing publication and cited publication, the in-text and global citation frequencies of each cited work, and the nature of the cited work itself, including its publication and peer review status. This paper describes {CiTO} and illustrates its usefulness both for the annotation of bibliographic reference lists and for the visualization of citation networks. The latest version of {CiTO}, which this paper describes, is {CiTO} Version 1.6, published on 19 March 2010. {CiTO} is written in the Web Ontology Language {OWL}, uses the namespace http://purl.org/net/cito/, and is available from http://purl.org/net/cito/. This site uses content negotiation to deliver to the user an {OWLDoc} Web version of the ontology if accessed via a Web browser, or the {OWL} ontology itself if accessed from an ontology management tool such as Protégé 4 (http://protege.stanford.edu/). Collaborative work is currently under way to harmonize {CiTO} with other ontologies describing bibliographies and the rhetorical structure of scientific discourse.},
	pages = {S6},
	number = {1},
	journaltitle = {J Biomed Semant},
	author = {Shotton, David},
	urldate = {2023-05-25},
	date = {2010-06-22},
	langid = {english},
	keywords = {semantic web, project-provenance-pp},
	file = {Full Text PDF:/home/sam/Zotero/storage/NBZPC9S5/Shotton - 2010 - CiTO, the Citation Typing Ontology.pdf:application/pdf},
}

@software{grande_nf-prov_2023,
	title = {nf-prov},
	rights = {Apache-2.0},
	url = {https://github.com/Sage-Bionetworks-Workflows/nf-prov},
	publisher = {Sage-Bionetworks-Workflows},
	author = {Grande, Bruno and Sherman, Ben and Di Tomasso, Paolo},
	urldate = {2023-05-25},
	date = {2023-05-07},
	note = {original-date: 2022-12-19T21:16:30Z},
	keywords = {provenance, project-provenance-pp},
}

@inproceedings{gruber_empirical_2021,
	title = {An Empirical Study of Flaky Tests in Python},
	doi = {10.1109/ICST49551.2021.00026},
	abstract = {Tests that cause spurious failures without any code changes, i.e., flaky tests, hamper regression testing, increase maintenance costs, may shadow real bugs, and decrease trust in tests. While the prevalence and importance of flakiness is well established, prior research focused on Java projects, thus raising the question of how the findings generalize. In order to provide a better understanding of the role of flakiness in software development beyond Java, we empirically study the prevalence, causes, and degree of flakiness within software written in Python, one of the currently most popular programming languages. For this, we sampled 22 352 open source projects from the popular {PyPI} package index, and analyzed their 876 186 test cases for flakiness. Our investigation suggests that flakiness is equally prevalent in Python as it is in Java. The reasons, however, are different: Order dependency is a much more dominant problem in Python, causing 59 \% of the 7 571 flaky tests in our dataset. Another 28 \% were caused by test infrastructure problems, which represent a previously undocumented cause of flakiness. The remaining 13 \% can mostly be attributed to the use of network and randomness {APIs} by the projects, which is indicative of the type of software commonly written in Python. Our data also suggests that finding flaky tests requires more runs than are often done in the literature: A 95 \% confidence that a passing test case is not flaky on average would require 170 reruns.},
	eventtitle = {2021 14th {IEEE} Conference on Software Testing, Verification and Validation ({ICST})},
	pages = {148--158},
	booktitle = {2021 14th {IEEE} Conference on Software Testing, Verification and Validation ({ICST})},
	author = {Gruber, Martin and Lukasczyk, Stephan and Kroiß, Florian and Fraser, Gordon},
	date = {2021-04},
	note = {{ISSN}: 2159-4848},
	keywords = {software testing, project-provenance-pp, software mining},
	file = {IEEE Xplore Abstract Record:/home/sam/Zotero/storage/ZM52F9IF/9438576.html:text/html;IEEE Xplore Full Text PDF:/home/sam/Zotero/storage/A2WGSNPH/Gruber et al. - 2021 - An Empirical Study of Flaky Tests in Python.pdf:application/pdf},
}

@article{weibel_dublin_2000,
	title = {The Dublin Core Metadata Initiative: Mission, Current Activities, and Future Directions},
	volume = {6},
	issn = {1082-9873},
	url = {http://www.dlib.org/dlib/december00/weibel/12weibel.html},
	doi = {10.1045/december2000-weibel},
	shorttitle = {The Dublin Core Metadata Initiative},
	number = {12},
	journaltitle = {D-Lib Magazine},
	author = {Weibel, Stuart L. and Koch, Traugott},
	urldate = {2023-05-25},
	date = {2000-12},
	langid = {english},
	keywords = {semantic web, project-provenance-pp},
}

@online{gandon_rdf_2014,
	title = {{RDF} 1.1 {XML} Syntax},
	url = {https://www.w3.org/TR/rdf-syntax-grammar/#section-Syntax-blank-nodes},
	abstract = {This document defines an {XML} syntax for {RDF} called {RDF}/{XML} in terms of Namespaces in {XML}, the {XML} Information Set and {XML} Base.},
	titleaddon = {W3C Standards},
	author = {Gandon, Fabian and Shcreiber, Guus and Beckett, David},
	urldate = {2023-05-26},
	date = {2014-02-25},
	keywords = {semantic web, project-provenance-pp},
	file = {RDF 1.1 XML Syntax:/home/sam/Zotero/storage/XP5APX3P/rdf-syntax-grammar.html:text/html},
}

@article{groth_anatomy_2010,
	title = {The anatomy of a nanopublication},
	volume = {30},
	issn = {0167-5265},
	url = {https://content.iospress.com/articles/information-services-and-use/isu613},
	doi = {10.3233/ISU-2010-0613},
	abstract = {As the amount of scholarly communication increases, it is increasingly difficult for specific core scientific statements to be found, connected and curated. Additionally, the redundancy of these statements in multiple fora makes it difficult to deter},
	pages = {51--56},
	number = {1},
	journaltitle = {Information Services \& Use},
	author = {Groth, Paul and Gibson, Andrew and Velterop, Jan},
	urldate = {2023-05-26},
	date = {2010-01-01},
	langid = {english},
	note = {Publisher: {IOS} Press},
	keywords = {semantic web, academic publishing, project-provenance-pp},
	file = {Full Text PDF:/home/sam/Zotero/storage/LLFTU3XH/Groth et al. - 2010 - The anatomy of a nanopublication.pdf:application/pdf},
}

@inproceedings{erxleben_introducing_2014,
	location = {Cham},
	title = {Introducing Wikidata to the Linked Data Web},
	isbn = {978-3-319-11964-9},
	doi = {10.1007/978-3-319-11964-9_4},
	series = {Lecture Notes in Computer Science},
	abstract = {Wikidata is the central data management platform of Wikipedia. By the efforts of thousands of volunteers, the project has produced a large, open knowledge base with many interesting applications. The data is highly interlinked and connected to many other datasets, but it is also very rich, complex, and not available in {RDF}. To address this issue, we introduce new {RDF} exports that connect Wikidata to the Linked Data Web. We explain the data model of Wikidata and discuss its encoding in {RDF}. Moreover, we introduce several partial exports that provide more selective or simplified views on the data. This includes a class hierarchy and several other types of ontological axioms that we extract from the site. All datasets we discuss here are freely available online and updated regularly.},
	pages = {50--65},
	booktitle = {The Semantic Web – {ISWC} 2014},
	publisher = {Springer International Publishing},
	author = {Erxleben, Fredo and Günther, Michael and Krötzsch, Markus and Mendez, Julian and Vrandečić, Denny},
	editor = {Mika, Peter and Tudorache, Tania and Bernstein, Abraham and Welty, Chris and Knoblock, Craig and Vrandečić, Denny and Groth, Paul and Noy, Natasha and Janowicz, Krzysztof and Goble, Carole},
	date = {2014},
	langid = {english},
	keywords = {semantic web, project-provenance-pp},
	file = {Full Text PDF:/home/sam/Zotero/storage/QZACD8IN/Erxleben et al. - 2014 - Introducing Wikidata to the Linked Data Web.pdf:application/pdf},
}

@article{soiland-reyes_wf4ever_2013,
	title = {Wf4Ever Research Object Model},
	rights = {Creative Commons Attribution 4.0, Open Access},
	url = {https://zenodo.org/record/12744},
	doi = {10.5281/ZENODO.12744},
	abstract = {The Wf4Ever Research Object Model provides a vocabulary for the description of workflow-centric Research Objects: aggregations of resources relating to scientific workflows.

{\textless}strong{\textgreater}Permalink{\textless}/strong{\textgreater}: https://w3id.org/ro/2013-11-30/},
	author = {Soiland-Reyes, Stian and Bechhofer, Sean and Belhajjame, Khalid and Klyne, Graham and Garijo, Daniel and Coricho, Oscar and García Cuesta, Esteban and Palma, Raul},
	urldate = {2023-05-26},
	date = {2013-11-30},
	note = {Publisher: Zenodo},
	keywords = {semantic web, provenance, project-provenance-pp},
}

@inproceedings{garijo_new_2011,
	location = {New York, {NY}, {USA}},
	title = {A new approach for publishing workflows: abstractions, standards, and linked data},
	isbn = {978-1-4503-1100-7},
	url = {https://dl.acm.org/doi/10.1145/2110497.2110504},
	doi = {10.1145/2110497.2110504},
	series = {{WORKS} '11},
	shorttitle = {A new approach for publishing workflows},
	abstract = {In recent years, a variety of systems have been developed that export the workflows used to analyze data and make them part of published articles. We argue that the workflows that are published in current approaches are dependent on the specific codes used for execution, the specific workflow system used, and the specific workflow catalogs where they are published. In this paper, we describe a new approach that addresses these shortcomings and makes workflows more reusable through: 1) the use of abstract workflows to complement executable workflows to make them reusable when the execution environment is different, 2) the publication of both abstract and executable workflows using standards such as the Open Provenance Model that can be imported by other workflow systems, 3) the publication of workflows as Linked Data that results in open web accessible workflow repositories. We illustrate this approach using a complex workflow that we re-created from an influential publication that describes the generation of 'drugomes'.},
	pages = {47--56},
	booktitle = {Proceedings of the 6th workshop on Workflows in support of large-scale science},
	publisher = {Association for Computing Machinery},
	author = {Garijo, Daniel and Gil, Yolanda},
	urldate = {2023-05-26},
	date = {2011-11-14},
	keywords = {workflow managers, semantic web, project-provenance-pp},
	file = {Full Text PDF:/home/sam/Zotero/storage/5FU8H8X6/Garijo and Gil - 2011 - A new approach for publishing workflows abstracti.pdf:application/pdf},
}

@article{soiland-reyes_packaging_2022,
	title = {Packaging research artefacts with {RO}-Crate},
	volume = {5},
	issn = {2451-8484},
	url = {https://content.iospress.com/articles/data-science/ds210053},
	doi = {10.3233/DS-210053},
	abstract = {An increasing number of researchers support reproducibility by including pointers to and descriptions of datasets, software and methods in their publications. However, scientific articles may be ambiguous, incomplete and difficult to process by autom},
	pages = {97--138},
	number = {2},
	journaltitle = {Data Science},
	author = {Soiland-Reyes, Stian and Sefton, Peter and Crosas, Mercè and Castro, Leyla Jael and Coppens, Frederik and Fernández, José M. and Garijo, Daniel and Grüning, Björn and La Rosa, Marco and Leo, Simone and Ó Carragáin, Eoghan and Portier, Marc and Trisovic, Ana and {RO}-Crate Community and Groth, Paul and Goble, Carole},
	urldate = {2023-05-26},
	date = {2022-01-01},
	langid = {english},
	note = {Publisher: {IOS} Press
interest: 99},
	keywords = {reproducibility engineering, project-acm-rep, project-provenance-pp},
	file = {Full Text PDF:/home/sam/Zotero/storage/YHYB8T2Y/Soiland-Reyes et al. - 2022 - Packaging research artefacts with RO-Crate.pdf:application/pdf},
}

@inproceedings{gray_bioschemas_2017,
	title = {Bioschemas: From Potato Salad to Protein Annotation},
	url = {https://research.manchester.ac.uk/en/publications/bioschemas-from-potato-salad-to-protein-annotation},
	shorttitle = {Bioschemas},
	eventtitle = {The 16th International Semantic Web Conference 2017},
	booktitle = {{ISWC} 2017 Posters \& Demonstrations and Industry Tracks: Proceedings of the {ISWC} 2017 Posters \& Demonstrations and Industry Tracks co-located with 16th International Semantic Web Conference ({ISWC} 2017)},
	publisher = {{RWTH} Aachen University},
	author = {Gray, Alasdair J. G. and Goble, Carole and Jimenez, Rafael C.},
	urldate = {2023-05-26},
	date = {2017-10-22},
	keywords = {semantic web, project-provenance-pp},
	file = {Full Text PDF:/home/sam/Zotero/storage/XU99KNK2/Gray et al. - 2017 - Bioschemas From Potato Salad to Protein Annotatio.pdf:application/pdf},
}

@online{wilder-james_description_2017,
	title = {Description of a Project wiki},
	url = {https://github.com/ewilderj/doap/wiki},
	abstract = {{DOAP} is a project to create an {XML}/{RDF} vocabulary to describe software projects, and in particular open source projects.

In addition to developing an {RDF} schema and examples, the {DOAP} project aims to provide tool support in all the popular programming languages.},
	author = {Wilder-James, Edd},
	urldate = {2023-05-26},
	date = {2017-01-13},
	keywords = {semantic web, project-provenance-pp},
	file = {Home · ewilderj/doap Wiki:/home/sam/Zotero/storage/6H2FHPPR/wiki.html:text/html},
}

@unpublished{grayson_automatic_2023,
	location = {{ACM} {REP}},
	title = {Automatic Reproduction of Workflows in the Snakemake Workflow Catalog and nf-core Registries},
	url = {https://github.com/charmoniumQ/wf-reg-test/blob/main/docs/reports/Understanding_the_results_of_automatic_reproduction_of_workflows_in_nf_core_and_Snakemake_Workflow_Catalog.pdf},
	abstract = {Workflows make it easier for scientists to assemble computational experiments consisting of many disparate components. However, those disparate components also increase the probability that the computational experiment fails to be reproducible. Even if software is reproducible today, it may become irreproducible tomorrow without the software changing at all, because of the constantly changing software environment in which the software is run.

To alleviate irreproducibility, workflow engines integrate with container engines. Additionally, communities that sprung up around workflow engines started to host registries for workflows that follow standards. These standards reduce the effort needed to make workflows automatically reproducible.

We study automatically reproducing workflows from two registries, focusing on non-crashing executions. The experimental data lets us analyze the upper bound to which workflow engines achieve reproducibility. We identify lessons learned in achieving reproducibility in practice.},
	type = {Accepted; awaiting publication},
	howpublished = {Accepted; awaiting publication},
	author = {Grayson, Samuel and Milewicz, Reed and Marinov, Darko and Katz, Daniel S.},
	date = {2023-06-27},
	keywords = {reproducibility engineering, project-provenance-pp},
}

@inproceedings{ton_that_sciunits_2017,
	title = {Sciunits: Reusable Research Objects},
	doi = {10.1109/eScience.2017.51},
	shorttitle = {Sciunits},
	abstract = {Science is conducted collaboratively, often requiring knowledge sharing about computational experiments. When experiments include only datasets, they can be shared using Uniform Resource Identifiers ({URIs}) or Digital Object Identifiers ({DOIs}). An experiment, however, seldom includes only datasets, but more often includes software, its past execution, provenance, and associated documentation. The Research Object has recently emerged as a comprehensive and systematic method for aggregation and identification of diverse elements of computational experiments. While a necessary method, mere aggregation is not sufficient for the sharing of computational experiments. Other users must be able to easily recompute on these shared research objects. In this paper, we present the sciunit, a reusable research object in which aggregated content is recomputable. We describe a Git-like client that efficiently creates, stores, and repeats sciunits. We show through analysis that sciunits repeat computational experiments with minimal storage and processing overhead. Finally, we provide an overview of sharing and reproducible cyberinfrastructure based on sciunits gaining adoption in the domain of geosciences.},
	eventtitle = {2017 {IEEE} 13th International Conference on e-Science (e-Science)},
	pages = {374--383},
	booktitle = {2017 {IEEE} 13th International Conference on e-Science (e-Science)},
	author = {Ton That, Dai Hai and Fils, Gabriel and Yuan, Zhihao and Malik, Tanu},
	date = {2017-10},
	keywords = {provenance, project-provenance-pp},
	file = {IEEE Xplore Abstract Record:/home/sam/Zotero/storage/ILYKBCUH/8109156.html:text/html;IEEE Xplore Full Text PDF:/home/sam/Zotero/storage/4RA2L32H/Ton That et al. - 2017 - Sciunits Reusable Research Objects.pdf:application/pdf},
}

@inproceedings{balakrishnan_opus_2013,
	title = {\{{OPUS}\}: A Lightweight System for Observational Provenance in User Space},
	url = {https://www.usenix.org/conference/tapp13/technical-sessions/presentation/balakrishnan},
	shorttitle = {{OPUS}},
	eventtitle = {5th {USENIX} Workshop on the Theory and Practice of Provenance ({TaPP} 13)},
	author = {Balakrishnan, Nikilesh and Bytheway, Thomas and Sohan, Ripduman and Hopper, Andy},
	urldate = {2023-07-06},
	date = {2013},
	langid = {english},
	keywords = {provenance, project-provenance-pp, provenance-tool},
	file = {Full Text PDF:/home/sam/Zotero/storage/APTAJA64/Balakrishnan et al. - 2013 - OPUS A Lightweight System for Observational Pro.pdf:application/pdf},
}

@inproceedings{muniswamy-reddy_layering_2009,
	location = {{USA}},
	title = {Layering in provenance systems},
	doi = {10.5555/1855807.1855817},
	series = {{USENIX}'09},
	abstract = {Digital provenance describes the ancestry or history of a digital object. Most existing provenance systems, however, operate at only one level of abstraction: the system call layer, a workflow specification, or the high-level constructs of a particular application. The provenance collectable in each of these layers is different, and all of it can be important. Single-layer systems fail to account for the different levels of abstraction at which users need to reason about their data and processes. These systems cannot integrate data provenance across layers and cannot answer questions that require an integrated view of the provenance. We have designed a provenance collection structure facilitating the integration of provenance across multiple levels of abstraction, including a workflow engine, a web browser, and an initial runtime Python provenance tracking wrapper. We layer these components atop provenance-aware network storage ({NFS}) that builds upon a Provenance-Aware Storage System ({PASS}). We discuss the challenges of building systems that integrate provenance across multiple layers of abstraction, present how we augmented systems in each layer to integrate provenance, and present use cases that demonstrate how provenance spanning multiple layers provides functionality not available in existing systems. Our evaluation shows that the overheads imposed by layering provenance systems are reasonable.},
	pages = {10},
	booktitle = {Proceedings of the 2009 conference on {USENIX} Annual technical conference},
	publisher = {{USENIX} Association},
	author = {Muniswamy-Reddy, Kiran-Kumar and Braun, Uri and Holland, David A. and Macko, Peter and Maclean, Diana and Margo, Daniel and Seltzer, Margo and Smogor, Robin},
	urldate = {2023-07-18},
	date = {2009-06-14},
	langid = {english},
	keywords = {provenance, project-provenance-pp, provenance-tool},
	file = {Muniswamy-Reddy et al. - Layering in Provenance Systems.html:/home/sam/Zotero/storage/EPL6YLK6/Muniswamy-Reddy et al. - Layering in Provenance Systems.html:text/html;Muniswamy-Reddy et al. - Layering in Provenance Systems.pdf:/home/sam/Zotero/storage/XPIHNXFW/Muniswamy-Reddy et al. - Layering in Provenance Systems.pdf:application/pdf;Muniswamy-Reddy et al. - Layering in Provenance Systems.pdf:/home/sam/Zotero/storage/GUGJKZNA/Muniswamy-Reddy et al. - Layering in Provenance Systems.pdf:application/pdf},
}

@inproceedings{vangoor_fuse_2017,
	title = {To \{{FUSE}\} or Not to \{{FUSE}\}: Performance of \{User-Space\} File Systems},
	isbn = {978-1-931971-36-2},
	url = {https://www.usenix.org/conference/fast17/technical-sessions/presentation/vangoor},
	shorttitle = {To \{{FUSE}\} or Not to \{{FUSE}\}},
	eventtitle = {15th {USENIX} Conference on File and Storage Technologies ({FAST} 17)},
	pages = {59--72},
	author = {Vangoor, Bharath Kumar Reddy and Tarasov, Vasily and Zadok, Erez},
	urldate = {2023-07-25},
	date = {2017},
	langid = {english},
	keywords = {filesystems, operating systems, project-provenance-pp},
	file = {fast17_slides_vangoor.pdf:/home/sam/Zotero/storage/5RCSUA7B/fast17_slides_vangoor.pdf:application/pdf;Full Text PDF:/home/sam/Zotero/storage/3N3VXAWI/Vangoor et al. - 2017 - To FUSE or Not to FUSE Performance of User-S.pdf:application/pdf},
}

@inproceedings{zhao_applying_2006,
	location = {Berlin, Heidelberg},
	title = {Applying the Virtual Data Provenance Model},
	isbn = {978-3-540-46303-0},
	doi = {10.1007/11890850_16},
	series = {Lecture Notes in Computer Science},
	abstract = {In many domains of science, engineering, and commerce, data analysis systems are employed to derive new data (and ultimately, one hopes, knowledge) from datasets describing experimental results or simulated phenomena. To support such analyses, we have developed a “virtual data system” that allows users first to define, then to invoke, and finally explore the provenance of procedures (and workflows comprising multiple procedure calls) that perform such data derivations. The underlying execution model is “functional” in the sense that procedures read (but do not modify) their input and produce output via deterministic computations. This property makes it straightforward for the virtual data system to record not only the recipe for producing any given data object but also sufficient information about the environment in which the recipe has been executed, all with sufficient fidelity that the steps used to create a data object can be re-executed to reproduce the data object at a later time or a different location. The virtual data system maintains this information in an integrated schema alongside semantic annotations, and thus enables a powerful query capability in which the rich semantic information implied by knowledge of the structure of data derivation procedures can be exploited to provide an information environment that fuses recipe, history, and application-specific semantics. We provide here an overview of this integration, the queries and transformations that it enables, and examples of how these capabilities can serve scientific processes.},
	pages = {148--161},
	booktitle = {Provenance and Annotation of Data},
	publisher = {Springer},
	author = {Zhao, Yong and Wilde, Michael and Foster, Ian},
	editor = {Moreau, Luc and Foster, Ian},
	date = {2006},
	langid = {english},
	keywords = {provenance, project-provenance-pp},
	file = {Full Text PDF:/home/sam/Zotero/storage/XFYDNEA7/Zhao et al. - 2006 - Applying the Virtual Data Provenance Model.pdf:application/pdf},
}

@inproceedings{chan_provmark_2019,
	location = {New York, {NY}, {USA}},
	title = {{ProvMark}: A Provenance Expressiveness Benchmarking System},
	isbn = {978-1-4503-7009-7},
	url = {https://dl.acm.org/doi/10.1145/3361525.3361552},
	doi = {10.1145/3361525.3361552},
	series = {Middleware '19},
	shorttitle = {{ProvMark}},
	abstract = {System level provenance is of widespread interest for applications such as security enforcement and information protection. However, testing the correctness or completeness of provenance capture tools is challenging and currently done manually. In some cases there is not even a clear consensus about what behavior is correct. We present an automated tool, {ProvMark}, that uses an existing provenance system as a black box and reliably identifies the provenance graph structure recorded for a given activity, by a reduction to subgraph isomorphism problems handled by an external solver. {ProvMark} is a beginning step in the much needed area of testing and comparing the expressiveness of provenance systems. We demonstrate {ProvMark}'s usefuless in comparing three capture systems with different architectures and distinct design philosophies.},
	pages = {268--279},
	booktitle = {Proceedings of the 20th International Middleware Conference},
	publisher = {Association for Computing Machinery},
	author = {Chan, Sheung Chi and Cheney, James and Bhatotia, Pramod and Pasquier, Thomas and Gehani, Ashish and Irshad, Hassaan and Carata, Lucian and Seltzer, Margo},
	urldate = {2023-08-21},
	date = {2019-12-09},
	keywords = {provenance, project-provenance-pp},
	file = {Full Text PDF:/home/sam/Zotero/storage/Q4ITJ896/Chan et al. - 2019 - ProvMark A Provenance Expressiveness Benchmarking.pdf:application/pdf},
}

@misc{kalibera_quantifying_2020,
	title = {Quantifying Performance Changes with Effect Size Confidence Intervals},
	url = {http://arxiv.org/abs/2007.10899},
	doi = {10.48550/arXiv.2007.10899},
	abstract = {Measuring performance \& quantifying a performance change are core evaluation techniques in programming language and systems research. Of 122 recent scientific papers, as many as 65 included experimental evaluation that quantified a performance change using a ratio of execution times. Few of these papers evaluated their results with the level of rigour that has come to be expected in other experimental sciences. The uncertainty of measured results was largely ignored. Scarcely any of the papers mentioned uncertainty in the ratio of the mean execution times, and most did not even mention uncertainty in the two means themselves. Most of the papers failed to address the non-deterministic execution of computer programs (caused by factors such as memory placement, for example), and none addressed non-deterministic compilation. It turns out that the statistical methods presented in the computer systems performance evaluation literature for the design and summary of experiments do not readily allow this either. This poses a hazard to the repeatability, reproducibility and even validity of quantitative results. Inspired by statistical methods used in other fields of science, and building on results in statistics that did not make it to introductory textbooks, we present a statistical model that allows us both to quantify uncertainty in the ratio of (execution time) means and to design experiments with a rigorous treatment of those multiple sources of non-determinism that might impact measured performance. Better still, under our framework summaries can be as simple as "system A is faster than system B by 5.5\% \${\textbackslash}pm\$ 2.5\%, with 95\% confidence", a more natural statement than those derived from typical current practice, which are often misinterpreted. November 2013},
	number = {{arXiv}:2007.10899},
	publisher = {{arXiv}},
	author = {Kalibera, Tomas and Jones, Richard},
	urldate = {2023-08-22},
	date = {2020-07-21},
	eprinttype = {arxiv},
	eprint = {2007.10899 [cs, stat]},
	keywords = {software benchmarking, project-provenance-pp},
	file = {arXiv Fulltext PDF:/home/sam/Zotero/storage/U4JKMEN2/Kalibera and Jones - 2020 - Quantifying Performance Changes with Effect Size C.pdf:application/pdf;arXiv.org Snapshot:/home/sam/Zotero/storage/U9LM4IQU/2007.html:text/html},
}

@article{suh_emp_2017,
	title = {{EMP}: execution time measurement protocol for compute-bound programs},
	volume = {47},
	rights = {Copyright © 2017 John Wiley \& Sons, Ltd.},
	issn = {0038-0644, 1097-024X},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/spe.2476},
	doi = {10.1002/spe.2476},
	shorttitle = {{EMP}},
	abstract = {Measuring execution time is one of the most used performance evaluation techniques in computer science research. Inaccurate measurements cannot be used for a fair performance comparison between programs. Despite the prevalence of its use, the intrinsic variability in the time measurement makes it hard to obtain repeatable and accurate timing results of a program running on an operating system. We propose a novel execution time measurement protocol (termed {EMP}) for measuring the execution time of a compute-bound program on Linux, while minimizing that measurement's variability. During the development of execution time measurement protocol, we identified several factors that disturb execution time measurement. We introduce successive refinements to the protocol by addressing each of these factors, in concert, reducing variability by more than an order of magnitude. We also introduce a new visualization technique, what we term ‘dual-execution scatter plot’ that highlights infrequent, long-running daemons, differentiating them from frequent and/or short-running daemons. Our empirical results show that the proposed protocol successfully achieves three major aspects—precision, accuracy, and scalability—in execution time measurement that can work for open-source and proprietary software. Copyright © 2017 John Wiley \& Sons, Ltd.},
	pages = {559--597},
	number = {4},
	journaltitle = {Software: Practice and Experience},
	author = {Suh, Young-Kyoon and Snodgrass, Richard T. and Kececioglu, John D. and Downey, Peter J. and Maier, Robert S. and Yi, Cheng},
	urldate = {2023-08-22},
	date = {2017-04},
	langid = {english},
	note = {interest: 71},
	keywords = {software benchmarking, project-provenance-pp, benchmarking},
	file = {Full Text PDF:/home/sam/Zotero/storage/FKY79VS6/Suh et al. - 2017 - EMP execution time measurement protocol for compu.pdf:application/pdf;Snapshot:/home/sam/Zotero/storage/LERXZLRE/spe.html:text/html},
}

@inproceedings{pasquier_practical_2017,
	location = {New York, {NY}, {USA}},
	title = {Practical whole-system provenance capture},
	isbn = {978-1-4503-5028-0},
	url = {https://dl.acm.org/doi/10.1145/3127479.3129249},
	doi = {10.1145/3127479.3129249},
	series = {{SoCC} '17},
	abstract = {Data provenance describes how data came to be in its present form. It includes data sources and the transformations that have been applied to them. Data provenance has many uses, from forensics and security to aiding the reproducibility of scientific experiments. We present {CamFlow}, a whole-system provenance capture mechanism that integrates easily into a {PaaS} offering. While there have been several prior whole-system provenance systems that captured a comprehensive, systemic and ubiquitous record of a system's behavior, none have been widely adopted. They either A) impose too much overhead, B) are designed for long-outdated kernel releases and are hard to port to current systems, C) generate too much data, or D) are designed for a single system. {CamFlow} addresses these shortcoming by: 1) leveraging the latest kernel design advances to achieve efficiency; 2) using a self-contained, easily maintainable implementation relying on a Linux Security Module, {NetFilter}, and other existing kernel facilities; 3) providing a mechanism to tailor the captured provenance data to the needs of the application; and 4) making it easy to integrate provenance across distributed systems. The provenance we capture is streamed and consumed by tenant-built auditor applications. We illustrate the usability of our implementation by describing three such applications: demonstrating compliance with data regulations; performing fault/intrusion detection; and implementing data loss prevention. We also show how {CamFlow} can be leveraged to capture meaningful provenance without modifying existing applications.},
	pages = {405--418},
	booktitle = {Proceedings of the 2017 Symposium on Cloud Computing},
	publisher = {Association for Computing Machinery},
	author = {Pasquier, Thomas and Han, Xueyuan and Goldstein, Mark and Moyer, Thomas and Eyers, David and Seltzer, Margo and Bacon, Jean},
	urldate = {2023-08-23},
	date = {2017-09-24},
	keywords = {project-provenance-pp, provenance-tool},
	file = {Full Text PDF:/home/sam/Zotero/storage/2KKAQ44R/Pasquier et al. - 2017 - Practical whole-system provenance capture.pdf:application/pdf},
}

@article{li_threat_2021,
	title = {Threat detection and investigation with system-level provenance graphs: A survey},
	volume = {106},
	issn = {0167-4048},
	url = {https://www.sciencedirect.com/science/article/pii/S0167404821001061},
	doi = {10.1016/j.cose.2021.102282},
	shorttitle = {Threat detection and investigation with system-level provenance graphs},
	abstract = {With the development of information technology, the border of the cyberspace gets much broader and thus also exposes increasingly more vulnerabilities to attackers. Traditional mitigation-based defence strategies are challenging to cope with the current complicated situation. Security practitioners urgently need better tools to describe and modelling attacks for defense. The provenance graph seems like an ideal method for threat modelling with powerful semantic expression ability and attacks historic correlation ability. In this paper, we firstly introduce the basic concepts about system-level provenance graph and present a typical system architecture for provenance graph-based threat detection and investigation. A comprehensive provenance graph-based threat detection system can be divided into three modules: data collection module, data management module, and threat detection modules. Each module contains several components and involves different research problems. We systematically taxonomize and compare the existing algorithms and designs involved in them. Based on these comparisons, we identify the strategy of technology selection for real-world deployment. We also provide insights and challenges about the existing work to guide future research in this area.},
	pages = {102282},
	journaltitle = {Computers \& Security},
	author = {Li, Zhenyuan and Chen, Qi Alfred and Yang, Runqing and Chen, Yan and Ruan, Wei},
	urldate = {2023-08-23},
	date = {2021-07-01},
	keywords = {project-provenance-pp, provenance-tool},
	file = {ScienceDirect Full Text PDF:/home/sam/Zotero/storage/EXE45PJ5/Li et al. - 2021 - Threat detection and investigation with system-lev.pdf:application/pdf;ScienceDirect Snapshot:/home/sam/Zotero/storage/4NB2FLWF/S0167404821001061.html:text/html},
}

@inproceedings{gehani_spade_2012,
	location = {Berlin, Heidelberg},
	title = {{SPADE}: Support for Provenance Auditing in Distributed Environments},
	isbn = {978-3-642-35170-9},
	doi = {10.1007/978-3-642-35170-9_6},
	series = {Lecture Notes in Computer Science},
	shorttitle = {{SPADE}},
	abstract = {{SPADE} is an open source software infrastructure for data provenance collection and management. The underlying data model used throughout the system is graph-based, consisting of vertices and directed edges that are modeled after the node and relationship types described in the Open Provenance Model. The system has been designed to decouple the collection, storage, and querying of provenance metadata. At its core is a novel provenance kernel that mediates between the producers and consumers of provenance information, and handles the persistent storage of records. It operates as a service, peering with remote instances to enable distributed provenance queries. The provenance kernel on each host handles the buffering, filtering, and multiplexing of incoming metadata from multiple sources, including the operating system, applications, and manual curation. Provenance elements can be located locally with queries that use wildcard, fuzzy, proximity, range, and Boolean operators. Ancestor and descendant queries are transparently propagated across hosts until a terminating expression is satisfied, while distributed path queries are accelerated with provenance sketches.},
	pages = {101--120},
	booktitle = {Middleware 2012},
	publisher = {Springer},
	author = {Gehani, Ashish and Tariq, Dawood},
	editor = {Narasimhan, Priya and Triantafillou, Peter},
	date = {2012},
	langid = {english},
	keywords = {project-provenance-pp, provenance-tool},
	file = {Full Text PDF:/home/sam/Zotero/storage/S8LZBZWF/Gehani and Tariq - 2012 - SPADE Support for Provenance Auditing in Distribu.pdf:application/pdf},
}

@inproceedings{lee_high_2017,
	title = {High Accuracy Attack Provenance via Binary-based Execution Partition},
	abstract = {An important aspect of cyber attack forensics is to understand the provenance of suspicious events, as it discloses the root cause and ramiﬁcations of cyber attacks. Traditionally, this is done by analyzing audit log. However, the presence of long running programs makes a live process receiving a large volume of inputs and produce many outputs and each output may be causally related to all the preceding inputs, leading to dependence explosion and making attack investigations almost infeasible. We observe that a long running execution can be partitioned into individual units by monitoring the execution of the program’s event-handling loops, with each iteration corresponding to the processing of an independent input/request. We reverse engineer such loops from application binaries. We also reverse engineer instructions that could cause workﬂows between units. Detecting such a workﬂow is critical to disclosing causality between units. We then perform selective logging for unit boundaries and unit dependences. Our experiments show that our technique, called {BEEP}, has negligible runtime overhead ({\textless} 1.4\%) and low space overhead (12.28\% on average). It is effective in capturing the minimal causal graph for every attack case we have studied, without any dependence explosion.},
	eventtitle = {Network and Distributed System Security ({NDSS}) 2017},
	booktitle = {Proceedings of the 2017 Network and Distributed System Security ({NDSS}) Symposium},
	author = {Lee, Kyu Hyung and Zhang, Xiangyu and Xu, Dongyan},
	date = {2017},
	langid = {english},
	keywords = {project-provenance-pp, provenance-tool},
	file = {Lee et al. - High Accuracy Attack Provenance via Binary-based E.pdf:/home/sam/Zotero/storage/ABDE2U7Q/Lee et al. - High Accuracy Attack Provenance via Binary-based E.pdf:application/pdf},
}

@inproceedings{ma_accurate_2015,
	location = {New York, {NY}, {USA}},
	title = {Accurate, Low Cost and Instrumentation-Free Security Audit Logging for Windows},
	isbn = {978-1-4503-3682-6},
	url = {https://dl.acm.org/doi/10.1145/2818000.2818039},
	doi = {10.1145/2818000.2818039},
	series = {{ACSAC} '15},
	abstract = {Audit logging is an important approach to cyber attack investigation. However, traditional audit logging either lacks accuracy or requires expensive and complex binary instrumentation. In this paper, we propose a Windows based audit logging technique that features accuracy and low cost. More importantly, it does not require instrumenting the applications, which is critical for commercial software with {IP} protection. The technique is build on Event Tracing for Windows ({ETW}). By analyzing {ETW} log and critical parts of application executables, a model can be constructed to parse {ETW} log to units representing independent sub-executions in a process. Causality inferred at the unit level renders much higher accuracy, allowing us to perform accurate attack investigation and highly effective log reduction.},
	pages = {401--410},
	booktitle = {Proceedings of the 31st Annual Computer Security Applications Conference},
	publisher = {Association for Computing Machinery},
	author = {Ma, Shiqing and Lee, Kyu Hyung and Kim, Chung Hwan and Rhee, Junghwan and Zhang, Xiangyu and Xu, Dongyan},
	urldate = {2023-08-23},
	date = {2015-12-07},
	keywords = {project-provenance-pp, provenance-tool},
	file = {Full Text PDF:/home/sam/Zotero/storage/8Q2P53GV/Ma et al. - 2015 - Accurate, Low Cost and Instrumentation-Free Securi.pdf:application/pdf},
}

@inproceedings{ma_mpi_2017,
	title = {\{{MPI}\}: Multiple Perspective Attack Investigation with Semantic Aware Execution Partitioning},
	isbn = {978-1-931971-40-9},
	url = {https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/ma},
	shorttitle = {\{{MPI}\}},
	eventtitle = {26th {USENIX} Security Symposium ({USENIX} Security 17)},
	pages = {1111--1128},
	author = {Ma, Shiqing and Zhai, Juan and Wang, Fei and Lee, Kyu Hyung and Zhang, Xiangyu and Xu, Dongyan},
	urldate = {2023-08-23},
	date = {2017},
	langid = {english},
	keywords = {project-provenance-pp, provenance-tool},
	file = {Full Text PDF:/home/sam/Zotero/storage/UXCKL2W3/Ma et al. - 2017 - MPI Multiple Perspective Attack Investigation w.pdf:application/pdf},
}

@inproceedings{ma_protracer_2016,
	location = {San Diego, {CA}},
	title = {{ProTracer}: Towards Practical Provenance Tracing by Alternating Between Logging and Tainting},
	isbn = {978-1-891562-41-9},
	url = {https://www.ndss-symposium.org/wp-content/uploads/2017/09/protracer-towards-practical-provenance-tracing-alternating-logging-tainting.pdf},
	doi = {10.14722/ndss.2016.23350},
	shorttitle = {{ProTracer}},
	abstract = {Provenance tracing is a very important approach to Advanced Persistent Threat ({APT}) attack detection and investigation. Existing techniques either suffer from the dependence explosion problem or have non-trivial space and runtime overhead, which hinder their application in practice. We propose {ProTracer}, a lightweight provenance tracing system that alternates between system event logging and unit level taint propagation. The technique is built on an on-the-ﬂy system event processing infrastructure that features a very lightweight kernel module and a sophisticated user space daemon that performs concurrent and out-of-order event processing. The evaluation with different realistic system workloads and a number of attack cases show that {ProTracer} only produces 13MB log data per day, and 0.84GB(Server)/2.32GB(Client) in 3 months without losing any important information. The space consumption is only {\textless} 1.28\% of the state-of-the-art, 7 times smaller than an off-line garbage collection technique. The run-time overhead averages {\textless}7\% for servers and {\textless}5\% for regular applications. The generated attack causal graphs are a few times smaller than those by existing techniques while they are equally informative.},
	eventtitle = {Network and Distributed System Security Symposium},
	booktitle = {Proceedings 2016 Network and Distributed System Security Symposium},
	publisher = {Internet Society},
	author = {Ma, Shiqing and Zhang, Xiangyu and Xu, Dongyan},
	urldate = {2023-08-23},
	date = {2016},
	langid = {english},
	keywords = {project-provenance-pp, provenance-tool},
	file = {Ma et al. - 2016 - ProTracer Towards Practical Provenance Tracing by.pdf:/home/sam/Zotero/storage/JW5VPETI/Ma et al. - 2016 - ProTracer Towards Practical Provenance Tracing by.pdf:application/pdf},
}

@inproceedings{yang_uiscope_2020,
	location = {San Diego, {CA}},
	title = {{UISCOPE}: Accurate, Instrumentation-free, and Visible Attack Investigation for {GUI} Applications},
	isbn = {978-1-891562-61-7},
	url = {https://www.ndss-symposium.org/wp-content/uploads/2020/02/24329.pdf},
	doi = {10.14722/ndss.2020.24329},
	shorttitle = {{UISCOPE}},
	abstract = {Existing attack investigation solutions for {GUI} applications suffer from a few limitations such as inaccuracy (because of the dependence explosion problem), requiring instrumentation, and providing very low visibility. Such limitations have hindered their widespread and practical deployment. In this paper, we present {UISCOPE}, a novel accurate, instrumentationfree, and visible attack investigation system for {GUI} applications. The core idea of {UISCOPE} is to perform causality analysis on both {UI} elements/events which represent users’ perspective and low-level system events which provide detailed information of what happens under the hood, and then correlate system events with {UI} events to provide high accuracy and visibility. Long running processes are partitioned to individual {UI} transitions, to which low-level system events are attributed, making the results accurate. The produced graphs contain (causally related) {UI} elements with which users are very familiar, making them easily accessible. We deployed {UISCOPE} on 7 machines for a week, and also utilized {UISCOPE} to conduct an investigation of 6 realworld attacks. Our evaluation shows that compared to existing works, {UISCOPE} introduces neglibible overhead (less than 1\% runtime overhead and 3.05 {MB} event logs per hour on average) while {UISCOPE} can precisely identify attack provenance while offering users thorough visibility into the attack context.},
	eventtitle = {Network and Distributed System Security Symposium},
	booktitle = {Proceedings 2020 Network and Distributed System Security Symposium},
	publisher = {Internet Society},
	author = {Yang, Runqing and Ma, Shiqing and Xu, Haitao and Zhang, Xiangyu and Chen, Yan},
	urldate = {2023-08-23},
	date = {2020},
	langid = {english},
	keywords = {project-provenance-pp, provenance-tool},
	file = {Yang et al. - 2020 - UISCOPE Accurate, Instrumentation-free, and Visib.pdf:/home/sam/Zotero/storage/IUQN7XVC/Yang et al. - 2020 - UISCOPE Accurate, Instrumentation-free, and Visib.pdf:application/pdf},
}

@inproceedings{hassan_towards_2018,
	location = {San Diego, {CA}},
	title = {Towards Scalable Cluster Auditing through Grammatical Inference over Provenance Graphs},
	isbn = {978-1-891562-49-5},
	url = {https://www.ndss-symposium.org/wp-content/uploads/2018/02/ndss2018_07B-1_Hassan_paper.pdf},
	doi = {10.14722/ndss.2018.23141},
	abstract = {Investigating the nature of system intrusions in large distributed systems remains a notoriously difﬁcult challenge. While monitoring tools (e.g., Firewalls, {IDS}) provide preliminary alerts through easy-to-use administrative interfaces, attack reconstruction still requires that administrators sift through gigabytes of system audit logs stored locally on hundreds of machines. At present, two fundamental obstacles prevent synergy between system-layer auditing and modern cluster monitoring tools: 1) the sheer volume of audit data generated in a data center is prohibitively costly to transmit to a central node, and 2) systemlayer auditing poses a “needle-in-a-haystack” problem, such that hundreds of employee hours may be required to diagnose a single intrusion.},
	eventtitle = {Network and Distributed System Security Symposium},
	booktitle = {Proceedings 2018 Network and Distributed System Security Symposium},
	publisher = {Internet Society},
	author = {Hassan, Wajih Ul and Lemay, Mark and Aguse, Nuraini and Bates, Adam and Moyer, Thomas},
	urldate = {2023-08-23},
	date = {2018},
	langid = {english},
	keywords = {project-provenance-pp, provenance-tool},
	file = {Hassan et al. - 2018 - Towards Scalable Cluster Auditing through Grammati.pdf:/home/sam/Zotero/storage/JRBUPWS8/Hassan et al. - 2018 - Towards Scalable Cluster Auditing through Grammati.pdf:application/pdf},
}

@inproceedings{kwon_ldx_2016,
	location = {New York, {NY}, {USA}},
	title = {{LDX}: Causality Inference by Lightweight Dual Execution},
	isbn = {978-1-4503-4091-5},
	url = {https://dl.acm.org/doi/10.1145/2872362.2872395},
	doi = {10.1145/2872362.2872395},
	series = {{ASPLOS} '16},
	shorttitle = {{LDX}},
	abstract = {Causality inference, such as dynamic taint anslysis, has many applications (e.g., information leak detection). It determines whether an event e is causally dependent on a preceding event c during execution. We develop a new causality inference engine {LDX}. Given an execution, it spawns a slave execution, in which it mutates c and observes whether any change is induced at e. To preclude non-determinism, {LDX} couples the executions by sharing syscall outcomes. To handle path differences induced by the perturbation, we develop a novel on-the-fly execution alignment scheme that maintains a counter to reflect the progress of execution. The scheme relies on program analysis and compiler transformation. {LDX} can effectively detect information leak and security attacks with an average overhead of 6.08\% while running the master and the slave concurrently on separate {CPUs}, much lower than existing systems that require instruction level monitoring. Furthermore, it has much better accuracy in causality inference.},
	pages = {503--515},
	booktitle = {Proceedings of the Twenty-First International Conference on Architectural Support for Programming Languages and Operating Systems},
	publisher = {Association for Computing Machinery},
	author = {Kwon, Yonghwi and Kim, Dohyeong and Sumner, William Nick and Kim, Kyungtae and Saltaformaggio, Brendan and Zhang, Xiangyu and Xu, Dongyan},
	urldate = {2023-08-23},
	date = {2016-03-25},
	keywords = {project-provenance-pp, provenance-tool},
	file = {Full Text PDF:/home/sam/Zotero/storage/LJ8B4UR9/Kwon et al. - 2016 - LDX Causality Inference by Lightweight Dual Execu.pdf:application/pdf},
}

@inproceedings{kwon_mci_2018,
	location = {San Diego, {CA}},
	title = {{MCI} : Modeling-based Causality Inference in Audit Logging for Attack Investigation},
	isbn = {978-1-891562-49-5},
	url = {https://www.ndss-symposium.org/wp-content/uploads/2018/02/ndss2018_07B-2_Kwon_paper.pdf},
	doi = {10.14722/ndss.2018.23306},
	shorttitle = {{MCI}},
	abstract = {In this paper, we develop a model based causality inference technique for audit logging that does not require any application instrumentation or kernel modiﬁcation. It leverages a recent dynamic analysis, dual execution ({LDX}), that can infer precise causality between system calls but unfortunately requires doubling the resource consumption such as {CPU} time and memory consumption. For each application, we use {LDX} to acquire precise causal models for a set of primitive operations. Each model is a sequence of system calls that have inter-dependences, some of them caused by memory operations and hence implicit at the system call level. These models are described by a language that supports various complexity such as regular, context-free, and even context-sensitive. In production run, a novel parser is deployed to parse audit logs (without any enhancement) to model instances and hence derive causality. Our evaluation on a set of real-world programs shows that the technique is highly effective. The generated models can recover causality with 0\% false-positives ({FP}) and false-negatives ({FN}) for most programs and only 8.3\% {FP} and 5.2\% {FN} in the worst cases. The models also feature excellent composibility, meaning that the models derived from primitive operations can be composed together to describe causality for large and complex real world missions. Applying our technique to attack investigation shows that the system-wide attack causal graphs are highly precise and concise, having better quality than the state-of-the-art.},
	eventtitle = {Network and Distributed System Security Symposium},
	booktitle = {Proceedings 2018 Network and Distributed System Security Symposium},
	publisher = {Internet Society},
	author = {Kwon, Yonghwi and Wang, Fei and Wang, Weihang and Lee, Kyu Hyung and Lee, Wen-Chuan and Ma, Shiqing and Zhang, Xiangyu and Xu, Dongyan and Jha, Somesh and Ciocarlie, Gabriela and Gehani, Ashish and Yegneswaran, Vinod},
	urldate = {2023-08-23},
	date = {2018},
	langid = {english},
	keywords = {project-provenance-pp, provenance-tool},
	file = {Kwon et al. - 2018 - MCI  Modeling-based Causality Inference in Audit .pdf:/home/sam/Zotero/storage/9I4EY8H5/Kwon et al. - 2018 - MCI  Modeling-based Causality Inference in Audit .pdf:application/pdf},
}

@inproceedings{ji_rain_2017,
	location = {New York, {NY}, {USA}},
	title = {{RAIN}: Refinable Attack Investigation with On-demand Inter-Process Information Flow Tracking},
	isbn = {978-1-4503-4946-8},
	url = {https://dl.acm.org/doi/10.1145/3133956.3134045},
	doi = {10.1145/3133956.3134045},
	series = {{CCS} '17},
	shorttitle = {{RAIN}},
	abstract = {As modern attacks become more stealthy and persistent, detecting or preventing them at their early stages becomes virtually impossible. Instead, an attack investigation or provenance system aims to continuously monitor and log interesting system events with minimal overhead. Later, if the system observes any anomalous behavior, it analyzes the log to identify who initiated the attack and which resources were affected by the attack and then assess and recover from any damage incurred. However, because of a fundamental tradeoff between log granularity and system performance, existing systems typically record system-call events without detailed program-level activities (e.g., memory operation) required for accurately reconstructing attack causality or demand that every monitored program be instrumented to provide program-level information. To address this issue, we propose {RAIN}, a Refinable Attack {INvestigation} system based on a record-replay technology that records system-call events during runtime and performs instruction-level dynamic information flow tracking ({DIFT}) during on-demand process replay. Instead of replaying every process with {DIFT}, {RAIN} conducts system-call-level reachability analysis to filter out unrelated processes and to minimize the number of processes to be replayed, making inter-process {DIFT} feasible. Evaluation results show that {RAIN} effectively prunes out unrelated processes and determines attack causality with negligible false positive rates. In addition, the runtime overhead of {RAIN} is similar to existing system-call level provenance systems and its analysis overhead is much smaller than full-system {DIFT}.},
	pages = {377--390},
	booktitle = {Proceedings of the 2017 {ACM} {SIGSAC} Conference on Computer and Communications Security},
	publisher = {Association for Computing Machinery},
	author = {Ji, Yang and Lee, Sangho and Downing, Evan and Wang, Weiren and Fazzini, Mattia and Kim, Taesoo and Orso, Alessandro and Lee, Wenke},
	urldate = {2023-08-23},
	date = {2017-10-30},
	keywords = {project-provenance-pp, provenance-tool},
	file = {Full Text PDF:/home/sam/Zotero/storage/T2R2UAKP/Ji et al. - 2017 - RAIN Refinable Attack Investigation with On-deman.pdf:application/pdf},
}

@inproceedings{ji_enabling_2018,
	title = {Enabling Refinable \{Cross-Host\} Attack Investigation with Efficient Data Flow Tagging and Tracking},
	isbn = {978-1-939133-04-5},
	url = {https://www.usenix.org/conference/usenixsecurity18/presentation/jia-yang},
	eventtitle = {27th {USENIX} Security Symposium ({USENIX} Security 18)},
	pages = {1705--1722},
	author = {Ji, Yang and Lee, Sangho and Fazzini, Mattia and Allen, Joey and Downing, Evan and Kim, Taesoo and Orso, Alessandro and Lee, Wenke},
	urldate = {2023-08-23},
	date = {2018},
	langid = {english},
	keywords = {project-provenance-pp, provenance-tool},
	file = {Full Text PDF:/home/sam/Zotero/storage/Y526VM3S/Ji et al. - 2018 - Enabling Refinable Cross-Host Attack Investigati.pdf:application/pdf},
}

@inproceedings{kemerlis_libdft_2012,
	location = {New York, {NY}, {USA}},
	title = {libdft: practical dynamic data flow tracking for commodity systems},
	isbn = {978-1-4503-1176-2},
	url = {https://dl.acm.org/doi/10.1145/2151024.2151042},
	doi = {10.1145/2151024.2151042},
	series = {{VEE} '12},
	shorttitle = {libdft},
	abstract = {Dynamic data flow tracking ({DFT}) deals with tagging and tracking data of interest as they propagate during program execution. {DFT} has been repeatedly implemented by a variety of tools for numerous purposes, including protection from zero-day and cross-site scripting attacks, detection and prevention of information leaks, and for the analysis of legitimate and malicious software. We present libdft, a dynamic {DFT} framework that unlike previous work is at once fast, reusable, and works with commodity software and hardware. libdft provides an {API} for building {DFT}-enabled tools that work on unmodified binaries, running on common operating systems and hardware, thus facilitating research and rapid prototyping. We explore different approaches for implementing the low-level aspects of instruction-level data tracking, introduce a more efficient and 64-bit capable shadow memory, and identify (and avoid) the common pitfalls responsible for the excessive performance overhead of previous studies. We evaluate libdft using real applications with large codebases like the Apache and {MySQL} servers, and the Firefox web browser. We also use a series of benchmarks and utilities to compare libdft with similar systems. Our results indicate that it performs at least as fast, if not faster, than previous solutions, and to the best of our knowledge, we are the first to evaluate the performance overhead of a fast dynamic {DFT} implementation in such depth. Finally, libdft is freely available as open source software.},
	pages = {121--132},
	booktitle = {Proceedings of the 8th {ACM} {SIGPLAN}/{SIGOPS} conference on Virtual Execution Environments},
	publisher = {Association for Computing Machinery},
	author = {Kemerlis, Vasileios P. and Portokalidis, Georgios and Jee, Kangkook and Keromytis, Angelos D.},
	urldate = {2023-08-23},
	date = {2012-03-03},
	keywords = {project-provenance-pp, prov-tool},
	file = {Full Text PDF:/home/sam/Zotero/storage/R4P4ZSXE/Kemerlis et al. - 2012 - libdft practical dynamic data flow tracking for c.pdf:application/pdf},
}

@article{hassan_omegalog_2020,
	title = {{OmegaLog}: High-Fidelity Attack Investigation via Transparent Multi-layer Log Analysis},
	url = {https://par.nsf.gov/biblio/10146531-omegalog-high-fidelity-attack-investigation-via-transparent-multi-layer-log-analysis},
	shorttitle = {{OmegaLog}},
	abstract = {Recent advances in causality analysis have enabled investigators to trace multi-stage attacks using whole- system provenance graphs. Based on system-layer audit logs (e.g., syscalls), these approaches omit vital sources of application context (e.g., email addresses, {HTTP} response codes) that can found in higher layers of the system. Although this information is often essential to understanding attack behaviors, incorporating this evidence into causal analysis engines is difficult due to the semantic gap that exists between system layers. To address this shortcoming, we propose the notion of universal provenance, which encodes all forensically-relevant causal dependencies regardless of their layer of origin. To transparently realize this vision on commodity systems, we present ω{LOG} (“Omega Log”), a provenance tracking mechanism that bridges the semantic gap between system and application logging contexts. ω{LOG} analyzes program binaries to identify and model application-layer logging behaviors, enabling application events to be accurately reconciled with system-layer accesses. ω{LOG} then intercepts applications’ runtime logging activities and grafts those events onto the system-layer provenance graph, allowing investigators to reason more precisely about the nature of attacks. We demonstrate that ω{LOG} is widely-applicable to existing software projects and can transparently facilitate execution partitioning of dependency graphs without any training or developer intervention. Evaluation on real-world attack scenarios shows that universal provenance graphs are concise and rich with semantic information as compared to the state-of-the-art, with 12\% average runtime overhead.},
	journaltitle = {Network and Distributed System Security Symposium},
	author = {Hassan, Wajih Ul and Noureddine, Mohammad Ali and Datta, Pubali and Bates, Adam},
	urldate = {2023-08-23},
	date = {2020-01},
	langid = {english},
	keywords = {project-provenance-pp, provenance-tool},
	file = {Full Text PDF:/home/sam/Zotero/storage/B7Y8LDR8/Hassan et al. - 2020 - OmegaLog High-Fidelity Attack Investigation via T.pdf:application/pdf},
}

@incollection{hutchison_issues_2006,
	location = {Berlin, Heidelberg},
	title = {Issues in Automatic Provenance Collection},
	volume = {4145},
	isbn = {978-3-540-46302-3 978-3-540-46303-0},
	url = {http://link.springer.com/10.1007/11890850_18},
	abstract = {Automatic provenance collection describes systems that observe processes and data transformations inferring, collecting, and maintaining provenance about them. Automatic collection is a powerful tool for analysis of objects and processes, providing a level of transparency and pervasiveness not found in more conventional provenance systems. Unfortunately, automatic collection is also diﬃcult. We discuss the challenges we encountered and the issues we exposed as we developed an automatic provenance collector that runs at the operating system level.},
	pages = {171--183},
	booktitle = {Provenance and Annotation of Data},
	publisher = {Springer Berlin Heidelberg},
	author = {Braun, Uri and Garfinkel, Simson and Holland, David A. and Muniswamy-Reddy, Kiran-Kumar and Seltzer, Margo I.},
	editor = {Moreau, Luc and Foster, Ian},
	editorb = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard},
	editorbtype = {redactor},
	urldate = {2023-08-23},
	date = {2006},
	langid = {english},
	doi = {10.1007/11890850_18},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {project-provenance-pp, provenance-tool},
	file = {Braun et al. - 2006 - Issues in Automatic Provenance Collection.pdf:/home/sam/Zotero/storage/KXYB7A6B/Braun et al. - 2006 - Issues in Automatic Provenance Collection.pdf:application/pdf},
}

@article{berrada_baseline_2020,
	title = {A baseline for unsupervised advanced persistent threat detection in system-level provenance},
	volume = {108},
	issn = {0167-739X},
	url = {https://www.sciencedirect.com/science/article/pii/S0167739X19320448},
	doi = {10.1016/j.future.2020.02.015},
	abstract = {Advanced persistent threats ({APTs}) are stealthy, sophisticated, and unpredictable cyberattacks that can steal intellectual property, damage critical infrastructure, or cause millions of dollars in damage. Detecting {APTs} by monitoring system-level activity is difficult because manually inspecting the high volume of normal system activity is overwhelming for security analysts. We evaluate the effectiveness of unsupervised batch and streaming anomaly detection algorithms over multiple gigabytes of provenance traces recorded on four different operating systems to determine whether they can detect realistic {APT}-like attacks reliably and efficiently. This article is the first detailed study of the effectiveness of generic unsupervised anomaly detection techniques in this setting.},
	pages = {401--413},
	journaltitle = {Future Generation Computer Systems},
	author = {Berrada, Ghita and Cheney, James and Benabderrahmane, Sidahmed and Maxwell, William and Mookherjee, Himan and Theriault, Alec and Wright, Ryan},
	urldate = {2023-08-23},
	date = {2020-07-01},
	keywords = {project-provenance-pp, provenance-tool},
	file = {ScienceDirect Full Text PDF:/home/sam/Zotero/storage/6LH8V7HU/Berrada et al. - 2020 - A baseline for unsupervised advanced persistent th.pdf:application/pdf;ScienceDirect Snapshot:/home/sam/Zotero/storage/J7DXZAI5/S0167739X19320448.html:text/html},
}

@inproceedings{tariq_towards_2012,
	title = {Towards Automated Collection of \{Application-Level\} Data Provenance},
	url = {https://www.usenix.org/conference/tapp12/workshop-program/presentation/tariq},
	eventtitle = {4th {USENIX} Workshop on the Theory and Practice of Provenance ({TaPP} 12)},
	author = {Tariq, Dawood and Masaim, Ali and Gehani, Ashish},
	urldate = {2023-08-23},
	date = {2012},
	langid = {english},
	keywords = {project-provenance-pp, provenance-tool},
	file = {Full Text PDF:/home/sam/Zotero/storage/PG74HTCE/2012 - Towards Automated Collection of Application-Level.pdf:application/pdf},
}

@inproceedings{chan_expressiveness_2017,
	title = {Expressiveness Benchmarking for \{System-Level\} Provenance},
	url = {https://www.usenix.org/conference/tapp17/workshop-program/presentation/chan},
	eventtitle = {9th {USENIX} Workshop on the Theory and Practice of Provenance ({TaPP} 2017)},
	author = {Chan, Sheung Chi and Gehani, Ashish and Cheney, James and Sohan, Ripduman and Irshad, Hassaan},
	urldate = {2023-08-23},
	date = {2017},
	langid = {english},
	keywords = {project-provenance-pp, provenance-tool},
	file = {Full Text PDF:/home/sam/Zotero/storage/YWH8KUWW/Chan et al. - 2017 - Expressiveness Benchmarking for System-Level Pro.pdf:application/pdf},
}

@inproceedings{sultana_file_2013,
	location = {New York, {NY}, {USA}},
	title = {A file provenance system},
	isbn = {978-1-4503-1890-7},
	url = {https://dl.acm.org/doi/10.1145/2435349.2435368},
	doi = {10.1145/2435349.2435368},
	series = {{CODASPY} '13},
	abstract = {A file provenance system supports the automatic collection and management of provenance i.e. the complete processing history of a data object. File system level provenance provides functionality unavailable in the existing provenance systems. In this paper, we discuss the design objectives for a flexible and efficient file provenance system and then propose the design of such a system, called {FiPS}. We design {FiPS} as a thin stackable file system for capturing provenance in a portable manner. {FiPS} can capture provenance at various degrees of granularity, can transform provenance records into secure information, and can direct the resulting provenance data to various persistent storage systems.},
	pages = {153--156},
	booktitle = {Proceedings of the third {ACM} conference on Data and application security and privacy},
	publisher = {Association for Computing Machinery},
	author = {Sultana, Salmin and Bertino, Elisa},
	urldate = {2023-08-23},
	date = {2013-02-18},
	keywords = {project-provenance-pp, provenance-tool},
	file = {Full Text PDF:/home/sam/Zotero/storage/L543Q7CC/Sultana and Bertino - 2013 - A file provenance system.pdf:application/pdf},
}

@inproceedings{muniswamy-reddy_provenance-aware_2006,
	title = {Provenance-Aware Storage Systems},
	rights = {open},
	url = {https://dash.harvard.edu/handle/1/23853812},
	abstract = {A Provenance-Aware Storage System ({PASS}) is a storage system that automatically collects and maintains provenance or lineage, the complete history or ancestry of an item. We discuss the advantages of treating provenance as meta-data collected and maintained by the storage system, rather than as manual annotations stored in a separately administered database. We describe a {PASS} implementation, discussing the challenges it presents, performance cost it incurs, and the new functionality it enables. We show that with reasonable overhead, we can provide useful functionality not available in today’s ﬁle systems or provenance management systems.},
	eventtitle = {2006 {USENIX} Annual Technical Conference},
	booktitle = {2006 {USENIX} Annual Technical Conference},
	author = {Muniswamy-Reddy, Kiran-Kumar and Holland, David A and Braun, Uri and Seltzer, Margo},
	date = {2006},
	langid = {american},
	note = {Accepted: 2015-12-07T19:07:43Z},
	keywords = {project-provenance-pp, provenance-tool},
	file = {Full Text PDF:/home/sam/Zotero/storage/2NZWHDHT/Muniswamy-Reddy et al. - 2006 - Provenance-Aware Storage Systems.pdf:application/pdf;Muniswamy-Reddy et al. - 2006 - Provenance-Aware Storage Systems.pdf:/home/sam/Zotero/storage/BYWE4X2D/Muniswamy-Reddy et al. - 2006 - Provenance-Aware Storage Systems.pdf:application/pdf},
}

@inproceedings{vahdat_transparent_1998,
	location = {{USA}},
	title = {Transparent result caching},
	series = {{ATEC} '98},
	abstract = {The goal of this work is to develop a general framework for transparently managing the interactions and dependencies among input files, development tools, and output files. By unobtrusively monitoring the execution of unmodified programs, we are able to track process lineage--each process's parent, children, input files, and output files, and file dependency--for each file, the sequence of operations and the set of input files used to create the file. We use this information to implement Transparent Result Caching ({TREC}) and describe how {TREC} is used to build a number of useful user utilities. Unmake allows users to query {TREC} for file lineage information, including the full sequence of programs executed to create a particular output file. Transparent Make uses {TREC} to automatically generate dependency information by observing program execution, freeing end users from the need to explicitly specify dependency information (i.e., Makefiles can be replaced by shell scripts). Dynamic Web Object Caching allows for the caching of certain dynamically generated web pages, improving server performance and client latency.},
	pages = {3},
	booktitle = {Proceedings of the annual conference on {USENIX} Annual Technical Conference},
	publisher = {{USENIX} Association},
	author = {Vahdat, Amin and Anderson, Thomas},
	urldate = {2023-08-23},
	date = {1998-06-15},
	keywords = {project-provenance-pp, provenance-tool},
}

@inproceedings{pohly_hi-fi_2012,
	location = {New York, {NY}, {USA}},
	title = {Hi-Fi: collecting high-fidelity whole-system provenance},
	isbn = {978-1-4503-1312-4},
	url = {https://dl.acm.org/doi/10.1145/2420950.2420989},
	doi = {10.1145/2420950.2420989},
	series = {{ACSAC} '12},
	shorttitle = {Hi-Fi},
	abstract = {Data provenance---a record of the origin and evolution of data in a system---is a useful tool for forensic analysis. However, existing provenance collection mechanisms fail to achieve sufficient breadth or fidelity to provide a holistic view of a system's operation over time. We present Hi-Fi, a kernel-level provenance system which leverages the Linux Security Modules framework to collect high-fidelity whole-system provenance. We demonstrate that Hi-Fi is able to record a variety of malicious behavior within a compromised system. In addition, our benchmarks show the collection overhead from Hi-Fi to be less than 1\% for most system calls and 3\% in a representative workload, while simultaneously generating a system measurement that fully reflects system evolution. In this way, we show that we can collect broad, high-fidelity provenance data which is capable of supporting detailed forensic analysis.},
	pages = {259--268},
	booktitle = {Proceedings of the 28th Annual Computer Security Applications Conference},
	publisher = {Association for Computing Machinery},
	author = {Pohly, Devin J. and {McLaughlin}, Stephen and {McDaniel}, Patrick and Butler, Kevin},
	urldate = {2023-08-23},
	date = {2012-12-03},
	keywords = {project-provenance-pp, provenance-tool},
	file = {Full Text PDF:/home/sam/Zotero/storage/HBBVHQER/Pohly et al. - 2012 - Hi-Fi collecting high-fidelity whole-system prove.pdf:application/pdf},
}

@unpublished{sar_lineage_nodate,
	title = {Lineage File System},
	url = {http://crypto.stanford.edu/~cao/lineage.html},
	author = {Sar, Can and Cao, Pei},
	urldate = {2023-08-23},
	keywords = {project-provenance-pp, provenance-tool},
}

@inproceedings{macko_collecting_2011,
	title = {Collecting Provenance via the Xen Hypervisor},
	url = {https://www.usenix.org/legacy/events/tapp11/tech/final_files/MackoChiariniSeltzer.pdf},
	abstract = {The Provenance Aware Storage Systems project ({PASS})
currently collects system-level provenance by intercept-
ing system calls in the Linux kernel and storing the
provenance in a stackable filesystem. While this ap-
proach is reasonably efficient, it suffers from two sig-
nificant drawbacks: each new revision of the kernel re-
quires reintegration of {PASS} changes, the stability of
which must be continually tested; also, the use of a stack-
able filesystem makes it difficult to collect provenance
on root volumes, especially during early boot. In this pa-
per we describe an approach to collecting system-level
provenance from virtual guest machines running under
the Xen hypervisor. We make the case that our approach
alleviates the aforementioned difficulties and promotes
adoption of provenance collection within cloud comput-
ing platforms.},
	eventtitle = {Theory and Practice of Provenance ({TaPP})},
	booktitle = {Proceedings of the Theory and Practice of Provenance ({TaPP})},
	publisher = {{USENIX}},
	author = {Macko, Peter and Chiarini, Marc and Seltzer, Margo},
	date = {2011},
	keywords = {project-provenance-pp, provenance-tool},
}

@article{holland_passing_2008,
	title = {{PASSing} the provenance challenge},
	volume = {20},
	rights = {Copyright © 2007 John Wiley \& Sons, Ltd.},
	issn = {1532-0634},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.1227},
	doi = {10.1002/cpe.1227},
	abstract = {Provenance-aware storage systems ({PASS}) are a new class of storage system treating provenance as a first-class object, providing automatic collection, storage, and management of provenance as well as query capabilities. We developed the first {PASS} prototype between 2005 and 2006, targeting scientific end users. Prior to undertaking the provenance challenge, we had focused on provenance collection and storage, without much emphasis on a query model or language. The challenge forced us to (quickly) develop a query model and infrastructure implementing this model. We present a brief overview of the {PASS} prototype and a discussion of the evolution of the query model that we developed for the challenge. Copyright © 2007 John Wiley \& Sons, Ltd.},
	pages = {531--540},
	number = {5},
	journaltitle = {Concurrency and Computation: Practice and Experience},
	author = {Holland, David A. and Seltzer, Margo I. and Braun, Uri and Muniswamy-Reddy, Kiran-Kumar},
	urldate = {2023-08-23},
	date = {2008},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/cpe.1227},
	keywords = {project-provenance-pp, provenance-tool},
	file = {Full Text PDF:/home/sam/Zotero/storage/VD4HIMW2/Holland et al. - 2008 - PASSing the provenance challenge.pdf:application/pdf;Snapshot:/home/sam/Zotero/storage/8HRDN6SV/cpe.html:text/html},
}

@article{carata_primer_2014,
	title = {A primer on provenance},
	volume = {57},
	issn = {0001-0782},
	url = {https://dl.acm.org/doi/10.1145/2596628},
	doi = {10.1145/2596628},
	abstract = {Better understanding data requires tracking its history and context.},
	pages = {52--60},
	number = {5},
	journaltitle = {Commun. {ACM}},
	author = {Carata, Lucian and Akoush, Sherif and Balakrishnan, Nikilesh and Bytheway, Thomas and Sohan, Ripduman and Seltzer, Margo and Hopper, Andy},
	urldate = {2023-08-23},
	date = {2014-05-01},
	keywords = {project-provenance-pp, provenance-tool},
	file = {Full Text PDF:/home/sam/Zotero/storage/K2EIEAY9/Carata et al. - 2014 - A primer on provenance.pdf:application/pdf},
}

@article{zipperle_provenance-based_2022,
	title = {Provenance-based Intrusion Detection Systems: A Survey},
	volume = {55},
	issn = {0360-0300},
	url = {https://dl.acm.org/doi/10.1145/3539605},
	doi = {10.1145/3539605},
	shorttitle = {Provenance-based Intrusion Detection Systems},
	abstract = {Traditional Intrusion Detection Systems ({IDS}) cannot cope with the increasing number and sophistication of cyberattacks such as Advanced Persistent Threats ({APT}). Due to their high false-positive rate and the required effort of security experts to validate them, incidents can remain undetected for up to several months. As a result, enterprises suffer from data loss and severe financial damage. Recent research explored data provenance for Host-based Intrusion Detection Systems ({HIDS}) as one promising data source to tackle this issue. Data provenance represents information flows between system entities as Direct Acyclic Graph ({DAG}). Provenance-based Intrusion Detection Systems ({PIDS}) utilize data provenance to enhance the detection performance of intrusions and reduce false-alarm rates compared to traditional {IDS}. This survey demonstrates the potential of {PIDS} by providing a detailed evaluation of recent research in the field, proposing a novel taxonomy for {PIDS}, discussing current issues, and potential future research directions. This survey aims to help and motivate researchers to get started in the field of {PIDS} by tackling issues of data collection, graph summarization, intrusion detection, and developing real-world benchmark datasets.},
	pages = {135:1--135:36},
	number = {7},
	journaltitle = {{ACM} Comput. Surv.},
	author = {Zipperle, Michael and Gottwalt, Florian and Chang, Elizabeth and Dillon, Tharam},
	urldate = {2023-08-23},
	date = {2022-12-15},
	keywords = {project-provenance-pp, provenance-tool},
	file = {Full Text PDF:/home/sam/Zotero/storage/AZFUMA8Z/Zipperle et al. - 2022 - Provenance-based Intrusion Detection Systems A Su.pdf:application/pdf},
}

@inproceedings{yin_panorama_2007,
	location = {New York, {NY}, {USA}},
	title = {Panorama: capturing system-wide information flow for malware detection and analysis},
	isbn = {978-1-59593-703-2},
	url = {https://dl.acm.org/doi/10.1145/1315245.1315261},
	doi = {10.1145/1315245.1315261},
	series = {{CCS} '07},
	shorttitle = {Panorama},
	abstract = {Malicious programs spy on users' behavior and compromise their privacy. Even software from reputable vendors, such as Google Desktop and Sony {DRM} media player, may perform undesirable actions. Unfortunately, existing techniques for detecting malware and analyzing unknown code samples are insufficient and have significant shortcomings. We observe that malicious information access and processing behavior is the fundamental trait of numerous malware categories breaching users' privacy (including keyloggers, password thieves, network sniffers, stealth backdoors, spyware and rootkits), which separates these malicious applications from benign software. We propose a system, Panorama, to detect and analyze malware by capturing this fundamental trait. In our extensive experiments, Panorama successfully detected all the malware samples and had very few false positives. Furthermore, by using Google Desktop as a case study, we show that our system can accurately capture its information access and processing behavior, and we can confirm that it does send back sensitive information to remote servers in certain settings. We believe that a system such as Panorama will offer indispensable assistance to code analysts and malware researchers by enabling them to quickly comprehend the behavior and innerworkings of an unknown sample.},
	pages = {116--127},
	booktitle = {Proceedings of the 14th {ACM} conference on Computer and communications security},
	publisher = {Association for Computing Machinery},
	author = {Yin, Heng and Song, Dawn and Egele, Manuel and Kruegel, Christopher and Kirda, Engin},
	urldate = {2023-08-23},
	date = {2007-10-28},
	keywords = {project-provenance-pp, provenance-tool},
	file = {Full Text PDF:/home/sam/Zotero/storage/L7K5I9UH/Yin et al. - 2007 - Panorama capturing system-wide information flow f.pdf:application/pdf},
}

@online{noauthor_event_2021,
	title = {Event Tracing - Win32 apps},
	url = {https://learn.microsoft.com/en-us/windows/win32/etw/event-tracing-portal},
	abstract = {This documentation is for user-mode applications that want to use {ETW}. For information about instrumenting device drivers that run in kernel mode, see {WPP} Software Tracing and Adding Event Tracing to Kernel-Mode Drivers in the Windows Driver Kit ({WDK}).},
	urldate = {2023-08-23},
	date = {2021-01-07},
	langid = {english},
	keywords = {project-provenance-pp, provenance-tool},
	file = {Snapshot:/home/sam/Zotero/storage/FCT2GL8J/event-tracing-portal.html:text/html},
}

@online{noauthor_about_nodate,
	title = {About {DTrace}},
	url = {http://dtrace.org/blogs/about/},
	urldate = {2023-08-23},
	langid = {english},
	keywords = {operating systems, project-provenance-pp, prov-tool},
	file = {Snapshot:/home/sam/Zotero/storage/TDNCI4W3/about.html:text/html},
}

@online{markruss_sysmon_2023,
	title = {Sysmon - Sysinternals},
	url = {https://learn.microsoft.com/en-us/sysinternals/downloads/sysmon},
	abstract = {Monitors and reports key system activity via the Windows event log.},
	author = {markruss},
	urldate = {2023-08-23},
	date = {2023-04-11},
	langid = {english},
	keywords = {operating systems, project-provenance-pp, prov-tool},
	file = {Snapshot:/home/sam/Zotero/storage/VNS2CHDN/sysmon.html:text/html},
}

@inproceedings{stamatogiannakis_decoupling_2015,
	location = {{USA}},
	title = {Decoupling provenance capture and analysis from execution},
	series = {{TaPP}'15},
	abstract = {Capturing provenance usually involves the direct observation and instrumentation of the execution of a program or workflow. However, this approach restricts provenance analysis to pre-determined programs and methods. This may not pose a problem when one is interested in the provenance of a well-defined workflow, but may limit the analysis of unstructured processes such as interactive desktop computing. In this paper, we present a new approach to capturing provenance based on full execution record and replay. Our approach leverages full-system execution trace logging and replay, which allows the complete decoupling of analysis from the original execution. This enables the selective analysis of the execution using progressively heavier instrumentation.},
	pages = {3},
	booktitle = {Proceedings of the 7th {USENIX} Conference on Theory and Practice of Provenance},
	publisher = {{USENIX} Association},
	author = {Stamatogiannakis, Manolis and Groth, Paul and Bos, Herbert},
	urldate = {2023-08-24},
	date = {2015-07-08},
	keywords = {project-provenance-pp, provenance-tool},
}

@inproceedings{stamatogiannakis_looking_2015,
	location = {Cham},
	title = {Looking Inside the Black-Box: Capturing Data Provenance Using Dynamic Instrumentation},
	isbn = {978-3-319-16462-5},
	doi = {10.1007/978-3-319-16462-5_12},
	series = {Lecture Notes in Computer Science},
	shorttitle = {Looking Inside the Black-Box},
	abstract = {Knowing the provenance of a data item helps in ascertaining its trustworthiness. Various approaches have been proposed to track or infer data provenance. However, these approaches either treat an executing program as a black-box, limiting the fidelity of the captured provenance, or require developers to modify the program to make it provenance-aware. In this paper, we introduce {DataTracker}, a new approach to capturing data provenance based on taint tracking, a technique widely used in the security and reverse engineering fields. Our system is able to identify data provenance relations through dynamic instrumentation of unmodified binaries, without requiring access to, or knowledge of, their source code. Hence, we can track provenance for a variety of well-known applications. Because {DataTracker} looks inside the executing program, it captures high-fidelity and accurate data provenance.},
	pages = {155--167},
	booktitle = {Provenance and Annotation of Data and Processes},
	publisher = {Springer International Publishing},
	author = {Stamatogiannakis, Manolis and Groth, Paul and Bos, Herbert},
	editor = {Ludäscher, Bertram and Plale, Beth},
	date = {2015},
	langid = {english},
	keywords = {project-provenance-pp, prov-tool},
	file = {Full Text PDF:/home/sam/Zotero/storage/ULXR9GF5/Stamatogiannakis et al. - 2015 - Looking Inside the Black-Box Capturing Data Prove.pdf:application/pdf},
}

@inproceedings{ji_recprov_2016,
	location = {Cham},
	title = {{RecProv}: Towards Provenance-Aware User Space Record and Replay},
	isbn = {978-3-319-40593-3},
	doi = {10.1007/978-3-319-40593-3_1},
	series = {Lecture Notes in Computer Science},
	shorttitle = {{RecProv}},
	abstract = {Deterministic record and replay systems have widely been used in software debugging, failure diagnosis, and intrusion detection. In order to detect the Advanced Persistent Threat ({APT}), online execution needs to be recorded with acceptable runtime overhead; then, investigators can analyze the replayed execution with heavy dynamic instrumentation. While most record and replay systems rely on kernel module or {OS} virtualization, those running at user space are favoured for being lighter weight and more portable without any of the changes needed for {OS}/Kernel virtualization. On the other hand, higher level provenance data at a higher level provides dynamic analysis with system causalities and hugely increases its efficiency. Considering both benefits, we propose a provenance-aware user space record and replay system, called {RecProv}. {RecProv} is designed to provide high provenance fidelity; specifically, with versioning files from the recorded trace logs and integrity protection to provenance data through real-time trace isolation. The collected provenance provides the high-level system dependency that helps pinpoint suspicious activities where further analysis can be applied. We show that {RecProv} is able to output accurate provenance in both visualized graph and W3C standardized {PROV}-{JSON} formats.},
	pages = {3--15},
	booktitle = {Provenance and Annotation of Data and Processes},
	publisher = {Springer International Publishing},
	author = {Ji, Yang and Lee, Sangho and Lee, Wenke},
	editor = {Mattoso, Marta and Glavic, Boris},
	date = {2016},
	langid = {english},
	keywords = {project-provenance-pp, prov-tool},
	file = {Full Text PDF:/home/sam/Zotero/storage/R4637A8D/Ji et al. - 2016 - RecProv Towards Provenance-Aware User Space Recor.pdf:application/pdf},
}

@inproceedings{wang_lprov_2018,
	location = {New York, {NY}, {USA}},
	title = {Lprov: Practical Library-aware Provenance Tracing},
	isbn = {978-1-4503-6569-7},
	url = {https://dl.acm.org/doi/10.1145/3274694.3274751},
	doi = {10.1145/3274694.3274751},
	series = {{ACSAC} '18},
	shorttitle = {Lprov},
	abstract = {With the continuing evolution of sophisticated {APT} attacks, provenance tracking is becoming an important technique for efficient attack investigation in enterprise networks. Most of existing provenance techniques are operating on system event auditing that discloses dependence relationships by scrutinizing syscall traces. Unfortunately, such auditing-based provenance is not able to track the causality of another important dimension in provenance, the shared libraries. Different from other data-only system entities like files and sockets, dynamic libraries are linked at runtime and may get executed, which poses new challenges in provenance tracking. For example, library provenance cannot be tracked by syscalls and mapping; whether a library function is called and how it is called within an execution context is invisible at syscall level; linking a library does not promise their execution at runtime. Addressing these challenges is critical to tracking sophisticated attacks leveraging libraries. In this paper, to facilitate fine-grained investigation inside the execution of library binaries, we develop Lprov, a novel provenance tracking system which combines library tracing and syscall tracing. Upon a syscall, Lprov identifies the library calls together with the stack which induces it so that the library execution provenance can be accurately revealed. Our evaluation shows that Lprov can precisely identify attack provenance involving libraries, including malicious library attack and library vulnerability exploitation, while syscall-based provenance tools fail to identify. It only incurs 7.0\% (in geometric mean) runtime overhead and consumes 3 times less storage space of a state-of-the-art provenance tool.},
	pages = {605--617},
	booktitle = {Proceedings of the 34th Annual Computer Security Applications Conference},
	publisher = {Association for Computing Machinery},
	author = {Wang, Fei and Kwon, Yonghwi and Ma, Shiqing and Zhang, Xiangyu and Xu, Dongyan},
	urldate = {2023-08-24},
	date = {2018-12-03},
	keywords = {project-provenance-pp, prov-tool},
	file = {Full Text PDF:/home/sam/Zotero/storage/3YFG9E7U/Wang et al. - 2018 - Lprov Practical Library-aware Provenance Tracing.pdf:application/pdf},
}

@article{rupprecht_improving_2020,
	title = {Improving reproducibility of data science pipelines through transparent provenance capture},
	volume = {13},
	issn = {2150-8097},
	url = {https://dl.acm.org/doi/10.14778/3415478.3415556},
	doi = {10.14778/3415478.3415556},
	abstract = {Data science has become prevalent in a large variety of domains. Inherent in its practice is an exploratory, probing, and fact finding journey, which consists of the assembly, adaptation, and execution of complex data science pipelines. The trustworthiness of the results of such pipelines rests entirely on their ability to be reproduced with fidelity, which is difficult if pipelines are not documented or recorded minutely and consistently. This difficulty has led to a reproducibility crisis and presents a major obstacle to the safe adoption of the pipeline results in production environments. The crisis can be resolved if the provenance for each data science pipeline is captured transparently as pipelines are executed. However, due to the complexity of modern data science pipelines, transparently capturing sufficient provenance to allow for reproducibility is challenging. As a result, most existing systems require users to augment their code or use specific tools to capture provenance, which hinders productivity and results in a lack of adoption. In this paper, we present Ursprung,1 a transparent provenance collection system designed for data science environments.2 The Ursprung philosophy is to capture provenance and build lineage by integrating with the execution environment to automatically track static and runtime configuration parameters of data science pipelines. Rather than requiring data scientists to make changes to their code, Ursprung records basic provenance information from system-level sources and combines it with provenance from application-level sources (e.g., log files, stdout), which can be accessed and recorded through a domain-specific language. In our evaluation, we show that Ursprung is able to capture sufficient provenance for a variety of use cases and only adds an overhead of up to 4\%.},
	pages = {3354--3368},
	number = {12},
	journaltitle = {Proc. {VLDB} Endow.},
	author = {Rupprecht, Lukas and Davis, James C. and Arnold, Constantine and Gur, Yaniv and Bhagwat, Deepavali},
	urldate = {2023-08-24},
	date = {2020-08-01},
	keywords = {project-provenance-pp, prov-tool},
	file = {Full Text PDF:/home/sam/Zotero/storage/YVLSLMIG/Rupprecht et al. - 2020 - Improving reproducibility of data science pipeline.pdf:application/pdf},
}

@inproceedings{lee_towards_2015,
	title = {Towards Secure Provenance in the Cloud: A Survey},
	doi = {10.1109/UCC.2015.102},
	shorttitle = {Towards Secure Provenance in the Cloud},
	abstract = {Provenance information are meta-data that summarize the history of the creation and the actions performed on an artefact e.g. data, process etc. Secure provenance is essential to improve data forensics, ensure accountability and increase the trust in the cloud. In this paper, we survey the existing cloud provenance management schemes and proposed security solutions. We investigate the current related security challenges resulting from the nature of the provenance model and the characteristics of the cloud and we finally identify potential research directions which we feel necessary t should be covered in order to build a secure cloud provenance for the next generation.},
	eventtitle = {2015 {IEEE}/{ACM} 8th International Conference on Utility and Cloud Computing ({UCC})},
	pages = {577--582},
	booktitle = {2015 {IEEE}/{ACM} 8th International Conference on Utility and Cloud Computing ({UCC})},
	author = {Lee, Brian and Awad, Abir and Awad, Mirna},
	date = {2015-12},
	keywords = {project-provenance-pp},
	file = {IEEE Xplore Full Text PDF:/home/sam/Zotero/storage/AN3W669M/Lee et al. - 2015 - Towards Secure Provenance in the Cloud A Survey.pdf:application/pdf},
}

@inproceedings{fadolalkarim_pandde_2016,
	location = {New Orleans Louisiana {USA}},
	title = {{PANDDE}: Provenance-based {ANomaly} Detection of Data Exfiltration},
	isbn = {978-1-4503-3935-3},
	url = {https://dl.acm.org/doi/10.1145/2857705.2857710},
	doi = {10.1145/2857705.2857710},
	series = {{CODASPY} '16},
	shorttitle = {{PANDDE}},
	abstract = {Preventing data exﬁltration by insiders is a challenging process since insiders are users that have access permissions to the data. Existing mechanisms focus on tracking users’ activities while they are connected to the database, and are unable to detect anomalous actions that the users perform on the data once they gain access to it. Being able to detect anomalous actions on the data is critical as these actions are often sign of attempts to misuse data. In this paper, we propose an approach to detect anomalous actions executed on data returned to the users from a database. The approach has been implemented as part of the Provenancebased {ANomaly} Detection of Data Exﬁltration ({PANDDE}) tool. {PANDDE} leverages data provenance information captured at the operating system level. Such information is then used to create proﬁles of users’ actions on the data once retrieved from the database. The proﬁles indicate actions that are consistent with the tasks of the users. Actions recorded in the proﬁles include data printing, emailing, and storage. Proﬁles are then used at run-time to detect anomalous actions.},
	eventtitle = {{CODASPY}'16: Sixth {ACM} Conference on Data and Application Security and Privacy},
	pages = {267--276},
	booktitle = {Proceedings of the Sixth {ACM} Conference on Data and Application Security and Privacy},
	publisher = {Association for Computing Machinery},
	author = {Fadolalkarim, Daren and Sallam, Asmaa and Bertino, Elisa},
	urldate = {2023-08-24},
	date = {2016-03-09},
	langid = {english},
	keywords = {project-provenance-pp},
	file = {Fadolalkarim et al. - 2016 - PANDDE Provenance-based ANomaly Detection of Data.pdf:/home/sam/Zotero/storage/6KESJXH2/Fadolalkarim et al. - 2016 - PANDDE Provenance-based ANomaly Detection of Data.pdf:application/pdf},
}

@inproceedings{wang_lprov_2018-1,
	location = {New York, {NY}, {USA}},
	title = {Lprov: Practical Library-aware Provenance Tracing},
	isbn = {978-1-4503-6569-7},
	url = {https://dl.acm.org/doi/10.1145/3274694.3274751},
	doi = {10.1145/3274694.3274751},
	series = {{ACSAC} '18},
	shorttitle = {Lprov},
	abstract = {With the continuing evolution of sophisticated {APT} attacks, provenance tracking is becoming an important technique for efficient attack investigation in enterprise networks. Most of existing provenance techniques are operating on system event auditing that discloses dependence relationships by scrutinizing syscall traces. Unfortunately, such auditing-based provenance is not able to track the causality of another important dimension in provenance, the shared libraries. Different from other data-only system entities like files and sockets, dynamic libraries are linked at runtime and may get executed, which poses new challenges in provenance tracking. For example, library provenance cannot be tracked by syscalls and mapping; whether a library function is called and how it is called within an execution context is invisible at syscall level; linking a library does not promise their execution at runtime. Addressing these challenges is critical to tracking sophisticated attacks leveraging libraries. In this paper, to facilitate fine-grained investigation inside the execution of library binaries, we develop Lprov, a novel provenance tracking system which combines library tracing and syscall tracing. Upon a syscall, Lprov identifies the library calls together with the stack which induces it so that the library execution provenance can be accurately revealed. Our evaluation shows that Lprov can precisely identify attack provenance involving libraries, including malicious library attack and library vulnerability exploitation, while syscall-based provenance tools fail to identify. It only incurs 7.0\% (in geometric mean) runtime overhead and consumes 3 times less storage space of a state-of-the-art provenance tool.},
	pages = {605--617},
	booktitle = {Proceedings of the 34th Annual Computer Security Applications Conference},
	publisher = {Association for Computing Machinery},
	author = {Wang, Fei and Kwon, Yonghwi and Ma, Shiqing and Zhang, Xiangyu and Xu, Dongyan},
	urldate = {2023-08-24},
	date = {2018-12-03},
	keywords = {project-provenance-pp, prov-tool},
	file = {Full Text PDF:/home/sam/Zotero/storage/UJNPTKYG/Wang et al. - 2018 - Lprov Practical Library-aware Provenance Tracing.pdf:application/pdf},
}

@online{keniston_kernel_nodate,
	title = {Kernel Probes (Kprobes)},
	url = {https://www.kernel.org/doc/html/latest/trace/kprobes.html},
	shorttitle = {Kernel Probes (Kprobes)},
	titleaddon = {The Linux Kernel documentation},
	author = {Keniston, Jim and Panchamukhi, Prasanna S and Hiramatsu, Masami},
	urldate = {2023-08-24},
	langid = {american},
	keywords = {operating systems, project-provenance-pp},
	file = {Kernel Probes (Kprobes) — The Linux Kernel documentation:/home/sam/Zotero/storage/PALWK2QM/kprobes.html:text/html},
}

@online{desnoyers_using_nodate,
	title = {Using the Linux Kernel Tracepoints},
	url = {https://www.kernel.org/doc/html/latest/trace/tracepoints.html},
	titleaddon = {The Linux Kernel documentation},
	author = {Desnoyers, Matthieu},
	urldate = {2023-08-24},
	keywords = {operating systems, project-provenance-pp},
	file = {Using the Linux Kernel Tracepoints — The Linux Kernel documentation:/home/sam/Zotero/storage/VCQP6JNQ/tracepoints.html:text/html},
}

@online{smalley_linux_nodate,
	title = {Linux Security Modules: General Security Hooks for Linux},
	url = {https://docs.kernel.org/security/lsm.html},
	titleaddon = {The Linux Kernel documentation},
	author = {Smalley, Stephen and Fraser, Timothy and Vance, Chris},
	urldate = {2023-08-24},
	keywords = {operating systems, project-provenance-pp},
	file = {Linux Security Modules\: General Security Hooks for Linux — The Linux Kernel documentation:/home/sam/Zotero/storage/ZPXUQP9J/lsm.html:text/html},
}

@online{noauthor_bpf_nodate,
	title = {{BPF} Documentation},
	url = {https://docs.kernel.org/bpf/index.html},
	titleaddon = {The Linux Kernel documentation},
	urldate = {2023-08-24},
	keywords = {operating systems, project-provenance-pp},
	file = {BPF Documentation — The Linux Kernel documentation:/home/sam/Zotero/storage/G8CJH67B/index.html:text/html},
}

@online{gooch_overview_nodate,
	title = {Overview of the Linux Virtual File System},
	url = {https://docs.kernel.org/filesystems/vfs.html},
	titleaddon = {The Linux Kernel documentation},
	author = {Gooch},
	urldate = {2023-08-24},
	keywords = {operating systems, project-provenance-pp},
	file = {Overview of the Linux Virtual File System — The Linux Kernel documentation:/home/sam/Zotero/storage/CLVRT56M/vfs.html:text/html},
}

@online{noauthor_fuse_nodate,
	title = {{FUSE}},
	url = {https://www.kernel.org/doc/html/latest/filesystems/fuse.html},
	titleaddon = {The Linux Kernel documentation},
	urldate = {2023-08-24},
	keywords = {operating systems, project-provenance-pp},
	file = {FUSE — The Linux Kernel documentation:/home/sam/Zotero/storage/V8S6UDQG/fuse.html:text/html},
}

@inproceedings{lattner_llvm_2004,
	title = {{LLVM}: a compilation framework for lifelong program analysis \& transformation},
	doi = {10.1109/CGO.2004.1281665},
	shorttitle = {{LLVM}},
	abstract = {We describe {LLVM} (low level virtual machine), a compiler framework designed to support transparent, lifelong program analysis and transformation for arbitrary programs, by providing high-level information to compiler transformations at compile-time, link-time, run-time, and in idle time between runs. {LLVM} defines a common, low-level code representation in static single assignment ({SSA}) form, with several novel features: a simple, language-independent type-system that exposes the primitives commonly used to implement high-level language features; an instruction for typed address arithmetic; and a simple mechanism that can be used to implement the exception handling features of high-level languages (and setjmp/longjmp in C) uniformly and efficiently. The {LLVM} compiler framework and code representation together provide a combination of key capabilities that are important for practical, lifelong analysis and transformation of programs. To our knowledge, no existing compilation approach provides all these capabilities. We describe the design of the {LLVM} representation and compiler framework, and evaluate the design in three ways: (a) the size and effectiveness of the representation, including the type information it provides; (b) compiler performance for several interprocedural problems; and (c) illustrative examples of the benefits {LLVM} provides for several challenging compiler problems.},
	eventtitle = {International Symposium on Code Generation and Optimization, 2004. {CGO} 2004.},
	pages = {75--86},
	booktitle = {International Symposium on Code Generation and Optimization, 2004. {CGO} 2004.},
	author = {Lattner, C. and Adve, V.},
	date = {2004-03},
	keywords = {compilers, project-provenance-pp},
	file = {IEEE Xplore Full Text PDF:/home/sam/Zotero/storage/PEAVHRLJ/Lattner and Adve - 2004 - LLVM a compilation framework for lifelong program.pdf:application/pdf},
}

@article{luk_pin_2005,
	title = {Pin: building customized program analysis tools with dynamic instrumentation},
	volume = {40},
	issn = {0362-1340},
	url = {https://dl.acm.org/doi/10.1145/1064978.1065034},
	doi = {10.1145/1064978.1065034},
	shorttitle = {Pin},
	abstract = {Robust and powerful software instrumentation tools are essential for program analysis tasks such as profiling, performance evaluation, and bug detection. To meet this need, we have developed a new instrumentation system called Pin. Our goals are to provide easy-to-use, portable, transparent, and efficient instrumentation. Instrumentation tools (called Pintools) are written in C/C++ using Pin's rich {API}. Pin follows the model of {ATOM}, allowing the tool writer to analyze an application at the instruction level without the need for detailed knowledge of the underlying instruction set. The {API} is designed to be architecture independent whenever possible, making Pintools source compatible across different architectures. However, a Pintool can access architecture-specific details when necessary. Instrumentation with Pin is mostly transparent as the application and Pintool observe the application's original, uninstrumented behavior. Pin uses dynamic compilation to instrument executables while they are running. For efficiency, Pin uses several techniques, including inlining, register re-allocation, liveness analysis, and instruction scheduling to optimize instrumentation. This fully automated approach delivers significantly better instrumentation performance than similar tools. For example, Pin is 3.3x faster than Valgrind and 2x faster than {DynamoRIO} for basic-block counting. To illustrate Pin's versatility, we describe two Pintools in daily use to analyze production software. Pin is publicly available for Linux platforms on four architectures: {IA}32 (32-bit x86), {EM}64T (64-bit x86), Itanium®, and {ARM}. In the ten months since Pin 2 was released in July 2004, there have been over 3000 downloads from its website.},
	pages = {190--200},
	number = {6},
	journaltitle = {{SIGPLAN} Not.},
	author = {Luk, Chi-Keung and Cohn, Robert and Muth, Robert and Patil, Harish and Klauser, Artur and Lowney, Geoff and Wallace, Steven and Reddi, Vijay Janapa and Hazelwood, Kim},
	urldate = {2023-08-24},
	date = {2005-06-12},
	keywords = {computer architecture, project-provenance-pp},
	file = {Full Text PDF:/home/sam/Zotero/storage/LQ986PRH/Luk et al. - 2005 - Pin building customized program analysis tools wi.pdf:application/pdf},
}

@unpublished{kleen_intel_2015,
	location = {Seattle, Washington, {USA}},
	title = {Intel® Processor Trace on Linux},
	note = {Tracing Summit 2015},
	author = {Kleen, Andi and Strong, Beeman},
	date = {2015-08-10},
	langid = {english},
	keywords = {computer architecture, project-provenance-pp},
	file = {Kleen and Strong - Intel® Processor Trace on Linux.pdf:/home/sam/Zotero/storage/LLWIKPGQ/Kleen and Strong - Intel® Processor Trace on Linux.pdf:application/pdf},
}

@online{noauthor_ptrace_nodate,
	title = {ptrace},
	url = {https://man7.org/linux/man-pages/man2/ptrace.2.html},
	titleaddon = {Linux manual page},
	urldate = {2023-08-24},
	keywords = {operating systems, project-provenance-pp},
	file = {ptrace(2) - Linux manual page:/home/sam/Zotero/storage/NVK5RHL3/ptrace.2.html:text/html},
}

@thesis{dolstra_purely_2006,
	title = {The Purely Functional Software Deployment Model},
	url = {https://dspace.library.uu.nl/handle/1874/7540},
	abstract = {Software deployment is the set of activities related to getting software components to work on the machines of end users. It includes activities such as installation, upgrading, uninstallation, and so on. Many tools have been developed to support deployment, but they all have serious limitations with respect to correctness. For instance, the installation of a component can lead to the failure of previously installed components; a component might require other components that are not present; and it is generally difficult to undo deployment actions. The fundamental causes of these problems are a lack of isolation between components, the difficulty in identifying the dependencies between components, and incompatibilities between versions and variants of components. This thesis describes a better approach based on a purely functional deployment model, implemented in a deployment system called Nix. Components are stored in isolation from each other in a Nix store. Each component has a name that contains a cryptographic hash of all inputs that contributed to its build process, and the content of a component never changes after it has been built. Hence the model is purely functional. This storage scheme provides several important advantages. First, it ensures isolation between components: if two components differ in any way, they will be stored in different locations and will not overwrite each other. Second, it allows us to identify component dependencies. Undeclared build time dependencies are prevented due to the absence of "global" component directories used in other deployment systems. Runtime dependencies can be found by scanning for cryptographic hashes in the binary contents of components, a technique analogous to conservative garbage collection in programming language implementation. Since dependency information is complete, complete deployment can be performed by copying closures of components under the dependency relation. Developers and users are not confronted with components' cryptographic hashes directly. Components are built automatically from Nix expressions, which describe how to build and compose arbitrary software components; hashes are computed as part of this process. Components are automatically made available to users through "user environments", which are synthesised sets of activated components. User environments enable atomic upgrades and rollbacks, as well as different sets of activated components for different users. Nix expressions provide a source-based deployment model. However, source-based deployment can be transparently optimised into binary deployment by making pre-built binaries (keyed on their cryptographic hashes) available in a shared location such as a network server. This is referred to as transparent source/binary deployment. The purely functional deployment model has been validated by applying it to the deployment of more than 278 existing Unix packages. In addition, this thesis shows that the model can be applied naturally to the related activities of continuous integration using build farms, service deployment and build management.},
	institution = {Utrecht University},
	type = {phdthesis},
	author = {Dolstra, Eelco},
	urldate = {2023-08-24},
	date = {2006-01-18},
	langid = {english},
	keywords = {operating systems, project-provenance-pp},
}

@inproceedings{suen_s2logger_2013,
	title = {S2Logger: End-to-End Data Tracking Mechanism for Cloud Data Provenance},
	doi = {10.1109/TrustCom.2013.73},
	shorttitle = {S2Logger},
	abstract = {The inability to effectively track data in cloud computing environments is becoming one of the top concerns for cloud stakeholders. This inability is due to two main reasons. Firstly, the lack of data tracking tools built for clouds. Secondly, current logging mechanisms are only designed from a system-centric perspective. There is a need for data-centric logging techniques which can trace data activities (e.g. file creation, edition, duplication, transfers, deletions, etc.) within and across all cloud servers. This will effectively enable full transparency and accountability for data movements in the cloud. In this paper, we introduce S2Logger, a data event logging mechanism which captures, analyses and visualizes data events in the cloud from the data point of view. By linking together atomic data events captured at both file and block level, the resulting sequence of data events depicts the cloud data provenance records throughout the data lifecycle. With this information, we can then detect critical data-related cloud security problems such as malicious actions, data leakages and data policy violations by analysing the data provenance. S2Logger also enables us to address the gaps and inadequacies of existing system-centric security tools.},
	eventtitle = {2013 12th {IEEE} International Conference on Trust, Security and Privacy in Computing and Communications},
	pages = {594--602},
	booktitle = {2013 12th {IEEE} International Conference on Trust, Security and Privacy in Computing and Communications},
	author = {Suen, Chun Hui and Ko, Ryan K.L. and Tan, Yu Shyang and Jagadpramana, Peter and Lee, Bu Sung},
	date = {2013-07},
	note = {{ISSN}: 2324-9013},
	keywords = {project-provenance-pp, prov-tool},
	file = {IEEE Xplore Full Text PDF:/home/sam/Zotero/storage/IV6JM6YF/Suen et al. - 2013 - S2Logger End-to-End Data Tracking Mechanism for C.pdf:application/pdf},
}

@inproceedings{bates_trustworthy_2015,
	title = {Trustworthy \{Whole-System\} Provenance for the Linux Kernel},
	isbn = {978-1-939133-11-3},
	url = {https://www.usenix.org/conference/usenixsecurity15/technical-sessions/presentation/bates},
	eventtitle = {24th {USENIX} Security Symposium ({USENIX} Security 15)},
	pages = {319--334},
	author = {Bates, Adam and Tian, Dave (Jing) and Butler, Kevin R. B. and Moyer, Thomas},
	urldate = {2023-08-25},
	date = {2015},
	langid = {english},
	keywords = {project-provenance-pp, prov-tool},
	file = {Full Text PDF:/home/sam/Zotero/storage/5QPU4S6E/Bates et al. - 2015 - Trustworthy Whole-System Provenance for the Linu.pdf:application/pdf},
}

@article{bartlett_xsdk_2017,
	title = {{xSDK} Foundations: Toward an Extreme-scale Scientific Software Development Kit},
	volume = {4},
	rights = {Copyright (c)},
	issn = {2313-8734},
	url = {https://superfri.org/index.php/superfri/article/view/127},
	doi = {10.14529/jsfi170104},
	shorttitle = {{xSDK} Foundations},
	abstract = {Extreme-scale computational science increasingly demands multiscale and multiphysics formulations. Combining software developed by independent groups is imperative: no single team has resources for all predictive science and decision support capabilities. Scientific libraries provide high-quality, reusable software components for constructing applications with improved robustness and portability.  However, without coordination, many libraries cannot be easily composed.  Namespace collisions, inconsistent arguments, lack of third-party software versioning, and additional difficulties make composition costly.The Extreme-scale Scientific Software Development Kit ({xSDK}) defines community policies to improve code quality and compatibility across independently developed packages (hypre, {PETSc}, {SuperLU}, Trilinos, and Alquimia) and provides a foundation for addressing broader issues in software interoperability, performance portability, and sustainability.  The {xSDK} provides turnkey installation of member software and seamless combination of aggregate capabilities, and it marks first steps toward extreme-scale scientific software ecosystems from which future applications can be composed rapidly with assured quality and scalability.},
	pages = {69--82},
	number = {1},
	journaltitle = {Supercomputing Frontiers and Innovations},
	author = {Bartlett, Roscoe and Demeshko, Irina and Gamblin, Todd and Hammond, Glenn and Heroux, Michael Allen and Johnson, Jeffrey and Klinvex, Alicia and Li, Xiaoye and {McInnes}, Lois Curfman and Moulton, J. David and Osei-Kuffuor, Daniel and Sarich, Jason and Smith, Barry and Willenbring, James and Yang, Ulrike Meier},
	urldate = {2023-08-31},
	date = {2017-02-25},
	langid = {english},
	note = {Number: 1},
	keywords = {high-performance computing, project-provenance-pp},
	file = {Full Text PDF:/home/sam/Zotero/storage/C3CDRTF5/Bartlett et al. - 2017 - xSDK Foundations Toward an Extreme-scale Scientif.pdf:application/pdf},
}


@inproceedings{cartaxo_role_2018,
	location = {New York, {NY}, {USA}},
	title = {The Role of Rapid Reviews in Supporting Decision-Making in Software Engineering Practice},
	isbn = {978-1-4503-6403-4},
	url = {https://dl.acm.org/doi/10.1145/3210459.3210462},
	doi = {10.1145/3210459.3210462},
	series = {{EASE} '18},
	abstract = {Context: Recent work on Evidence Based Software Engineering ({EBSE}) suggests that systematic reviews lack connection with Software Engineering ({SE}) practice. In Evidence Based Medicine there is a growing initiative to address this kind of problem, in particular through what has been named as Rapid Reviews ({RRs}). They are adaptations of regular systematic reviews made to fit practitioners constraints. Goal: Evaluate the perceptions from {SE} practitioners on the use of Rapid Reviews to support decision-making in {SE} practice. Method: We conducted an Action Research to evaluate {RRs} insertion in a real-world software development project. Results: Our results show that practitioners are rater positive about Rapid Reviews. They reported to have learned new concepts, reduced time and cost of decision-making, improved their understanding about the problem their facing, among other benefits. Additionally, two months after the introduction of the Rapid Review, in a follow up visit, we perceived that the practitioners have indeed adopted the evidence provided. Conclusions: Based on the positive results we obtained with this study, and the experiences reported in medicine, we believe {RRs} could play an important role towards knowledge transfer and decision-making support in {SE} practice.},
	pages = {24--34},
	booktitle = {Proceedings of the 22nd International Conference on Evaluation and Assessment in Software Engineering 2018},
	publisher = {Association for Computing Machinery},
	author = {Cartaxo, Bruno and Pinto, Gustavo and Soares, Sergio},
	urldate = {2023-10-26},
	date = {2018-06-28},
	keywords = {rapid reviews},
	file = {Full Text PDF:/home/sam/Zotero/storage/QDQC8QUF/Cartaxo et al. - 2018 - The Role of Rapid Reviews in Supporting Decision-M.pdf:application/pdf},
}

@incollection{cartaxo_rapid_2020,
	location = {Cham},
	title = {Rapid Reviews in Software Engineering},
	isbn = {978-3-030-32489-6},
	url = {https://doi.org/10.1007/978-3-030-32489-6_13},
	abstract = {Integrating research evidence into practice is one of the main goals of evidence-based software engineering ({EBSE}). Secondary studies, one of the main {EBSE} products, are intended to summarize the “best” research evidence and make them easily consumable by practitioners. However, recent studies show that some secondary studies lack connections with software engineering practice. In this chapter, we present the concept of Rapid Reviews, which are lightweight secondary studies focused on delivering evidence to practitioners in a timely manner. Rapid reviews support practitioners in their decision-making, and should be conducted bounded to a practical problem, inserted into a practical context. Thus, Rapid Reviews can be easily integrated in a knowledge/technology transfer initiative. After describing the basic concepts, we present the results and experiences of conducting two Rapid Reviews. We also provide guidelines to help researchers and practitioners who want to conduct Rapid Reviews, and we finally discuss topics that may concern the research community about the feasibility of Rapid Reviews as an evidence-based method. In conclusion, we believe Rapid Reviews might be of interest to researchers and practitioners working on the intersection of software engineering research and practice.},
	pages = {357--384},
	booktitle = {Contemporary Empirical Methods in Software Engineering},
	publisher = {Springer International Publishing},
	author = {Cartaxo, Bruno and Pinto, Gustavo and Soares, Sergio},
	editor = {Felderer, Michael and Travassos, Guilherme Horta},
	urldate = {2023-10-27},
	date = {2020},
	langid = {english},
	doi = {10.1007/978-3-030-32489-6_13},
	keywords = {rapid reviews},
	file = {Full Text PDF:/home/sam/Zotero/storage/4XQIHKDN/Cartaxo et al. - 2020 - Rapid Reviews in Software Engineering.pdf:application/pdf},
}

@article{schunemann_reviews_2015,
	title = {Reviews: Rapid! Rapid! Rapid! …and systematic},
	volume = {4},
	issn = {2046-4053},
	url = {https://doi.org/10.1186/2046-4053-4-4},
	doi = {10.1186/2046-4053-4-4},
	shorttitle = {Reviews},
	pages = {4},
	number = {1},
	journaltitle = {Systematic Reviews},
	author = {Schünemann, Holger J. and Moja, Lorenzo},
	urldate = {2023-10-27},
	date = {2015-01-14},
	keywords = {rapid reviews},
	file = {Full Text PDF:/home/sam/Zotero/storage/SVE98P9A/Schünemann and Moja - 2015 - Reviews Rapid! Rapid! Rapid! …and systematic.pdf:application/pdf;Snapshot:/home/sam/Zotero/storage/TJK8MEUF/2046-4053-4-4.html:text/html},
}
@report{xu_dxt_2017,
	title = {{DXT}: Darshan {eXtended} Tracing},
	url = {https://www.osti.gov/biblio/1392598},
	shorttitle = {{DXT}},
	abstract = {The U.S. Department of Energy's Office of Scientific and Technical Information},
	institution = {Argonne National Lab. ({ANL}), Argonne, {IL} (United States)},
	author = {Xu, Cong and Snyder, Shane and Venkatesan, Vishwanath and Carns, Philip and Kulkarni, Omkar and Byna, Suren and Sisneros, Roberto and Chadalavada, Kalyana},
	urldate = {2023-10-27},
	date = {2017-05-08},
	keywords = {provenance-tool},
	file = {Full Text PDF:/home/sam/Zotero/storage/3WW37QBM/Xu et al. - 2017 - DXT Darshan eXtended Tracing.pdf:application/pdf},
}

