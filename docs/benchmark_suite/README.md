---
from: markdown
verbosity: INFO
stadnalone: yes
dpi: 300
table-of-contents: no
strip-comments: yes
citeproc: yes
ccite-method: citeproc
bibliography: zotero.bib
link-citations: yes
link-bibliography: yes
notes-after-punctuation: yes
title: Evaluating prior research on system-level provenance for practical use
author:
  - Samuel Grayson
  - Reed Milewicz
  - Daniel S. Katz
  - Darko Marinov
date: 2024-01-30
documentclass: article
pagestyle: plain
papersize: letter
---

# Background

The Oxford English Dictionary defines _provenance_ as "a record of the ultimate origin and passage of an item through its previous owners."
In the domain of computational science, we will specialize this definition to mean "the computational input artifacts and computational processes that influenced a certain computational output artifact", as described by Freire [@freireProvenanceComputationalTasks2008].
This definition is usually applied recursively, so the computational provenance would describe the output, the inputs, the inputs' inputs, and so forth until the artifacts were generated outside the computational system (they could be generated by an opaque computational system or directly by a user)[^DSK-artifacts].

[^DSK-artifacts]: DSK: perhaps better "the artifacts come from outside the computational system, such as observations"?

Computational provenance has many potential applications, including the following [@pimentelSurveyCollectingManaging2019]:

1. **Caching**.
  A black-box system that captures provenance knows what inputs affect the output for each process, so it knows which processes to re-execute without a user-supplied dependency graph programmer, as in traditional Make[^DSK-caching].
  
[^DSK-caching]: DSK: I think of caching as knowing what not to re-execute, aka memoization. What you describe here is more checkpointing to me. They are related...

2. **Comprehension**.
   Provenance data helps the user understand, debug, and document workflows.

3. **Management**.
   Provenance data can help catalog, label, and recall experimental results based on the input parameters.

4. **Reproducibility**.
   A description of the inputs and processes used to generate a specific output can aid manual and automatic reproduction of that output[^acm-defns].

   [^acm-defns]: "Reproduction", in the ACM sense, where a **different team** uses the **same artifacts** to generate the output artifact [@acminc.staffArtifactReviewBadging2020].

One can capture computational provenance by modifying an application to report provenance data, leveraging a workflow engine or programming language to report provenance data, or leveraging an operating system to emit provenance data to report provenance data [@freireProvenanceComputationalTasks2008].

- **Application-level** provenance is the most semantically rich, since it knows the use of each input at the application-level (see @fig:app-lvl-prov), but the least general, since each application would have to be modified individually.

- **Workflow-level** or **language-level** provenance is a middle ground in semantic richness and generality;
  it only knows the use of inputs in a dataflow sense (see @fig:wf-lvl-prov), but all applications using the provenance-modified workflow engine or programming language would emit provenance data without themselves being modified to emit provenance data.

- **System-level** is the most general, since all applications on the system would emit provenance data, but it is the least semantically rich, since observed dependencies may overapproximate the true dependencies (see [@lst:sys-lvl-log; @fig:sys-lvl-prov]).

<div id="fig:prov">

  ![Application-level prov](app-lvl-prov.svg){#fig:app-lvl-prov width=24%}
  ![Workflow-level prov](wf-lvl-prov.svg){#fig:wf-lvl-prov width=24%}
```{#lst:sys-lvl-log caption="Sys-level log" width=24%}
read A
write B
read C
write D
write E
```
  ![System-level prov](sys-lvl-prov.svg){#fig:sys-lvl-prov width=24%}


Several provenance graphs collected at different levels.

</div>

# Motivation

Lack of reproducibility in computational experiments undermines the long-term credibility of science and hinders the day-to-day work of researchers.
Empirical studies [@trisovicLargescaleStudyResearch2022; @graysonAutomaticReproductionWorkflows2023; @collbergRepeatabilityComputerSystems2016; @zhaoWhyWorkflowsBreak2012] show that reproducibility is rarely achieved in practice, probably due to its difficulty under the short time constraints that scientists have for new publications.^[DSK: also due to a lack of incentives for achieving it, at least in comparison to other things that could be done with the same effort.]
Rather than exhorting researchers to spend more of their short time-budget on reproducibility, it may be more effective to make reproducibility easier to achieve with the same amount of effort.^[DSK: maybe rephrase as positive: If reproducibility doesn't require additional research effort, it's much more likely to happen.]

Provenance data improves manual reproducibility, because users have a record of the inputs, outputs, and processes used to create a computational artifact.
Provenance data also has the potential to enable automatic reproducibility, if the process trace is detailed enough to be "re-executed".

One may imagine an abstract tradeoff curve between "enabling provenance applications such as reproducibility" as the horizontal axis increasing rightwards and "cost to implement" that provenance data on the vertical axis increasing upwards (fig. [@fig:cost-vs-enabling]).
A typical status quo, not collecting any provenance data and not using workflows, is at the bottom left:
  no added cost and does nothing to enable provenance applications.
System-level, workflow/language-level, and application-level are on a curve, increasing cost and enabling more provenance applications.

The initial jump in cost from nothing to system-level may be low because the user need not change _anything_ about their application;
  they merely need to install some provenance tracer onto their system and run their code, without modifying it, in the tracer. ^[DSK: what about the performance penalty? Since you talk about performance in contributions, I think you have to introduce it here.]
Perceived ease of use is a critical factor in the adoption of new technologies (formalized in the Technology Acceptance Model [@davisTechnologyAcceptanceModel1985]).
Although the user may eventually use more semantically rich provenance, low-cost system-level provenance would get provenance's "foot in the door". ^[DSK: but what is the cost? at the start of the paragraph, you said there is **no** user cost, but there is at least the performance cost.]
While this data is less rich than that of the workflow or application level, it may be enough to enable important applications such as caching, reproducibility, and file-level comprehension.
Since system-level provenance collection is a possibly valuable tradeoff between cost and enabling provenance applications, system-level provenance will be the subject of this work.

# Contributions

This work aims to summarize state of the art, establish goalposts for future research in the area, and identify which provenance tools are practically usable.

This work contributes:

- **A rapid review**:
    There are scores of academic publications on system-level provenance (see @tbl:tools), so we collate all ^[DSK: how is **all** determined? this seems like a setup for a reviewer to tell you that you missed something] of prior provenance tools and classify them by _capture method_ (e.g., does the provenance collector require you to load a kernel module or run your code in a VM?). 

- **A benchmark suite**:
  Prior work does not use a consistent set of benchmarks; often publications use an overlapping set of benchmarks from prior work <!-- TODO: ref table -->.
  We collate benchmarks used in prior work, add some unrepresented areas, and find a statistically valid subset of the benchmark.

- **A quantitative performance comparison**:
  Prior publications often only compares the performance their provenance tool to the baseline, no-provenance performance, not to other provenance tools.
  It is difficult to compare provenance tools, given data of different benchmarks on different machines.
  This work runs a consistent set of benchmarks on a single machine over all provenance tools.

- **A predictive performance model**:
  The performance overhead of a single provenance system varies from <1% to 23% [@muniswamy-reddyLayeringProvenanceSystems2009] based on the application, so a single number for overhead is not sufficient.
  This work develops a statistical model for predicting the overhead of \$X application in \$Y provenance system based on \$Y provenance system's performance on our benchmark suite and \$X application's performance characteristics (e.g., number of I/O syscalls).

# Methods

## Rapid review

We began a rapid review to identify the research state-of-the-art tools for automatic system-level provenance.

Rapid Reviews are a lighter-weight alternative to systematic literature reviews with a focus on timely feedback for decision-making.
Schünemann and Moja [@schunemannReviewsRapidRapid2015] show that Rapid Reviews can yield substantially similar results to a systematic literature review, albeit with less detail.
Although developed in medicine, Cartaxo et al. show that Rapid Reviews are useful for informing software engineering design decisions [@cartaxoRoleRapidReviews2018; @cartaxoRapidReviewsSoftware2020].

We conducted a rapid review with the following parameters:

- **Objective**: Identify system-level provenance collection tools.

- **Search terms**: "system-level" AND "provenance"

- **Search engine**: Google Scholar

- **Number of results**: 50

  - This threshold is the point of diminishing returns, as no new tools came up in the 40th – 50th results.

- **Criteria**: A relevant publication would center on one or more operating system-level tools that capture file provenance. A tool requiring that the user use a specific application or platform would be irrelevant.

We record the following features for each system-level provenance tool:

- **Capture method**: What method does the tool use to capture provenance?

  - **User-level tracing**:
    A provenance tool may use "debugging" or "tracing" features provided by the kernel to trace the I/O operations of another program, e.g., `ptrace(2)` [@Ptrace].

  - **Built-in auditing service**:
    A provenance tool may use auditing service built in to the kernel, e.g., Linux Auditing Framework [@madabhushanaConfigureLinuxSystem2021], enhanced Berkeley Packet Filter (eBPF) [@BPFDocumentation], kprobes [@kenistonKernelProbesKprobes], and ETW [@EventTracingWin322021] for Windows.

  - **Filesystem instrumentation**:
    A provenance tool may set up a file system, so it can log I/O operations, e.g., using Filesystem in User SpacE (FUSE) interface [@FUSE], or Virtual File System (VFS) interface [@goochOverviewLinuxVirtual].

  - **Dynamic library instrumentation**:
    A provenance tool may replace a library used to execute I/O operations (e.g., glibc) with one that logs the calls before executing them.

  - **Binary instrumentation**:
    A provenance tool may use binary instrumentation (dynamic or static) to identify I/O operations in another program.

  - **Compile-time instrumentation**:
    A provenance tool may be a compiler pass that modifies the program to emit provenance data, especially intra-program control flow.

  - **Kernel instrumentation**:
    A provenance tool may be a modified kernel either by directly modifying the kernel's source tree or loading a kernel module.

  - **VM instrumentation**:
    A provenance tool may execute the program in a virtual machine, where it can observe the program's I/O operations.

- **Is source code available?**:
  We use the categorical codes given by Collberg and Proebsting [@collbergRepeatabilityComputerSystems2016] to describe whether the source code is in the article, found on the web, found by an email from the author, refused from an email by the author, or the authors did not reply.

## Selecting benchmarks

Using the tools selected above, we identify all benchmarks that have been used in prior work.
<!-- TODO: Explain methodology for selecting/implementing benchmark -->
To get consistent measurements, we select as many benchmarks and provenance tracers as we reasonably can, and run a complete matrix (every tracer on every benchmark).

We also added new benchmarks:

- **Workflows**:
  Only one of the commonly used benchmarks from prior work (BLAST) resembles an e-science workflow (multiple intermediate inputs/outputs on the filesystem), so we added non-containerized Snakemake workflows from prior work [@graysonAutomaticReproductionWorkflows2023].

- **Data science**:
  None of the benchmarks resembled a typical data science program, so we added the most popular Notebooks from Kaggle.com, a data science competition website.

- **Compilations**:
  Prior work uses compilation of Apache or of Linux.
  We added compilation of several other packages (any package in Spack) to our benchmark.
  Compiling packages is a good use-case for a provenance tracer, because a user might trial-and-error multiple compile commands and not remember the exact sequence of "correct" commands;
  the provenance tracker would be able to recall the commands which did not get overwritten, so the user can know what commands "actually worked". ^[DSK: this reminds me of VisTrails from Utah]

- **Computational simulations**:
  High-performance computing (HPC) scientific simulations could benefit from provenance tracing.
  These HPC applications may have access patterns quite different than conventional desktop applications.
  The xSDK framework [@bartlettXSDKFoundationsExtremescale2017] collects a ^[DSK: end is missing]

In order to reduce the amount of work needed to reduce the amount of time it takes for the now numerous benchmarks to run for future work, we subset our benchmark based on interpolative matrix decomposition.
**Interpolative decomposition** (ID) factors a matrix $A \in \mathbb R^{n \times m}$ of rank $k$ into two factors $B \times P$ where the $B$ contains a subset of the columns of $A$ and a subset of the columns of $P$ form the $k \times k$ identity matrix [@libertyRandomizedAlgorithmsLowrank2007]^[Also see the function scipy.linalg.interpolative of the Python package scipy.].
When $A$ is not of rank $k$, the factorization is guaranteed to be within a certain factor of the best unrestricted $R^{n \times k} \times R^{k \times m}$ factorization.

To apply ID to benchmark subsetting, suppose $A_{ij}$ was the runtime performance of the $i$th system on the $j$th benchmark, then the ID essentially finds $k$  "non-redundant" benchmarks and approximates the rest (which I call "redundant") as a linear function of the "non-redundant" benchmarks, while minimizing $\|A - BP\|_2$.
$$\left(\begin{array}{ccc}
|       &        & | \\
A_{:,1} & \cdots & A_{:,n} \\
|       &        & | \\
\end{array}\right) = A \approx BP = \left(\begin{array}{ccc}
| &  & | \\
A_{:,1} & \cdots & A_{:,k} \\
| &  & | \\
\end{array}\right) \left(\begin{array}{ccccccc}
1      & 0      & \cdots & 0      & |       &        & | \\
0      & \ddots &        & \vdots & P_{:,j} & \cdots & P_{:,m}\\
\vdots &        & \ddots & 0      & |       &        & | \\
0      & \cdots & 0      & 1      & |       &        & | \\
\end{array}\right).$$
- If the $j$th benchmark is non-redundant, then the $j$th column of $A$ is "copied" to the $j$th column of $B$, and the $j$th row of$P$ is set to the $j$th row of the identity matrix; multiplying $B$ by $P$ "passes through verbatim" the $j$th column of $B$.
- Otherwise, the $j$th benchmark is redundant and the $j$th column of $P$ is set to the coefficients of a linear regression from the non-redundant benchmarks to the $j$the benchmark; multiplying $B$ by $P$ evaluates a prediction based on the linear regression in the $j$th column.


## Quantitative performance comparison

We use BenchExec [@beyerReliableBenchmarkingRequirements2019] to precisely measure the CPU time, wall time, memory utilization, and other attributes of the process (including child processes) in a Linux CGroup without networking, isolated from other processes on the system.

## Predictive performance model

# Results

## Qualitative feature comparison

@Tbl:tools shows the provenance tools we collected and their qualitative features, while @tbl:excluded shows the tools that have been called "system-level provenance tools" but do not fit the definition used in this paper.
Of these, @tbl:prior-benchmarks shows the benchmarks used to evaluate each tool.
Running HTTP servers may be a popular benchmark because prior work focuses overwhelmingly on provenance for the sake of security (auditing, intrusion detection, or digital forensics);
  securing HTTP servers and web applications is a common task.

| Tool                                                           | Collection method                        | Collection tool                            | Notes                       |
|---------------------------------------|------------------------------------------|--------------------------------------------|-----------------------------|
| SPADE [@gehaniSPADESupportProvenance2012]                      | Audit, FS, **or** compile-time           | Multiple[^spade]                           | Can use multiple sources    |
| OPUS [@balakrishnanOPUSLightweightSystem2013]                  | Library instrumentation                  | libc instrumentaiton                       |                             |
| FiPS [@sultanaFileProvenanceSystem2013]                        | FS. ins.                                 | VFS                                        |                             |
| RecProv [@jiRecProvProvenanceAwareUser2016]                    | Tracing                                  | rr, ptrace                                 |                             |
| PANDDE [@fadolalkarimPANDDEProvenancebasedANomaly2016]         | FS. ins.                                 | Custom VFS                                 |                             |
| URSprung [@rupprechtImprovingReproducibilityData2020]          | Audit, file-system ins.                  | auditd, IBM Spectrum Scale                 | For IBM Spectrum Scale      |
| Event Tracer for Windows [@EventTracingWin322021]              | Audit                                    | NT Kernel                                  | For Windows                 |
| TREC [@vahdatTransparentResultCaching1998]                     | Audit                                    | Proc filesystem                            | For Solaris                 |
| DTrace [@DTrace]                                               | Audit                                    | Respective kernels                         | Does processing in kernel   |
| Sysmon [@markrussSysmonSysinternals2023]                       | Audit                                    | NT Kernel                                  | Implemented for Windows     |
| Ma et al. [@maAccurateLowCost2015]                             | Kernel ins.                              | ETW                                        |                             |
| BEEP [@leeHighAccuracyAttack2017]                              | Dyn., static binary ins.                 | Intel Pin, PEBIL                           |                             |
| libdft [@kemerlisLibdftPracticalDynamic2012]                   | Dyn. binary ins.                         | Intel Pin                                  |                             |
| RAIN [@jiRAINRefinableAttack2017]                              | kernel ins., lib. ins., dyn. binary ins. | libc ins., custom kernel module, Intel Pin | Replays syscalls under DIFT |
| DataTracker [@stamatogiannakisLookingBlackBoxCapturing2015]    | Dinamic binary ins.                      |                                            |                             |
| MPI[@maMPIMultiplePerspective2017]                             | Compile-time ins.                        | LLVM pass                                  | Requires manual input       |
| LDX [@kwonLDXCausalityInference2016]                           | Compile-time ins.                        | LLVM pass                                  |                             |
| S2Logger [@suenS2LoggerEndtoEndData2013]                       | Kernel ins.                              | Kernel module or Linux Security Module     |                             |
| ProTracer [@maProTracerPracticalProvenance2016]                | Kernel ins.                              | Linux tracepoints, custom kernel module    |                             |
| Hi-Fi [@pohlyHiFiCollectingHighfidelity2012]                   | Kernel ins.                              | Linux Security Module                      |                             |
| Lineage FS [@sarLineageFileSystem]                             | Kernel ins.                              | Modified kernel                            |                             |
| PASS/Pasta [@muniswamy-reddyProvenanceAwareStorageSystems2006] | Kernel ins., filesystem ins.             | Modified kernel, VFS                       |                             |
| PASSv2/Lasagna [@muniswamy-reddyLayeringProvenanceSystems2009] | Kernel ins., filesystem, lib. ins.       | Modified kernel, instrumented libc, VFS    |                             |
| RTAG [@jiEnablingRefinableCrossHost2018]                       | Kernel ins.                              | Modified kernel                            |                             |
| LPM/ProvMon [@batesTrustworthyWholeSystemProvenance2015]       | Kernel ins.                              | Modified kernel, kernel module, NetFilter  |                             |
| CamFlow [@pasquierPracticalWholesystemProvenance2017]          | Kernel ins.                              | Linux Security Module, NetFilter           |                             |
| LPROV [@wangLprovPracticalLibraryaware2018]                    | Kernel ins., library ins.                | Custom kernel module, custom loader, BEEP  |                             |
| Panorama [@yinPanoramaCapturingSystemwide2007]                 | VM ins.                                  | QEMU                                       |                             |
| PROV-Tracer [@stamatogiannakisDecouplingProvenanceCapture2015] | VM ins.                                  | QEMU, PANDA                                |                             |

: Provenance trackers mentioned in primary and secondary studies in our search results. {#tbl:tools}

[^spade]: SPADE can use multiple backends, including other provenance collectors. On Linux, SPADE can use: Auditd, CamFlow, FUSE; On MacOS: OpenBSM, MacFUSE, Fuse4x; On Windows: ProcessMonitor; On any platform: import static data (e.g., from logs on disk), applications instrumented with API, applications compiled with LLVM pass.

| Tool                                                  | Reason                                                          |
|-------------------------------------------------------|-----------------------------------------------------------------|
| ES3 [@frewES3DemonstrationTransparent2008]            | specific to ES3 platform                                        |
| Chimera [@fosterChimeraVirtualData2002]               | specific to Chimera platform                                    |
| INSPECTOR [@thalheimInspectorDataProvenance2016a]     | doesn't track files                                             |
| MCI [@jiEnablingRefinableCrossHost2018]               | offline; depends on online-LDX                                  |
| OmegaLog [@hassanOmegaLogHighFidelityAttack2020]      | depends on app-level logs                                       |
| LogGC [@leeLogGCGarbageCollecting2013]                | contribution is deleting irrelevant events in the logs          |
| UIScope [@yangUISCOPEAccurateInstrumentationfree2020] | captures UI interactions; uses ETW to capture I/O operations    |
| Winnower [@hassanScalableClusterAuditing2018]         | specific to Docker Swarm                                        |

: Excluded tools. {#tbl:excluded}

| Provenance publication                                   | Benchmarks                                                                                                                             | Comparisons          | Year |
|----------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------|----------------------|------|
| TREC [@vahdatTransparentResultCaching1998]               | open/close, compile Apache, compile LaTeX doc                                                                                          | Native               | 1999 |
| PASS [@muniswamy-reddyProvenanceAwareStorageSystems2006] | BLAST                                                                                                                                  | Native ext2          | 2006 |
| Panorama [@yinPanoramaCapturingSystemwide2007]           | curl, scp, gzip, bzip2                                                                                                                 | Native               | 2007 |
| PASSv2 [@muniswamy-reddyLayeringProvenanceSystems2009]   | BLAST, compile Linux, Postmark, Mercurial, Kepler                                                                                  | Native ext3, NFS     | 2009 |
| SPADEv2 [@gehaniSPADESupportProvenance2012]              | BLAST, compile Apache, Apache                                                                                                          | Native               | 2012 |
| Hi-Fi [@pohlyHiFiCollectingHighfidelity2012]             | lmbench, compile Linux, Postmark                                                                                                       | Native               | 2012 |
| libdft [@kemerlisLibdftPracticalDynamic2012]             | scp, {tar, gzip, bzip2} x {extract, compress}                                                                                          | PIN                  | 2012 |
| LogGC [@leeLogGCGarbageCollecting2013]                   | RUBiS, Firefox, MC, Pidgin, Pine, Proftpd, Sendmail, sshd, vim, w3m, wget, xpdf, yafc, Audacious, bash, Apache, mysqld | [^loggc-bench]       | 2013 |
| LPM/ProvMon [@batesTrustworthyWholeSystemProvenance2015] | lmbench, compile Linux, Postmark, BLAST                                                                                                | Native               | 2015 |
| Ma et al. [@maAccurateLowCost2015]                       | TextTransfer, Chromium, DrawTool, NetFTP, AdvancedFTP, Apache, IE, Paint, Notepad, Notepad++, simplehttp, Sublime Text                 | Native               | 2015 |
| ProTracer [@maProTracerPracticalProvenance2016]          | Apache, miniHTTP, ProFTPD, Vim, Firefox, w3m, wget, mplayer, Pine, xpdf, MC, yafc                                  | Auditd, BEEP, LogGC  | 2016 |
| LDX [@kwonLDXCausalityInference2016]                     | SPEC CPU 2006, Firefox, lynx, nginx, tnftp, sysstat, gif2png, mp3info, prozilla                                                    | Native               | 2016 |
| (LDX continued)                                          | yopsweb, ngircd, gocr, Apache, pbzip2, pigz, axel, x264                                                                        |                      |      |
| MPI [@maMPIMultiplePerspective2017]                      | Apache, bash, Evince, Firefox, Krusader, wget, most, MC, mplayer,                                                  | Audit, LPM-HiFi      | 2017 |
| (MPI continued)                                          | MPV, nano, Pine, ProFTPd, SKOD, TinyHTTPd, Transmission, Vim, w3m, Xpdf, Yafc                                  |                      |      |
| CamFlow [@pasquierPracticalWholesystemProvenance2017]    | lmbench, postmark, unpack kernel, compile Linux, Apache, Memcache, redis, php, pybench                                                 | Native               | 2017 |
| BEEP [@leeHighAccuracyAttack2017]                        | Apache, Vim, Firefox, wget, Cherokee, w3m, ProFTPd, yafc, Transmission, Pine, bash, MC, sshd, sendmail                 | Native               | 2017 |
| RAIN [@jiRAINRefinableAttack2017]                        | SPEC CPU 2006, cp linux, wget, compile libc, Firefox, SPLASH-3                                                                         | Native               | 2017 |
| Sciunit [@tonthatSciunitsReusableResearch2017]           | VIC, FIE                                                                                                                               | Native               | 2017 |
| LPROV [@wangLprovPracticalLibraryaware2018]              | Apache, simplehttp, proftpd, sshd, firefox, filezilla, lynx, links, w3m, wget, ssh, pine, vim, emacs, xpdf                             | Native               | 2018 |
| MCI [@kwonMCIModelingbasedCausality2018]                 | Firefox, Apache, Lighttpd, nginx, ProFTPd, CUPS, vim, elinks,                                                              | BEEP                 | 2018 |
| (MCI continued)                                          | alpine, zip, transmission, lftp, yafc, wget, ping, procps                                                                      |                      |      |
| RTAG [@jiEnablingRefinableCrossHost2018]                 | SPEC CPU 2006, scp, wget, compile llvm, Apache                                                                                         | RAIN                 | 2018 |
| URSPRING [@rupprechtImprovingReproducibilityData2020]    | open/close, fork/exec/exit, pipe/dup/close, socket/connect, CleanML, Vanderbilt, Spark, ImageML                                | Native, SPADE+auditd | 2020 |

: Benchmarks used in various provenance publications. {#tbl:prior-benchmarks}

[^loggc]: LogGC measures the offline running time and size of garbage collected logs; there is no comparison to native would be applicable.

| Benchmark group                                                                                     | Uses in prior work |
|-----------------------------------------------------------------------------------------------------|--------------------|
| BLAST                                                                                               | 4                  |
| Python data science                                                                                 | 0                  |
| HTTP server/traffic (Apache httpd, miniHTTP, simplehttp, lighttpd, Nginx, tinyhttpd, cherokee)      | 10                 |
| HTTP serer/client (curl, wget, prozilla, axel)                                                      | 9                  |
| FTP server/traffic (ProFTPd)                                                                        | 5                  |
| FTP client (lftp, yafc, tnftp, skod)                                                                | 2                  |
| SPEC CPU 2006                                                                                       | 3                  |
| Compile (Apache, LLVM, libc, Linux, LaTeX doc)                                                      | 8                  |
| lmbench[^lmbench]                                                                                   | 5                  |
| Postmark                                                                                            | 4                  |
| SPLASH-3                                                                                            | 1                  |
| Browsers (Firefox, Chromium, w3m)                                                                   | 8                  |
| VCS (Mercurial, git)                                                                                | 1                  |
| Un/archive ({compress, decompress} x tar x {nothing, bzip2, pbzip, gzip, pigz, zip})[^unpack-linux] | 5                  |
| Shellbench (bash, sh)                                                                               | 3                  |
| Workflows (Snakemake-workflow-catalog, Nf-core, CleanML, Spark, VIC, FIE)                           | 2                  |
| Microbenchmarks (open/close, fork/exec, pipe/dup/close, socket/connect)                             | 2                  |
| xSDK codes                                                                                          | 0                  |
| Small, fast binaries (procps, sysstat, gif2png, mp3info, gocr)                                      | 2                  |
| x264                                                                                                | 1                  |

: Benchmarks implemented by this work {#tbl:implemented-benchmarks}

[^unpack-linux]: We count "unpack linux" as an occurrence of an un/archive benchmark.
[^lmbench]: Noting that lmbench contains open/close, fork/exec, and pipe/close.

## Benchmarks

## Quantitative performance comparison

## Predictive model

# Analysis

## Threats to validity

# Future work

# Conclusion

# References
